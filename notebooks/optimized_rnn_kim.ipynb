{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac72ae-9be5-43d4-87d3-b2319a0f757b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Time Series COVID-19 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a246bb7-0cdf-4939-a4e9-870f871706f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd5297-79e4-4864-bb68-8deb37dbc70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c99354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload imported module every time a jupyter cell is executed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de425903-9365-4407-8ae8-9b20a6e41dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "import seaborn as sns \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import requests\n",
    "import pandas_profiling\n",
    "from typing import overload\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop \n",
    "from covid_time_series_prediction.ml_logic import preprocessor\n",
    "# from ml_logic.country_data import country_output\n",
    "from covid_time_series_prediction.ml_logic.preprocessor import train_test_set, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad4771-e464-4eb1-86f5-c5c480cea8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e1591b-31c5-4a38-b0fb-4bfaec0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Data project directory\n",
    "# data_dir = '../data/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760eea31",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, figsize=(17,7))\n",
    "# plt.plot(          ['new_deaths']);\n",
    "# ax.set_title(\"Covid 19 calculation for different countries\", size=10)\n",
    "# ax.set_ylabel(\"Number of death cases\", size=10)\n",
    "# ax.set_xlabel(\"Date\", size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b3bb-c12b-43a3-9dda-7bbd905f3622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122911e-70c8-4131-bf2f-8db270b3ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TENSORFLOW & RNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636afbb-aab0-4500-897b-311961289203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recurrent Neural Network (sequences data) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f081b-7ee6-405b-8e13-04bf267ac2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427dc3-25b3-4a5a-abba-20e92d67ab1b",
   "metadata": {},
   "source": [
    "X.shape = (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES) and y = RNN(X) where $X_{i,j}^{t}$\n",
    "\n",
    "with $_{i}$ is the sample/sequence, $_{j}$ is the feature measured and  $^{t}$ is the time at which the observation is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8151dc-1f0e-4981-91a0-c1164230af26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- **retrieve dataset** from Sumedha & Alberto\n",
    "\n",
    "    - **clean dataset**: \n",
    "        \n",
    "        - **drop first lines == 0** *(before Covid arrived)*\n",
    "        \n",
    "        - **check Nan**: \n",
    "- **strategy 1 country by country** sequences split as follow:\n",
    "\n",
    "- **strategy 2 one sequence per country**:\n",
    "    - **split X train, set** \n",
    "    - **Pad sequences**\n",
    "    - **create one csv per country**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848eaa-a889-47ee-972c-e9f2e54214cc",
   "metadata": {},
   "source": [
    "## Training strategies:\n",
    "- Get NB dataset (cleaned) from Alberto & Sumedha\n",
    "- 1/ Indicator in precentage %\n",
    "- 2/ Indicator as categorical labels\n",
    "- Run same RNN model in parallel with Kim & Thomas\n",
    "- Identify best dataset\n",
    "- Parameters to fit:\n",
    "    - increase **nb of sequences**\n",
    "    - train series modulation (ex: [50, 150, 200, 300, 400 nb of days = n_obs]) < take time to compute\n",
    "    - **learning_rate** in Optimizer(parameters)\n",
    "    - model layers architecture (**simple** -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "        > LSTM\n",
    "        > Dense\n",
    "       (> LSTM\n",
    "        > LSTM\n",
    "        > Dense)\n",
    "     >> **try to overfit** the model with the loss (train over val) or (early_stopping)\n",
    "     >> **(X_val, y_val)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4cdbb-56aa-4522-84a6-833471a7b34a",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775435d-8b7a-4742-bcb2-a76e1c2ffb1c",
   "metadata": {},
   "source": [
    "# RNN model Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648961ed-ee19-4342-b137-8f5029082404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "n_seq = 200 ## nb of sequences (samples)\n",
    "n_obs = 61 # maxi = 96 (stay around 70 or more test_split)\n",
    "n_feat = 20 #  X_train.shape[1] # 20 feature:\n",
    "n_pred = 1 # nb of days where we can predict new daily deaths\n",
    "n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d337-90d7-47b1-a4ee-b508e8731c39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test datasplit the dataset into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 1, 2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('France', split_train=0.7, split_val=0.9)\n",
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4fba2-9562-49eb-9b94-7c5ab57fd163",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create sequences (`X`,`y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e6437-7538-4c29-b8d9-d00d4d804064",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Generates an entire dataset of multiple subsamples with shape $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b0cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence(X, y, X_len, y_len) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length` (eg n_obs).\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    X_y_len = X_len + y_len\n",
    "    print('_len + y_len',  X_len,  y_len)\n",
    "    print('X.shape[0] >= X_y_len', X.shape[0], X_y_len)\n",
    "    if X.shape[0] >= X_y_len:\n",
    "        last_possible = X.shape[0] - X_y_len\n",
    "    else:\n",
    "        last_possible = X.shape[0]\n",
    "        print('X_y_len = ?', X.shape[0])\n",
    "    # How to split sequences? we could do it manually...\n",
    "    print('X.shape[0]', X.shape[0])\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    # X start and y end\n",
    "    X_sample = X[random_start : random_start + X_len]\n",
    "    y_sample = y[random_start + X_len : (random_start + X_y_len)]\n",
    "    print(\"X[random_start : random_start + X_len]\", f\"X[{random_start} : {random_start + X_len}]\")\n",
    "    print(\"y[random_start : random_start + X_y_len]\", f\"y[{random_start} : {(random_start + X_y_len)}]\")\n",
    "    \n",
    "    return np.array(X_sample), np.array(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a54ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1993c868-606c-4d31-ac8d-997bc85d63d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[548 : 609]\n",
      "y[random_start : random_start + X_y_len] y[548 : 610]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((61, 20), (1,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsample sequence\n",
    "(X_sample, y_sample) = subsample_sequence(X_train, y_train, X_len=n_obs, y_len=n_pred)\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb0927-e52b-4d41-8dcb-6ca6f204a1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_X_y(df, n_sequences, length)**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cde1f94-64b6-472d-8007-5a91ec085172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(X, y, X_len, y_len, n_sequences) -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "        X_list.append(xi)\n",
    "        y_list.append(yi)\n",
    "        \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9096e3f-477e-45ae-a9a2-bdb1c8163ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 40, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq_val = n_seq // 5 # number of sequences in test set ?\n",
    "n_seq_test = n_seq // 10 # number of sequences in test set ?\n",
    "n_seq, n_seq_val, n_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1682240c-818e-4e96-9a2a-8b596cd235bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 20), (97,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3af4e7e-d602-4fcd-a36a-942ab4dc0b72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[23 : 84]\n",
      "y[random_start : random_start + X_y_len] y[23 : 85]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[7 : 68]\n",
      "y[random_start : random_start + X_y_len] y[7 : 69]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[9 : 70]\n",
      "y[random_start : random_start + X_y_len] y[9 : 71]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[25 : 86]\n",
      "y[random_start : random_start + X_y_len] y[25 : 87]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[34 : 95]\n",
      "y[random_start : random_start + X_y_len] y[34 : 96]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 63]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[12 : 73]\n",
      "y[random_start : random_start + X_y_len] y[12 : 74]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[16 : 77]\n",
      "y[random_start : random_start + X_y_len] y[16 : 78]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[14 : 75]\n",
      "y[random_start : random_start + X_y_len] y[14 : 76]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[22 : 83]\n",
      "y[random_start : random_start + X_y_len] y[22 : 84]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[24 : 85]\n",
      "y[random_start : random_start + X_y_len] y[24 : 86]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[0 : 61]\n",
      "y[random_start : random_start + X_y_len] y[0 : 62]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[33 : 94]\n",
      "y[random_start : random_start + X_y_len] y[33 : 95]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[21 : 82]\n",
      "y[random_start : random_start + X_y_len] y[21 : 83]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[33 : 94]\n",
      "y[random_start : random_start + X_y_len] y[33 : 95]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 68]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 73]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[10 : 71]\n",
      "y[random_start : random_start + X_y_len] y[10 : 72]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 68]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 97 62\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 68]\n",
      "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20, 61, 20), (20, 1), 20, 61, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_X_y(X_test, y_test, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n",
    "print('X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat')\n",
    "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497196a7-2d6a-4663-82a9-c8873c4ad0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[121 : 182]\n",
      "y[random_start : random_start + X_y_len] y[121 : 183]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[116 : 177]\n",
      "y[random_start : random_start + X_y_len] y[116 : 178]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[123 : 184]\n",
      "y[random_start : random_start + X_y_len] y[123 : 185]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[57 : 118]\n",
      "y[random_start : random_start + X_y_len] y[57 : 119]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[93 : 154]\n",
      "y[random_start : random_start + X_y_len] y[93 : 155]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[59 : 120]\n",
      "y[random_start : random_start + X_y_len] y[59 : 121]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[74 : 135]\n",
      "y[random_start : random_start + X_y_len] y[74 : 136]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[15 : 76]\n",
      "y[random_start : random_start + X_y_len] y[15 : 77]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[91 : 152]\n",
      "y[random_start : random_start + X_y_len] y[91 : 153]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[123 : 184]\n",
      "y[random_start : random_start + X_y_len] y[123 : 185]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[3 : 64]\n",
      "y[random_start : random_start + X_y_len] y[3 : 65]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[85 : 146]\n",
      "y[random_start : random_start + X_y_len] y[85 : 147]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[120 : 181]\n",
      "y[random_start : random_start + X_y_len] y[120 : 182]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[55 : 116]\n",
      "y[random_start : random_start + X_y_len] y[55 : 117]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[38 : 99]\n",
      "y[random_start : random_start + X_y_len] y[38 : 100]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[71 : 132]\n",
      "y[random_start : random_start + X_y_len] y[71 : 133]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[123 : 184]\n",
      "y[random_start : random_start + X_y_len] y[123 : 185]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[89 : 150]\n",
      "y[random_start : random_start + X_y_len] y[89 : 151]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[7 : 68]\n",
      "y[random_start : random_start + X_y_len] y[7 : 69]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[5 : 66]\n",
      "y[random_start : random_start + X_y_len] y[5 : 67]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[60 : 121]\n",
      "y[random_start : random_start + X_y_len] y[60 : 122]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 63]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[58 : 119]\n",
      "y[random_start : random_start + X_y_len] y[58 : 120]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[124 : 185]\n",
      "y[random_start : random_start + X_y_len] y[124 : 186]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[122 : 183]\n",
      "y[random_start : random_start + X_y_len] y[122 : 184]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[39 : 100]\n",
      "y[random_start : random_start + X_y_len] y[39 : 101]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[36 : 97]\n",
      "y[random_start : random_start + X_y_len] y[36 : 98]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[113 : 174]\n",
      "y[random_start : random_start + X_y_len] y[113 : 175]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[127 : 188]\n",
      "y[random_start : random_start + X_y_len] y[127 : 189]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[70 : 131]\n",
      "y[random_start : random_start + X_y_len] y[70 : 132]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[61 : 122]\n",
      "y[random_start : random_start + X_y_len] y[61 : 123]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[35 : 96]\n",
      "y[random_start : random_start + X_y_len] y[35 : 97]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[38 : 99]\n",
      "y[random_start : random_start + X_y_len] y[38 : 100]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[111 : 172]\n",
      "y[random_start : random_start + X_y_len] y[111 : 173]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[56 : 117]\n",
      "y[random_start : random_start + X_y_len] y[56 : 118]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[18 : 79]\n",
      "y[random_start : random_start + X_y_len] y[18 : 80]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[71 : 132]\n",
      "y[random_start : random_start + X_y_len] y[71 : 133]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[14 : 75]\n",
      "y[random_start : random_start + X_y_len] y[14 : 76]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[69 : 130]\n",
      "y[random_start : random_start + X_y_len] y[69 : 131]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 192 62\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[12 : 73]\n",
      "y[random_start : random_start + X_y_len] y[12 : 74]\n",
      "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((40, 61, 20), (40, 1), 200, 40, 20, 61, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = get_X_y(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "print('X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat')\n",
    "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a422c04-e3ef-4daf-b870-060c559ad2bb",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('United States', split_train=0.7, split_val=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e2fa4-2cf1-4ad1-bd39-6b48cd2d478d",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_test, y_test, X_len=n_obs, y_len=n_pred, n_seq_train=n_seq_test, X_val=X_val, y_val=y_val)\n",
    "X_train.shape, y_train.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7826c700-4273-4cd3-b7d7-4c3cca2b3582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[409 : 470]\n",
      "y[random_start : random_start + X_y_len] y[409 : 471]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[315 : 376]\n",
      "y[random_start : random_start + X_y_len] y[315 : 377]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[115 : 176]\n",
      "y[random_start : random_start + X_y_len] y[115 : 177]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[63 : 124]\n",
      "y[random_start : random_start + X_y_len] y[63 : 125]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[253 : 314]\n",
      "y[random_start : random_start + X_y_len] y[253 : 315]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[55 : 116]\n",
      "y[random_start : random_start + X_y_len] y[55 : 117]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[353 : 414]\n",
      "y[random_start : random_start + X_y_len] y[353 : 415]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[560 : 621]\n",
      "y[random_start : random_start + X_y_len] y[560 : 622]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[571 : 632]\n",
      "y[random_start : random_start + X_y_len] y[571 : 633]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[506 : 567]\n",
      "y[random_start : random_start + X_y_len] y[506 : 568]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[387 : 448]\n",
      "y[random_start : random_start + X_y_len] y[387 : 449]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[340 : 401]\n",
      "y[random_start : random_start + X_y_len] y[340 : 402]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[576 : 637]\n",
      "y[random_start : random_start + X_y_len] y[576 : 638]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[456 : 517]\n",
      "y[random_start : random_start + X_y_len] y[456 : 518]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[528 : 589]\n",
      "y[random_start : random_start + X_y_len] y[528 : 590]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[479 : 540]\n",
      "y[random_start : random_start + X_y_len] y[479 : 541]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[75 : 136]\n",
      "y[random_start : random_start + X_y_len] y[75 : 137]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[516 : 577]\n",
      "y[random_start : random_start + X_y_len] y[516 : 578]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[487 : 548]\n",
      "y[random_start : random_start + X_y_len] y[487 : 549]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[330 : 391]\n",
      "y[random_start : random_start + X_y_len] y[330 : 392]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[237 : 298]\n",
      "y[random_start : random_start + X_y_len] y[237 : 299]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[179 : 240]\n",
      "y[random_start : random_start + X_y_len] y[179 : 241]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[235 : 296]\n",
      "y[random_start : random_start + X_y_len] y[235 : 297]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[118 : 179]\n",
      "y[random_start : random_start + X_y_len] y[118 : 180]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[18 : 79]\n",
      "y[random_start : random_start + X_y_len] y[18 : 80]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[24 : 85]\n",
      "y[random_start : random_start + X_y_len] y[24 : 86]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[542 : 603]\n",
      "y[random_start : random_start + X_y_len] y[542 : 604]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[475 : 536]\n",
      "y[random_start : random_start + X_y_len] y[475 : 537]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[166 : 227]\n",
      "y[random_start : random_start + X_y_len] y[166 : 228]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[308 : 369]\n",
      "y[random_start : random_start + X_y_len] y[308 : 370]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[582 : 643]\n",
      "y[random_start : random_start + X_y_len] y[582 : 644]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[347 : 408]\n",
      "y[random_start : random_start + X_y_len] y[347 : 409]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[397 : 458]\n",
      "y[random_start : random_start + X_y_len] y[397 : 459]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[362 : 423]\n",
      "y[random_start : random_start + X_y_len] y[362 : 424]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[482 : 543]\n",
      "y[random_start : random_start + X_y_len] y[482 : 544]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[82 : 143]\n",
      "y[random_start : random_start + X_y_len] y[82 : 144]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[86 : 147]\n",
      "y[random_start : random_start + X_y_len] y[86 : 148]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[107 : 168]\n",
      "y[random_start : random_start + X_y_len] y[107 : 169]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[275 : 336]\n",
      "y[random_start : random_start + X_y_len] y[275 : 337]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[225 : 286]\n",
      "y[random_start : random_start + X_y_len] y[225 : 287]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[436 : 497]\n",
      "y[random_start : random_start + X_y_len] y[436 : 498]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[602 : 663]\n",
      "y[random_start : random_start + X_y_len] y[602 : 664]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[421 : 482]\n",
      "y[random_start : random_start + X_y_len] y[421 : 483]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[349 : 410]\n",
      "y[random_start : random_start + X_y_len] y[349 : 411]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[271 : 332]\n",
      "y[random_start : random_start + X_y_len] y[271 : 333]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[526 : 587]\n",
      "y[random_start : random_start + X_y_len] y[526 : 588]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[121 : 182]\n",
      "y[random_start : random_start + X_y_len] y[121 : 183]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[9 : 70]\n",
      "y[random_start : random_start + X_y_len] y[9 : 71]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[202 : 263]\n",
      "y[random_start : random_start + X_y_len] y[202 : 264]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[427 : 488]\n",
      "y[random_start : random_start + X_y_len] y[427 : 489]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[229 : 290]\n",
      "y[random_start : random_start + X_y_len] y[229 : 291]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[258 : 319]\n",
      "y[random_start : random_start + X_y_len] y[258 : 320]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[300 : 361]\n",
      "y[random_start : random_start + X_y_len] y[300 : 362]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[333 : 394]\n",
      "y[random_start : random_start + X_y_len] y[333 : 395]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[195 : 256]\n",
      "y[random_start : random_start + X_y_len] y[195 : 257]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[190 : 251]\n",
      "y[random_start : random_start + X_y_len] y[190 : 252]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[37 : 98]\n",
      "y[random_start : random_start + X_y_len] y[37 : 99]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[606 : 667]\n",
      "y[random_start : random_start + X_y_len] y[606 : 668]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[349 : 410]\n",
      "y[random_start : random_start + X_y_len] y[349 : 411]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[302 : 363]\n",
      "y[random_start : random_start + X_y_len] y[302 : 364]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[223 : 284]\n",
      "y[random_start : random_start + X_y_len] y[223 : 285]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[434 : 495]\n",
      "y[random_start : random_start + X_y_len] y[434 : 496]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[528 : 589]\n",
      "y[random_start : random_start + X_y_len] y[528 : 590]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[263 : 324]\n",
      "y[random_start : random_start + X_y_len] y[263 : 325]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[88 : 149]\n",
      "y[random_start : random_start + X_y_len] y[88 : 150]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[288 : 349]\n",
      "y[random_start : random_start + X_y_len] y[288 : 350]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[404 : 465]\n",
      "y[random_start : random_start + X_y_len] y[404 : 466]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[76 : 137]\n",
      "y[random_start : random_start + X_y_len] y[76 : 138]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[33 : 94]\n",
      "y[random_start : random_start + X_y_len] y[33 : 95]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[389 : 450]\n",
      "y[random_start : random_start + X_y_len] y[389 : 451]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[306 : 367]\n",
      "y[random_start : random_start + X_y_len] y[306 : 368]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[381 : 442]\n",
      "y[random_start : random_start + X_y_len] y[381 : 443]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[161 : 222]\n",
      "y[random_start : random_start + X_y_len] y[161 : 223]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[566 : 627]\n",
      "y[random_start : random_start + X_y_len] y[566 : 628]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[430 : 491]\n",
      "y[random_start : random_start + X_y_len] y[430 : 492]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[389 : 450]\n",
      "y[random_start : random_start + X_y_len] y[389 : 451]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[607 : 668]\n",
      "y[random_start : random_start + X_y_len] y[607 : 669]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[234 : 295]\n",
      "y[random_start : random_start + X_y_len] y[234 : 296]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[603 : 664]\n",
      "y[random_start : random_start + X_y_len] y[603 : 665]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[417 : 478]\n",
      "y[random_start : random_start + X_y_len] y[417 : 479]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[607 : 668]\n",
      "y[random_start : random_start + X_y_len] y[607 : 669]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[555 : 616]\n",
      "y[random_start : random_start + X_y_len] y[555 : 617]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[190 : 251]\n",
      "y[random_start : random_start + X_y_len] y[190 : 252]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[26 : 87]\n",
      "y[random_start : random_start + X_y_len] y[26 : 88]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[77 : 138]\n",
      "y[random_start : random_start + X_y_len] y[77 : 139]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[392 : 453]\n",
      "y[random_start : random_start + X_y_len] y[392 : 454]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[246 : 307]\n",
      "y[random_start : random_start + X_y_len] y[246 : 308]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[404 : 465]\n",
      "y[random_start : random_start + X_y_len] y[404 : 466]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[343 : 404]\n",
      "y[random_start : random_start + X_y_len] y[343 : 405]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[36 : 97]\n",
      "y[random_start : random_start + X_y_len] y[36 : 98]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[439 : 500]\n",
      "y[random_start : random_start + X_y_len] y[439 : 501]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[425 : 486]\n",
      "y[random_start : random_start + X_y_len] y[425 : 487]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[117 : 178]\n",
      "y[random_start : random_start + X_y_len] y[117 : 179]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[63 : 124]\n",
      "y[random_start : random_start + X_y_len] y[63 : 125]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[335 : 396]\n",
      "y[random_start : random_start + X_y_len] y[335 : 397]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[173 : 234]\n",
      "y[random_start : random_start + X_y_len] y[173 : 235]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[580 : 641]\n",
      "y[random_start : random_start + X_y_len] y[580 : 642]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[84 : 145]\n",
      "y[random_start : random_start + X_y_len] y[84 : 146]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[598 : 659]\n",
      "y[random_start : random_start + X_y_len] y[598 : 660]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[117 : 178]\n",
      "y[random_start : random_start + X_y_len] y[117 : 179]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[532 : 593]\n",
      "y[random_start : random_start + X_y_len] y[532 : 594]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[463 : 524]\n",
      "y[random_start : random_start + X_y_len] y[463 : 525]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[380 : 441]\n",
      "y[random_start : random_start + X_y_len] y[380 : 442]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[526 : 587]\n",
      "y[random_start : random_start + X_y_len] y[526 : 588]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[114 : 175]\n",
      "y[random_start : random_start + X_y_len] y[114 : 176]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[375 : 436]\n",
      "y[random_start : random_start + X_y_len] y[375 : 437]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[403 : 464]\n",
      "y[random_start : random_start + X_y_len] y[403 : 465]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[472 : 533]\n",
      "y[random_start : random_start + X_y_len] y[472 : 534]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[228 : 289]\n",
      "y[random_start : random_start + X_y_len] y[228 : 290]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[214 : 275]\n",
      "y[random_start : random_start + X_y_len] y[214 : 276]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[562 : 623]\n",
      "y[random_start : random_start + X_y_len] y[562 : 624]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[304 : 365]\n",
      "y[random_start : random_start + X_y_len] y[304 : 366]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[15 : 76]\n",
      "y[random_start : random_start + X_y_len] y[15 : 77]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[431 : 492]\n",
      "y[random_start : random_start + X_y_len] y[431 : 493]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 73]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[77 : 138]\n",
      "y[random_start : random_start + X_y_len] y[77 : 139]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[437 : 498]\n",
      "y[random_start : random_start + X_y_len] y[437 : 499]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[43 : 104]\n",
      "y[random_start : random_start + X_y_len] y[43 : 105]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[235 : 296]\n",
      "y[random_start : random_start + X_y_len] y[235 : 297]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[335 : 396]\n",
      "y[random_start : random_start + X_y_len] y[335 : 397]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[68 : 129]\n",
      "y[random_start : random_start + X_y_len] y[68 : 130]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[52 : 113]\n",
      "y[random_start : random_start + X_y_len] y[52 : 114]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[167 : 228]\n",
      "y[random_start : random_start + X_y_len] y[167 : 229]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[85 : 146]\n",
      "y[random_start : random_start + X_y_len] y[85 : 147]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[480 : 541]\n",
      "y[random_start : random_start + X_y_len] y[480 : 542]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[183 : 244]\n",
      "y[random_start : random_start + X_y_len] y[183 : 245]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[0 : 61]\n",
      "y[random_start : random_start + X_y_len] y[0 : 62]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[8 : 69]\n",
      "y[random_start : random_start + X_y_len] y[8 : 70]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[484 : 545]\n",
      "y[random_start : random_start + X_y_len] y[484 : 546]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[282 : 343]\n",
      "y[random_start : random_start + X_y_len] y[282 : 344]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[37 : 98]\n",
      "y[random_start : random_start + X_y_len] y[37 : 99]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[242 : 303]\n",
      "y[random_start : random_start + X_y_len] y[242 : 304]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[410 : 471]\n",
      "y[random_start : random_start + X_y_len] y[410 : 472]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[312 : 373]\n",
      "y[random_start : random_start + X_y_len] y[312 : 374]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[196 : 257]\n",
      "y[random_start : random_start + X_y_len] y[196 : 258]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[412 : 473]\n",
      "y[random_start : random_start + X_y_len] y[412 : 474]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[218 : 279]\n",
      "y[random_start : random_start + X_y_len] y[218 : 280]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[473 : 534]\n",
      "y[random_start : random_start + X_y_len] y[473 : 535]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[81 : 142]\n",
      "y[random_start : random_start + X_y_len] y[81 : 143]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[370 : 431]\n",
      "y[random_start : random_start + X_y_len] y[370 : 432]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[294 : 355]\n",
      "y[random_start : random_start + X_y_len] y[294 : 356]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[337 : 398]\n",
      "y[random_start : random_start + X_y_len] y[337 : 399]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[117 : 178]\n",
      "y[random_start : random_start + X_y_len] y[117 : 179]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[567 : 628]\n",
      "y[random_start : random_start + X_y_len] y[567 : 629]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[144 : 205]\n",
      "y[random_start : random_start + X_y_len] y[144 : 206]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[321 : 382]\n",
      "y[random_start : random_start + X_y_len] y[321 : 383]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[122 : 183]\n",
      "y[random_start : random_start + X_y_len] y[122 : 184]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[525 : 586]\n",
      "y[random_start : random_start + X_y_len] y[525 : 587]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[118 : 179]\n",
      "y[random_start : random_start + X_y_len] y[118 : 180]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[426 : 487]\n",
      "y[random_start : random_start + X_y_len] y[426 : 488]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[585 : 646]\n",
      "y[random_start : random_start + X_y_len] y[585 : 647]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[382 : 443]\n",
      "y[random_start : random_start + X_y_len] y[382 : 444]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[312 : 373]\n",
      "y[random_start : random_start + X_y_len] y[312 : 374]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[598 : 659]\n",
      "y[random_start : random_start + X_y_len] y[598 : 660]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[118 : 179]\n",
      "y[random_start : random_start + X_y_len] y[118 : 180]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[247 : 308]\n",
      "y[random_start : random_start + X_y_len] y[247 : 309]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[468 : 529]\n",
      "y[random_start : random_start + X_y_len] y[468 : 530]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[271 : 332]\n",
      "y[random_start : random_start + X_y_len] y[271 : 333]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[577 : 638]\n",
      "y[random_start : random_start + X_y_len] y[577 : 639]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[268 : 329]\n",
      "y[random_start : random_start + X_y_len] y[268 : 330]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[358 : 419]\n",
      "y[random_start : random_start + X_y_len] y[358 : 420]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[546 : 607]\n",
      "y[random_start : random_start + X_y_len] y[546 : 608]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[266 : 327]\n",
      "y[random_start : random_start + X_y_len] y[266 : 328]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[429 : 490]\n",
      "y[random_start : random_start + X_y_len] y[429 : 491]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[550 : 611]\n",
      "y[random_start : random_start + X_y_len] y[550 : 612]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[162 : 223]\n",
      "y[random_start : random_start + X_y_len] y[162 : 224]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[562 : 623]\n",
      "y[random_start : random_start + X_y_len] y[562 : 624]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[466 : 527]\n",
      "y[random_start : random_start + X_y_len] y[466 : 528]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[229 : 290]\n",
      "y[random_start : random_start + X_y_len] y[229 : 291]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[111 : 172]\n",
      "y[random_start : random_start + X_y_len] y[111 : 173]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[47 : 108]\n",
      "y[random_start : random_start + X_y_len] y[47 : 109]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[451 : 512]\n",
      "y[random_start : random_start + X_y_len] y[451 : 513]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[121 : 182]\n",
      "y[random_start : random_start + X_y_len] y[121 : 183]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[420 : 481]\n",
      "y[random_start : random_start + X_y_len] y[420 : 482]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[44 : 105]\n",
      "y[random_start : random_start + X_y_len] y[44 : 106]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[399 : 460]\n",
      "y[random_start : random_start + X_y_len] y[399 : 461]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[329 : 390]\n",
      "y[random_start : random_start + X_y_len] y[329 : 391]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[424 : 485]\n",
      "y[random_start : random_start + X_y_len] y[424 : 486]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[254 : 315]\n",
      "y[random_start : random_start + X_y_len] y[254 : 316]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[535 : 596]\n",
      "y[random_start : random_start + X_y_len] y[535 : 597]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[506 : 567]\n",
      "y[random_start : random_start + X_y_len] y[506 : 568]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[475 : 536]\n",
      "y[random_start : random_start + X_y_len] y[475 : 537]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[351 : 412]\n",
      "y[random_start : random_start + X_y_len] y[351 : 413]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[2 : 63]\n",
      "y[random_start : random_start + X_y_len] y[2 : 64]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[161 : 222]\n",
      "y[random_start : random_start + X_y_len] y[161 : 223]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[5 : 66]\n",
      "y[random_start : random_start + X_y_len] y[5 : 67]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[264 : 325]\n",
      "y[random_start : random_start + X_y_len] y[264 : 326]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[277 : 338]\n",
      "y[random_start : random_start + X_y_len] y[277 : 339]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[223 : 284]\n",
      "y[random_start : random_start + X_y_len] y[223 : 285]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[159 : 220]\n",
      "y[random_start : random_start + X_y_len] y[159 : 221]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[228 : 289]\n",
      "y[random_start : random_start + X_y_len] y[228 : 290]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[53 : 114]\n",
      "y[random_start : random_start + X_y_len] y[53 : 115]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[355 : 416]\n",
      "y[random_start : random_start + X_y_len] y[355 : 417]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[392 : 453]\n",
      "y[random_start : random_start + X_y_len] y[392 : 454]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[461 : 522]\n",
      "y[random_start : random_start + X_y_len] y[461 : 523]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[173 : 234]\n",
      "y[random_start : random_start + X_y_len] y[173 : 235]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[458 : 519]\n",
      "y[random_start : random_start + X_y_len] y[458 : 520]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[394 : 455]\n",
      "y[random_start : random_start + X_y_len] y[394 : 456]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[448 : 509]\n",
      "y[random_start : random_start + X_y_len] y[448 : 510]\n",
      "_len + y_len 61 1\n",
      "X.shape[0] >= X_y_len 672 62\n",
      "X.shape[0] 672\n",
      "X[random_start : random_start + X_len] X[427 : 488]\n",
      "y[random_start : random_start + X_y_len] y[427 : 489]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200, 61, 20), (200, 1), 200, 61, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n",
    "X_train.shape, y_train.shape, n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2a397-4b64-4358-8355-2a243e990afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "X_val.shape, y_val.shape, n_seq_val, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec27cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "661159e6-4eb3-48cc-b11d-fff40fb81a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Train set\n",
    "# def train_rnn_model(model, patience=7, epochs=200):\n",
    "#     es = EarlyStopping(monitor = 'val_loss',\n",
    "#                     patience = patience,\n",
    "#                     verbose = 1,\n",
    "#                     restore_best_weights = True)\n",
    "#     # Fit\n",
    "#     history =  model.fit(X_train, y_train, \n",
    "#             validation_split=0.1, # Auto split for validation data\n",
    "#             batch_size = 16,\n",
    "#             epochs = epochs,\n",
    "#             callbacks = [es],\n",
    "#             verbose=1)\n",
    "#     return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe229ed-38be-485e-9314-5a3c04da8847",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### How to split sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49128bf-f5e8-4319-8635-fa1f63c29ceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- randomly or\n",
    "\n",
    "- manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867c99-9dd9-4dc5-9cda-de741bf57ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **train_rnn_model(model, patience=2, epochs=200):**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ece011-197d-439e-b405-ee2370c9773d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RNN model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc24c32-f615-4696-b4e0-01e7d7281ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=7, epochs=200):\n",
    "    \"\"\" function that train a RNN model with hyperparameters:\n",
    "    - patience by default 7 to early stop\n",
    "    - epochs by default 200 to train over several epochs\n",
    "    - validation data by default (X_val, y_val)\n",
    "    \"\"\"\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "             # Auto split for validation data\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f844bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7bd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 01:43:18.245205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-14 01:43:18.245438: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-14 01:43:18.245551: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-S0MTM0QT): /proc/driver/nvidia/version does not exist\n",
      "2022-09-14 01:43:18.246795: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,501\n",
      "Trainable params: 3,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    " \n",
    "# Normalization layer\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "\n",
    "# 1. The Architecture\n",
    "\"\"\"   - model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model = Sequential()\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model.add(LSTM(units=20, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model.add(Dense(n_pred, activation = 'linear'))\n",
    "\n",
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8cd997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 5s 156ms/step - loss: 6473532928.0000 - mape: 99.9936 - val_loss: 19251621888.0000 - val_mape: 99.9883\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 32007528448.0000 - mape: 158.3742 - val_loss: 19229872128.0000 - val_mape: 99.9310\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 6327138816.0000 - mape: 96.6764 - val_loss: 19227764736.0000 - val_mape: 99.9256\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 6295217152.0000 - mape: 96.1397 - val_loss: 19225155584.0000 - val_mape: 99.9184\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 6212145664.0000 - mape: 94.7810 - val_loss: 19229542400.0000 - val_mape: 99.9300\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5555578880.0000 - mape: 86.1647 - val_loss: 19201193984.0000 - val_mape: 99.8559\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 34273894400.0000 - mape: 171.9991 - val_loss: 19215695872.0000 - val_mape: 99.8943\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 6179503616.0000 - mape: 96.9627 - val_loss: 19103772672.0000 - val_mape: 99.6035\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 6043121664.0000 - mape: 95.7988 - val_loss: 18958198784.0000 - val_mape: 99.2259\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 5447176704.0000 - mape: 89.8383 - val_loss: 18823913472.0000 - val_mape: 98.8771\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 312042946560.0000 - mape: 263.2553 - val_loss: 18661832704.0000 - val_mape: 98.4439\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 2953552640.0000 - mape: 59.0377 - val_loss: 18870259712.0000 - val_mape: 99.0060\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 2846938624.0000 - mape: 57.2094 - val_loss: 18522415104.0000 - val_mape: 98.0834\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 2679161088.0000 - mape: 54.4019 - val_loss: 18371596288.0000 - val_mape: 97.6268\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 2359398656.0000 - mape: 48.6519 - val_loss: 17418479616.0000 - val_mape: 94.6544\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 2014880384.0000 - mape: 44.0041 - val_loss: 15469305856.0000 - val_mape: 88.5814\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 1822599296.0000 - mape: 41.8469 - val_loss: 8314496512.0000 - val_mape: 61.3490\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 1517706752.0000 - mape: 40.2039 - val_loss: 9192647680.0000 - val_mape: 52.2515\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 1525054464.0000 - mape: 44.9719 - val_loss: 937619554304.0000 - val_mape: 411.2048\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 2818004736.0000 - mape: 54.1659 - val_loss: 526298841088.0000 - val_mape: 393.8946\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluating\n",
    "history = train_rnn_model(rnn_model, epochs=200, patience=3)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e336f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    99719.171875\n",
       "dtype: float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Prediction\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 2 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_2 = Sequential()\n",
    "rnn_model_2.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_2.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_2.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_2.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_2.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b589e",
   "metadata": {},
   "source": [
    "## RNN model 2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e03d90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model_2(rnn_model_2, patience=20, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_2.fit(X_train,\n",
    "            y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901218-eb1c-4d85-b40e-9beda6c3c548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model 2 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db1267f1-8a86-4f8b-82f5-01478bc0d350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.526006\n",
       "dtype: float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 2 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,482\n",
      "Trainable params: 6,441\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_2.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571254c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 8s 205ms/step - loss: 6473894400.0000 - mape: 99.9994 - val_loss: 19255900160.0000 - val_mape: 99.9995\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 6473641984.0000 - mape: 99.9982 - val_loss: 19255980032.0000 - val_mape: 99.9996\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 6473327104.0000 - mape: 99.9960 - val_loss: 19256070144.0000 - val_mape: 99.9999\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 6473032192.0000 - mape: 99.9933 - val_loss: 19256272896.0000 - val_mape: 100.0004\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 6472750080.0000 - mape: 99.9897 - val_loss: 19256297472.0000 - val_mape: 100.0005\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 6472491520.0000 - mape: 99.9859 - val_loss: 19256303616.0000 - val_mape: 100.0005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_2, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78dfb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.702928\n",
       "dtype: float32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1950356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 3, LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2ad1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(20, activation = 'relu'))\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae1226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47f878a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,482\n",
      "Trainable params: 6,441\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b737ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 10s 254ms/step - loss: 6508276736.0000 - mape: 99.9994 - val_loss: 6164699648.0000 - val_mape: 99.9978\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6508140544.0000 - mape: 99.9984 - val_loss: 6164560896.0000 - val_mape: 99.9964\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6507987456.0000 - mape: 99.9972 - val_loss: 6164376576.0000 - val_mape: 99.9946\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6507793920.0000 - mape: 99.9955 - val_loss: 6164177920.0000 - val_mape: 99.9918\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6507608064.0000 - mape: 99.9936 - val_loss: 6164004864.0000 - val_mape: 99.9896\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6507442176.0000 - mape: 99.9918 - val_loss: 6163833344.0000 - val_mape: 99.9876\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6507269120.0000 - mape: 99.9895 - val_loss: 6163668992.0000 - val_mape: 99.9857\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6507101696.0000 - mape: 99.9875 - val_loss: 6163517440.0000 - val_mape: 99.9839\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6506949120.0000 - mape: 99.9859 - val_loss: 6163371008.0000 - val_mape: 99.9820\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6506798592.0000 - mape: 99.9843 - val_loss: 6163215872.0000 - val_mape: 99.9801\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6506636800.0000 - mape: 99.9827 - val_loss: 6163060224.0000 - val_mape: 99.9781\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6506487808.0000 - mape: 99.9811 - val_loss: 6162910208.0000 - val_mape: 99.9763\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6506313728.0000 - mape: 99.9795 - val_loss: 6162750464.0000 - val_mape: 99.9744\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6506149888.0000 - mape: 99.9777 - val_loss: 6162589184.0000 - val_mape: 99.9724\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6505982464.0000 - mape: 99.9760 - val_loss: 6162425344.0000 - val_mape: 99.9696\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6505812480.0000 - mape: 99.9740 - val_loss: 6162258944.0000 - val_mape: 99.9660\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6505629696.0000 - mape: 99.9713 - val_loss: 6162074624.0000 - val_mape: 99.9601\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6505377280.0000 - mape: 99.9647 - val_loss: 6161884160.0000 - val_mape: 99.9533\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6505160192.0000 - mape: 99.9560 - val_loss: 6161664512.0000 - val_mape: 99.8708\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6504957952.0000 - mape: 99.9402 - val_loss: 6161454080.0000 - val_mape: 99.8597\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6504869376.0000 - mape: 99.9360 - val_loss: 6161263616.0000 - val_mape: 99.8512\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6504671744.0000 - mape: 99.9321 - val_loss: 6161063936.0000 - val_mape: 99.8424\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6504468992.0000 - mape: 99.9284 - val_loss: 6160872448.0000 - val_mape: 99.8341\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6504377856.0000 - mape: 99.9251 - val_loss: 6160659968.0000 - val_mape: 99.8251\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6504167424.0000 - mape: 99.9208 - val_loss: 6160439296.0000 - val_mape: 99.8156\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 6503949312.0000 - mape: 99.9169 - val_loss: 6160222208.0000 - val_mape: 99.8064\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6503733760.0000 - mape: 99.9133 - val_loss: 6160000512.0000 - val_mape: 99.7970\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6503341056.0000 - mape: 99.9078 - val_loss: 6159776768.0000 - val_mape: 99.7877\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6502926848.0000 - mape: 99.9030 - val_loss: 6159542784.0000 - val_mape: 99.7778\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6502680064.0000 - mape: 99.8982 - val_loss: 6159306752.0000 - val_mape: 99.7679\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6502430208.0000 - mape: 99.8941 - val_loss: 6159063552.0000 - val_mape: 99.7577\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6502171136.0000 - mape: 99.8898 - val_loss: 6158814720.0000 - val_mape: 99.7472\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6501907968.0000 - mape: 99.8850 - val_loss: 6158558208.0000 - val_mape: 99.7364\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 6501639168.0000 - mape: 99.8797 - val_loss: 6158313472.0000 - val_mape: 99.7261\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6501372928.0000 - mape: 99.8751 - val_loss: 6158049792.0000 - val_mape: 99.7149\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6501096448.0000 - mape: 99.8700 - val_loss: 6157776384.0000 - val_mape: 99.7034\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6500810752.0000 - mape: 99.8652 - val_loss: 6157510656.0000 - val_mape: 99.6922\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6500526592.0000 - mape: 99.8598 - val_loss: 6157228032.0000 - val_mape: 99.6802\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 6500228096.0000 - mape: 99.8553 - val_loss: 6156950528.0000 - val_mape: 99.6686\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6499938304.0000 - mape: 99.8496 - val_loss: 6156674048.0000 - val_mape: 99.6569\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6499641344.0000 - mape: 99.8437 - val_loss: 6156391424.0000 - val_mape: 99.6450\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6499338240.0000 - mape: 99.8391 - val_loss: 6156087808.0000 - val_mape: 99.6322\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6499021824.0000 - mape: 99.8331 - val_loss: 6155787264.0000 - val_mape: 99.6196\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6498702848.0000 - mape: 99.8274 - val_loss: 6155482112.0000 - val_mape: 99.6067\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6498377728.0000 - mape: 99.8219 - val_loss: 6155177984.0000 - val_mape: 99.5940\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6498057728.0000 - mape: 99.8158 - val_loss: 6154858496.0000 - val_mape: 99.5806\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6497719808.0000 - mape: 99.8098 - val_loss: 6154543616.0000 - val_mape: 99.5673\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6497387520.0000 - mape: 99.8033 - val_loss: 6154226688.0000 - val_mape: 99.5540\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6497045504.0000 - mape: 99.7976 - val_loss: 6153883136.0000 - val_mape: 99.5395\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6496690176.0000 - mape: 99.7916 - val_loss: 6153556992.0000 - val_mape: 99.5258\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6496339968.0000 - mape: 99.7856 - val_loss: 6153226240.0000 - val_mape: 99.5120\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6495986688.0000 - mape: 99.7793 - val_loss: 6152866816.0000 - val_mape: 99.4968\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6495616512.0000 - mape: 99.7721 - val_loss: 6152530944.0000 - val_mape: 99.4827\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6495256064.0000 - mape: 99.7663 - val_loss: 6152173056.0000 - val_mape: 99.4677\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6494883328.0000 - mape: 99.7586 - val_loss: 6151821824.0000 - val_mape: 99.4529\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6494507520.0000 - mape: 99.7522 - val_loss: 6151458304.0000 - val_mape: 99.4376\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6494121472.0000 - mape: 99.7457 - val_loss: 6151087104.0000 - val_mape: 99.4220\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6493730816.0000 - mape: 99.7383 - val_loss: 6150706176.0000 - val_mape: 99.4060\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6493336064.0000 - mape: 99.7312 - val_loss: 6150336512.0000 - val_mape: 99.3905\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6492937728.0000 - mape: 99.7244 - val_loss: 6149958144.0000 - val_mape: 99.3745\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6492535808.0000 - mape: 99.7168 - val_loss: 6149563392.0000 - val_mape: 99.3580\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6492120064.0000 - mape: 99.7104 - val_loss: 6149168128.0000 - val_mape: 99.3413\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6491704832.0000 - mape: 99.7022 - val_loss: 6148774912.0000 - val_mape: 99.3248\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6491290624.0000 - mape: 99.6943 - val_loss: 6148381696.0000 - val_mape: 99.3083\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6490868736.0000 - mape: 99.6873 - val_loss: 6147960832.0000 - val_mape: 99.2906\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6490429952.0000 - mape: 99.6795 - val_loss: 6147550208.0000 - val_mape: 99.2733\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6489995776.0000 - mape: 99.6713 - val_loss: 6147134976.0000 - val_mape: 99.2559\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6489556992.0000 - mape: 99.6644 - val_loss: 6146724864.0000 - val_mape: 99.2386\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6489122816.0000 - mape: 99.6555 - val_loss: 6146309120.0000 - val_mape: 99.2211\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6488676352.0000 - mape: 99.6482 - val_loss: 6145858048.0000 - val_mape: 99.2022\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6488211968.0000 - mape: 99.6394 - val_loss: 6145429504.0000 - val_mape: 99.1842\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6487755264.0000 - mape: 99.6312 - val_loss: 6144981504.0000 - val_mape: 99.1653\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6487282176.0000 - mape: 99.6238 - val_loss: 6144529408.0000 - val_mape: 99.1463\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6486805504.0000 - mape: 99.6157 - val_loss: 6144083456.0000 - val_mape: 99.1275\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6486341120.0000 - mape: 99.6069 - val_loss: 6143642624.0000 - val_mape: 99.1090\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6485863936.0000 - mape: 99.5983 - val_loss: 6143173120.0000 - val_mape: 99.0892\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6485376512.0000 - mape: 99.5888 - val_loss: 6142722560.0000 - val_mape: 99.0703\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6484898816.0000 - mape: 99.5798 - val_loss: 6142257152.0000 - val_mape: 99.0507\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6484407296.0000 - mape: 99.5712 - val_loss: 6141788672.0000 - val_mape: 99.0310\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6483904000.0000 - mape: 99.5631 - val_loss: 6141289984.0000 - val_mape: 99.0100\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6483392512.0000 - mape: 99.5530 - val_loss: 6140812800.0000 - val_mape: 98.9899\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6482880000.0000 - mape: 99.5453 - val_loss: 6140328448.0000 - val_mape: 98.9696\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6482370560.0000 - mape: 99.5356 - val_loss: 6139826688.0000 - val_mape: 98.9484\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6481843712.0000 - mape: 99.5259 - val_loss: 6139332096.0000 - val_mape: 98.9276\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6481319424.0000 - mape: 99.5173 - val_loss: 6138819584.0000 - val_mape: 98.9060\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 6480785920.0000 - mape: 99.5072 - val_loss: 6138338304.0000 - val_mape: 98.8858\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6480266752.0000 - mape: 99.4976 - val_loss: 6137811968.0000 - val_mape: 98.8637\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6479719936.0000 - mape: 99.4874 - val_loss: 6137293824.0000 - val_mape: 98.8419\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6479171584.0000 - mape: 99.4785 - val_loss: 6136772096.0000 - val_mape: 98.8199\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6478623232.0000 - mape: 99.4689 - val_loss: 6136251392.0000 - val_mape: 98.7979\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6478080000.0000 - mape: 99.4573 - val_loss: 6135739392.0000 - val_mape: 98.7764\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6477524480.0000 - mape: 99.4489 - val_loss: 6135194112.0000 - val_mape: 98.7534\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6476950528.0000 - mape: 99.4392 - val_loss: 6134660608.0000 - val_mape: 98.7310\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6476391936.0000 - mape: 99.4279 - val_loss: 6134119424.0000 - val_mape: 98.7082\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6475816960.0000 - mape: 99.4179 - val_loss: 6133564928.0000 - val_mape: 98.6848\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6475233280.0000 - mape: 99.4081 - val_loss: 6133006336.0000 - val_mape: 98.6613\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6474642432.0000 - mape: 99.3962 - val_loss: 6132466176.0000 - val_mape: 98.6385\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6474073600.0000 - mape: 99.3861 - val_loss: 6131882496.0000 - val_mape: 98.6140\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6473466880.0000 - mape: 99.3752 - val_loss: 6131320320.0000 - val_mape: 98.5903\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6472870400.0000 - mape: 99.3653 - val_loss: 6130745856.0000 - val_mape: 98.5661\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6472263168.0000 - mape: 99.3545 - val_loss: 6130168832.0000 - val_mape: 98.5418\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6471650816.0000 - mape: 99.3434 - val_loss: 6129572864.0000 - val_mape: 98.5167\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6471036416.0000 - mape: 99.3314 - val_loss: 6129009664.0000 - val_mape: 98.4929\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6470425600.0000 - mape: 99.3208 - val_loss: 6128411648.0000 - val_mape: 98.4678\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6469798912.0000 - mape: 99.3095 - val_loss: 6127811584.0000 - val_mape: 98.4425\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6469170688.0000 - mape: 99.2983 - val_loss: 6127213568.0000 - val_mape: 98.4173\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6468537856.0000 - mape: 99.2875 - val_loss: 6126615552.0000 - val_mape: 98.3921\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6467900928.0000 - mape: 99.2759 - val_loss: 6125984768.0000 - val_mape: 98.3655\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6467242496.0000 - mape: 99.2650 - val_loss: 6125365248.0000 - val_mape: 98.3394\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6466592256.0000 - mape: 99.2531 - val_loss: 6124742144.0000 - val_mape: 98.3131\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6465943552.0000 - mape: 99.2410 - val_loss: 6124142080.0000 - val_mape: 98.2878\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6465295360.0000 - mape: 99.2298 - val_loss: 6123507712.0000 - val_mape: 98.2611\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6464621056.0000 - mape: 99.2176 - val_loss: 6122849792.0000 - val_mape: 98.2333\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6463946240.0000 - mape: 99.2053 - val_loss: 6122235392.0000 - val_mape: 98.2074\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6463282176.0000 - mape: 99.1937 - val_loss: 6121567232.0000 - val_mape: 98.1792\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6462589952.0000 - mape: 99.1814 - val_loss: 6120927232.0000 - val_mape: 98.1522\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6461906944.0000 - mape: 99.1686 - val_loss: 6120270848.0000 - val_mape: 98.1246\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6461219840.0000 - mape: 99.1561 - val_loss: 6119614976.0000 - val_mape: 98.0969\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 6460520448.0000 - mape: 99.1445 - val_loss: 6118927360.0000 - val_mape: 98.0680\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6459804672.0000 - mape: 99.1322 - val_loss: 6118249984.0000 - val_mape: 98.0393\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 6459100672.0000 - mape: 99.1173 - val_loss: 6117630976.0000 - val_mape: 98.0132\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6458427904.0000 - mape: 99.1063 - val_loss: 6116946944.0000 - val_mape: 97.9844\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6457717760.0000 - mape: 99.0937 - val_loss: 6116282368.0000 - val_mape: 97.9563\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6457001984.0000 - mape: 99.0808 - val_loss: 6115572224.0000 - val_mape: 97.9263\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6456265216.0000 - mape: 99.0672 - val_loss: 6114880512.0000 - val_mape: 97.8972\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6455533568.0000 - mape: 99.0540 - val_loss: 6114176000.0000 - val_mape: 97.8675\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6454795776.0000 - mape: 99.0408 - val_loss: 6113479168.0000 - val_mape: 97.8380\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6454053888.0000 - mape: 99.0283 - val_loss: 6112760832.0000 - val_mape: 97.8077\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6453305344.0000 - mape: 99.0137 - val_loss: 6112066048.0000 - val_mape: 97.7784\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6452567552.0000 - mape: 98.9996 - val_loss: 6111360512.0000 - val_mape: 97.7486\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6451807232.0000 - mape: 98.9883 - val_loss: 6110607872.0000 - val_mape: 97.7168\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6451036160.0000 - mape: 98.9747 - val_loss: 6109897728.0000 - val_mape: 97.6869\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6450280448.0000 - mape: 98.9607 - val_loss: 6109171200.0000 - val_mape: 97.6562\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6449509376.0000 - mape: 98.9472 - val_loss: 6108449280.0000 - val_mape: 97.6257\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6448759808.0000 - mape: 98.9326 - val_loss: 6107724288.0000 - val_mape: 97.5951\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6447985664.0000 - mape: 98.9178 - val_loss: 6106979328.0000 - val_mape: 97.5637\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6447202304.0000 - mape: 98.9043 - val_loss: 6106223616.0000 - val_mape: 97.5317\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6446403584.0000 - mape: 98.8905 - val_loss: 6105469952.0000 - val_mape: 97.4999\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 6445608448.0000 - mape: 98.8764 - val_loss: 6104700416.0000 - val_mape: 97.4674\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 6444796928.0000 - mape: 98.8627 - val_loss: 6103919104.0000 - val_mape: 97.4344\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 9s 802ms/step - loss: 6443990528.0000 - mape: 98.8470 - val_loss: 6103187968.0000 - val_mape: 97.4035\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 11s 968ms/step - loss: 6443204608.0000 - mape: 98.8337 - val_loss: 6102422528.0000 - val_mape: 97.3711\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 11s 909ms/step - loss: 6442385920.0000 - mape: 98.8193 - val_loss: 6101635584.0000 - val_mape: 97.3379\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 25190s 2290s/step - loss: 6441568256.0000 - mape: 98.8030 - val_loss: 6100842496.0000 - val_mape: 97.3043\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 6440734720.0000 - mape: 98.7884 - val_loss: 6100064768.0000 - val_mape: 97.2715\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 6439918592.0000 - mape: 98.7722 - val_loss: 6099286528.0000 - val_mape: 97.2386\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6439088128.0000 - mape: 98.7585 - val_loss: 6098482688.0000 - val_mape: 97.2047\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6438243328.0000 - mape: 98.7427 - val_loss: 6097652224.0000 - val_mape: 97.1695\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6437378560.0000 - mape: 98.7289 - val_loss: 6096856064.0000 - val_mape: 97.1358\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 6436543488.0000 - mape: 98.7117 - val_loss: 6096045568.0000 - val_mape: 97.1016\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 5s 398ms/step - loss: 6435686912.0000 - mape: 98.6974 - val_loss: 6095242240.0000 - val_mape: 97.0676\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6434829824.0000 - mape: 98.6816 - val_loss: 6094400512.0000 - val_mape: 97.0320\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6433958400.0000 - mape: 98.6658 - val_loss: 6093599744.0000 - val_mape: 96.9982\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6433100288.0000 - mape: 98.6503 - val_loss: 6092768256.0000 - val_mape: 96.9630\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6432218624.0000 - mape: 98.6355 - val_loss: 6091921920.0000 - val_mape: 96.9272\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6431330304.0000 - mape: 98.6198 - val_loss: 6091060736.0000 - val_mape: 96.8908\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 9s 846ms/step - loss: 6430428672.0000 - mape: 98.6038 - val_loss: 6090221056.0000 - val_mape: 96.8552\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 11s 916ms/step - loss: 6429548544.0000 - mape: 98.5869 - val_loss: 6089390080.0000 - val_mape: 96.8201\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 11s 916ms/step - loss: 6428662272.0000 - mape: 98.5709 - val_loss: 6088537600.0000 - val_mape: 96.7840\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 11s 974ms/step - loss: 6427777536.0000 - mape: 98.5549 - val_loss: 6087695360.0000 - val_mape: 96.7484\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 6s 480ms/step - loss: 6426875904.0000 - mape: 98.5383 - val_loss: 6086842880.0000 - val_mape: 96.7123\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6425980416.0000 - mape: 98.5218 - val_loss: 6085960192.0000 - val_mape: 96.6749\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6425059328.0000 - mape: 98.5064 - val_loss: 6085098496.0000 - val_mape: 96.6384\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6424138752.0000 - mape: 98.4893 - val_loss: 6084212736.0000 - val_mape: 96.6009\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 135ms/step - loss: 6423206912.0000 - mape: 98.4731 - val_loss: 6083347456.0000 - val_mape: 96.5643\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 10s 861ms/step - loss: 6422297088.0000 - mape: 98.4567 - val_loss: 6082461184.0000 - val_mape: 96.5268\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6421357568.0000 - mape: 98.4408 - val_loss: 6081574400.0000 - val_mape: 96.4892\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6420424192.0000 - mape: 98.4224 - val_loss: 6080670208.0000 - val_mape: 96.4509\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6419477504.0000 - mape: 98.4047 - val_loss: 6079757312.0000 - val_mape: 96.4122\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6418521088.0000 - mape: 98.3875 - val_loss: 6078865920.0000 - val_mape: 96.3745\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6417574912.0000 - mape: 98.3712 - val_loss: 6077943808.0000 - val_mape: 96.3354\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6416607744.0000 - mape: 98.3549 - val_loss: 6077035520.0000 - val_mape: 96.2969\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6415651840.0000 - mape: 98.3359 - val_loss: 6076113920.0000 - val_mape: 96.2579\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6414682112.0000 - mape: 98.3190 - val_loss: 6075190272.0000 - val_mape: 96.2188\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6413705216.0000 - mape: 98.3011 - val_loss: 6074249216.0000 - val_mape: 96.1788\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6412712960.0000 - mape: 98.2852 - val_loss: 6073304064.0000 - val_mape: 96.1388\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6411723264.0000 - mape: 98.2674 - val_loss: 6072379392.0000 - val_mape: 96.0996\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 6410744832.0000 - mape: 98.2477 - val_loss: 6071456768.0000 - val_mape: 96.0605\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6409767936.0000 - mape: 98.2315 - val_loss: 6070499840.0000 - val_mape: 96.0199\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6408760832.0000 - mape: 98.2121 - val_loss: 6069562880.0000 - val_mape: 95.9802\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 6407768576.0000 - mape: 98.1962 - val_loss: 6068599808.0000 - val_mape: 95.9393\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6406759936.0000 - mape: 98.1769 - val_loss: 6067643904.0000 - val_mape: 95.8988\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 6405757440.0000 - mape: 98.1577 - val_loss: 6066658816.0000 - val_mape: 95.8570\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6404725760.0000 - mape: 98.1408 - val_loss: 6065695744.0000 - val_mape: 95.8162\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6403698176.0000 - mape: 98.1225 - val_loss: 6064714752.0000 - val_mape: 95.7745\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6402673664.0000 - mape: 98.1030 - val_loss: 6063726080.0000 - val_mape: 95.7326\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 6401635840.0000 - mape: 98.0850 - val_loss: 6062760960.0000 - val_mape: 95.6916\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6400611328.0000 - mape: 98.0646 - val_loss: 6061750784.0000 - val_mape: 95.6488\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6399559168.0000 - mape: 98.0451 - val_loss: 6060818432.0000 - val_mape: 95.6092\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6398554112.0000 - mape: 98.0300 - val_loss: 6059819008.0000 - val_mape: 95.5668\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 6397498368.0000 - mape: 98.0102 - val_loss: 6058795008.0000 - val_mape: 95.5233\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6396430336.0000 - mape: 97.9895 - val_loss: 6057762816.0000 - val_mape: 95.4795\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6395361280.0000 - mape: 97.9693 - val_loss: 6056786432.0000 - val_mape: 95.4380\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 6394317824.0000 - mape: 97.9518 - val_loss: 6055816192.0000 - val_mape: 95.3968\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 6393278464.0000 - mape: 97.9336 - val_loss: 6054767616.0000 - val_mape: 95.3523\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6392184832.0000 - mape: 97.9140 - val_loss: 6053740544.0000 - val_mape: 95.3086\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6391096320.0000 - mape: 97.8950 - val_loss: 6052667392.0000 - val_mape: 95.2630\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6389998592.0000 - mape: 97.8740 - val_loss: 6051666432.0000 - val_mape: 95.2205\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6388923392.0000 - mape: 97.8552 - val_loss: 6050647040.0000 - val_mape: 95.1772\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6387849216.0000 - mape: 97.8363 - val_loss: 6049614848.0000 - val_mape: 95.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_3, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c144bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.335179\n",
       "dtype: float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_3.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 4 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_4 = Sequential()\n",
    "rnn_model_4.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_4.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_4.add(Dense(15, activation = 'relu'))\n",
    "rnn_model_4.add(Dense(5, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_4.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 4 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,482\n",
      "Trainable params: 6,441\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_4.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cc51a-a8a3-4a61-9c9e-96317f159d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ecee49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_4, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_4.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d401da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 10s 198ms/step - loss: 6508309504.0000 - mape: 99.9989 - val_loss: 6164724224.0000 - val_mape: 99.9962\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6508161536.0000 - mape: 99.9980 - val_loss: 6164567040.0000 - val_mape: 99.9949\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6507995136.0000 - mape: 99.9966 - val_loss: 6164374016.0000 - val_mape: 99.9926\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6507831296.0000 - mape: 99.9950 - val_loss: 6164200448.0000 - val_mape: 99.9903\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6507670016.0000 - mape: 99.9935 - val_loss: 6164045312.0000 - val_mape: 99.9883\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6507507712.0000 - mape: 99.9921 - val_loss: 6163877888.0000 - val_mape: 99.9865\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6507330048.0000 - mape: 99.9907 - val_loss: 6163716608.0000 - val_mape: 99.9845\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6507155968.0000 - mape: 99.9890 - val_loss: 6163531264.0000 - val_mape: 99.9823\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6506940416.0000 - mape: 99.9861 - val_loss: 6163309568.0000 - val_mape: 99.9798\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6506671616.0000 - mape: 99.9830 - val_loss: 6163034624.0000 - val_mape: 99.9766\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 6506415616.0000 - mape: 99.9801 - val_loss: 6162813952.0000 - val_mape: 99.9720\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6506196480.0000 - mape: 99.9768 - val_loss: 6162584576.0000 - val_mape: 99.9665\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6505921024.0000 - mape: 99.9708 - val_loss: 6162353152.0000 - val_mape: 99.9583\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6505641984.0000 - mape: 99.9655 - val_loss: 6162095104.0000 - val_mape: 99.9425\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6505360384.0000 - mape: 99.9566 - val_loss: 6161836032.0000 - val_mape: 99.9168\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6505086464.0000 - mape: 99.9466 - val_loss: 6161579008.0000 - val_mape: 99.8712\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6504816640.0000 - mape: 99.9384 - val_loss: 6161316352.0000 - val_mape: 99.8550\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 6504544256.0000 - mape: 99.9329 - val_loss: 6161058816.0000 - val_mape: 99.8433\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6504267264.0000 - mape: 99.9273 - val_loss: 6160783872.0000 - val_mape: 99.8312\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6503980544.0000 - mape: 99.9222 - val_loss: 6160518144.0000 - val_mape: 99.8196\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6503688704.0000 - mape: 99.9169 - val_loss: 6160229376.0000 - val_mape: 99.8072\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6503382528.0000 - mape: 99.9114 - val_loss: 6159937024.0000 - val_mape: 99.7947\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6503076352.0000 - mape: 99.9055 - val_loss: 6159657984.0000 - val_mape: 99.7828\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6502772224.0000 - mape: 99.9008 - val_loss: 6159360000.0000 - val_mape: 99.7701\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6502457344.0000 - mape: 99.8945 - val_loss: 6159053824.0000 - val_mape: 99.7571\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6502131712.0000 - mape: 99.8884 - val_loss: 6158739456.0000 - val_mape: 99.7439\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6501797376.0000 - mape: 99.8827 - val_loss: 6158412288.0000 - val_mape: 99.7301\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6501446656.0000 - mape: 99.8767 - val_loss: 6158079488.0000 - val_mape: 99.7160\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6501096960.0000 - mape: 99.8703 - val_loss: 6157739008.0000 - val_mape: 99.7017\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6500740608.0000 - mape: 99.8634 - val_loss: 6157398016.0000 - val_mape: 99.6873\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6500376576.0000 - mape: 99.8575 - val_loss: 6157053440.0000 - val_mape: 99.6728\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6500008960.0000 - mape: 99.8513 - val_loss: 6156696064.0000 - val_mape: 99.6578\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6499627520.0000 - mape: 99.8443 - val_loss: 6156324864.0000 - val_mape: 99.6422\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6499242496.0000 - mape: 99.8365 - val_loss: 6155967488.0000 - val_mape: 99.6272\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6498853888.0000 - mape: 99.8304 - val_loss: 6155581952.0000 - val_mape: 99.6109\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6498450944.0000 - mape: 99.8235 - val_loss: 6155188224.0000 - val_mape: 99.5944\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6498036224.0000 - mape: 99.8156 - val_loss: 6154800128.0000 - val_mape: 99.5781\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6497618432.0000 - mape: 99.8090 - val_loss: 6154398208.0000 - val_mape: 99.5612\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6497198080.0000 - mape: 99.8002 - val_loss: 6153999360.0000 - val_mape: 99.5444\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6496772096.0000 - mape: 99.7931 - val_loss: 6153576448.0000 - val_mape: 99.5266\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6496325120.0000 - mape: 99.7848 - val_loss: 6153165312.0000 - val_mape: 99.5094\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 6495890944.0000 - mape: 99.7773 - val_loss: 6152748544.0000 - val_mape: 99.4919\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6495443456.0000 - mape: 99.7686 - val_loss: 6152297472.0000 - val_mape: 99.4729\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 6494969856.0000 - mape: 99.7606 - val_loss: 6151861248.0000 - val_mape: 99.4546\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6494510592.0000 - mape: 99.7524 - val_loss: 6151396864.0000 - val_mape: 99.4351\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 6494026240.0000 - mape: 99.7441 - val_loss: 6150956544.0000 - val_mape: 99.4165\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6493550592.0000 - mape: 99.7361 - val_loss: 6150489600.0000 - val_mape: 99.3969\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 6493058048.0000 - mape: 99.7264 - val_loss: 6150020096.0000 - val_mape: 99.3772\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6492561408.0000 - mape: 99.7183 - val_loss: 6149544448.0000 - val_mape: 99.3572\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6492055040.0000 - mape: 99.7083 - val_loss: 6149045248.0000 - val_mape: 99.3362\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6491537408.0000 - mape: 99.6983 - val_loss: 6148569088.0000 - val_mape: 99.3162\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6491027456.0000 - mape: 99.6902 - val_loss: 6148052480.0000 - val_mape: 99.2944\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6490486784.0000 - mape: 99.6807 - val_loss: 6147550208.0000 - val_mape: 99.2733\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6489946112.0000 - mape: 99.6709 - val_loss: 6147025408.0000 - val_mape: 99.2513\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6489395712.0000 - mape: 99.6610 - val_loss: 6146506240.0000 - val_mape: 99.2294\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6488850944.0000 - mape: 99.6512 - val_loss: 6145982464.0000 - val_mape: 99.2074\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6488288768.0000 - mape: 99.6418 - val_loss: 6145443840.0000 - val_mape: 99.1848\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6487718912.0000 - mape: 99.6308 - val_loss: 6144881664.0000 - val_mape: 99.1611\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6487134208.0000 - mape: 99.6214 - val_loss: 6144321536.0000 - val_mape: 99.1376\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6486547968.0000 - mape: 99.6099 - val_loss: 6143768064.0000 - val_mape: 99.1143\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6485960192.0000 - mape: 99.5997 - val_loss: 6143212544.0000 - val_mape: 99.0909\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6485367808.0000 - mape: 99.5885 - val_loss: 6142638592.0000 - val_mape: 99.0667\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6484754944.0000 - mape: 99.5790 - val_loss: 6142056960.0000 - val_mape: 99.0423\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6484146176.0000 - mape: 99.5664 - val_loss: 6141458432.0000 - val_mape: 99.0171\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6483514368.0000 - mape: 99.5563 - val_loss: 6140859904.0000 - val_mape: 98.9919\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 6482891264.0000 - mape: 99.5448 - val_loss: 6140267008.0000 - val_mape: 98.9670\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6482257408.0000 - mape: 99.5338 - val_loss: 6139663360.0000 - val_mape: 98.9416\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6481619456.0000 - mape: 99.5216 - val_loss: 6139061760.0000 - val_mape: 98.9162\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6480974336.0000 - mape: 99.5107 - val_loss: 6138432512.0000 - val_mape: 98.8898\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6480313856.0000 - mape: 99.4975 - val_loss: 6137804800.0000 - val_mape: 98.8633\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6479649792.0000 - mape: 99.4859 - val_loss: 6137161216.0000 - val_mape: 98.8363\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6478970368.0000 - mape: 99.4745 - val_loss: 6136516608.0000 - val_mape: 98.8091\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6478294016.0000 - mape: 99.4613 - val_loss: 6135865344.0000 - val_mape: 98.7817\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 6477597184.0000 - mape: 99.4493 - val_loss: 6135192576.0000 - val_mape: 98.7534\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6476898304.0000 - mape: 99.4374 - val_loss: 6134545920.0000 - val_mape: 98.7261\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6476203520.0000 - mape: 99.4255 - val_loss: 6133856768.0000 - val_mape: 98.6971\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6475487744.0000 - mape: 99.4113 - val_loss: 6133173248.0000 - val_mape: 98.6683\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6474763264.0000 - mape: 99.3992 - val_loss: 6132477952.0000 - val_mape: 98.6391\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6474032640.0000 - mape: 99.3862 - val_loss: 6131792896.0000 - val_mape: 98.6102\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6473301504.0000 - mape: 99.3739 - val_loss: 6131083776.0000 - val_mape: 98.5803\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6472556032.0000 - mape: 99.3587 - val_loss: 6130386432.0000 - val_mape: 98.5509\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6471814144.0000 - mape: 99.3472 - val_loss: 6129658880.0000 - val_mape: 98.5203\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6471052288.0000 - mape: 99.3327 - val_loss: 6128927744.0000 - val_mape: 98.4895\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 6470282752.0000 - mape: 99.3182 - val_loss: 6128205824.0000 - val_mape: 98.4591\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6469515264.0000 - mape: 99.3057 - val_loss: 6127437312.0000 - val_mape: 98.4267\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6468722688.0000 - mape: 99.2906 - val_loss: 6126707712.0000 - val_mape: 98.3959\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6467945984.0000 - mape: 99.2773 - val_loss: 6125972480.0000 - val_mape: 98.3649\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6467164672.0000 - mape: 99.2616 - val_loss: 6125223936.0000 - val_mape: 98.3334\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6466376704.0000 - mape: 99.2476 - val_loss: 6124470272.0000 - val_mape: 98.3016\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6465570816.0000 - mape: 99.2335 - val_loss: 6123688448.0000 - val_mape: 98.2687\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6464743424.0000 - mape: 99.2201 - val_loss: 6122882560.0000 - val_mape: 98.2347\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6463896576.0000 - mape: 99.2060 - val_loss: 6122073088.0000 - val_mape: 98.2006\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6463058944.0000 - mape: 99.1895 - val_loss: 6121290240.0000 - val_mape: 98.1675\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6462233600.0000 - mape: 99.1742 - val_loss: 6120499712.0000 - val_mape: 98.1342\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 6461385216.0000 - mape: 99.1596 - val_loss: 6119669248.0000 - val_mape: 98.0992\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6460527104.0000 - mape: 99.1438 - val_loss: 6118887936.0000 - val_mape: 98.0662\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6459690496.0000 - mape: 99.1280 - val_loss: 6118078976.0000 - val_mape: 98.0322\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6458823168.0000 - mape: 99.1148 - val_loss: 6117219328.0000 - val_mape: 97.9958\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6457934848.0000 - mape: 99.0979 - val_loss: 6116396032.0000 - val_mape: 97.9611\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6457060352.0000 - mape: 99.0811 - val_loss: 6115536896.0000 - val_mape: 97.9249\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6456163328.0000 - mape: 99.0657 - val_loss: 6114675712.0000 - val_mape: 97.8885\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6455261184.0000 - mape: 99.0488 - val_loss: 6113853952.0000 - val_mape: 97.8539\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6454377984.0000 - mape: 99.0339 - val_loss: 6112998400.0000 - val_mape: 97.8178\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6453478400.0000 - mape: 99.0163 - val_loss: 6112130560.0000 - val_mape: 97.7811\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6452555264.0000 - mape: 99.0012 - val_loss: 6111244288.0000 - val_mape: 97.7437\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6451629568.0000 - mape: 98.9842 - val_loss: 6110356992.0000 - val_mape: 97.7063\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6450690048.0000 - mape: 98.9682 - val_loss: 6109443584.0000 - val_mape: 97.6677\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6449730048.0000 - mape: 98.9493 - val_loss: 6108557312.0000 - val_mape: 97.6303\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6448790528.0000 - mape: 98.9322 - val_loss: 6107623424.0000 - val_mape: 97.5908\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6447810560.0000 - mape: 98.9173 - val_loss: 6106699776.0000 - val_mape: 97.5518\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6446839808.0000 - mape: 98.9002 - val_loss: 6105800192.0000 - val_mape: 97.5139\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 6445884928.0000 - mape: 98.8824 - val_loss: 6104884224.0000 - val_mape: 97.4751\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6444928000.0000 - mape: 98.8639 - val_loss: 6103969792.0000 - val_mape: 97.4365\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6443942400.0000 - mape: 98.8473 - val_loss: 6102994944.0000 - val_mape: 97.3953\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 6442930176.0000 - mape: 98.8270 - val_loss: 6102048256.0000 - val_mape: 97.3553\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6441931264.0000 - mape: 98.8091 - val_loss: 6101072384.0000 - val_mape: 97.3141\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 124ms/step - loss: 6440908800.0000 - mape: 98.7918 - val_loss: 6100147200.0000 - val_mape: 97.2750\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6439910912.0000 - mape: 98.7749 - val_loss: 6099148288.0000 - val_mape: 97.2327\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 6438875648.0000 - mape: 98.7557 - val_loss: 6098161664.0000 - val_mape: 97.1911\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6437835776.0000 - mape: 98.7374 - val_loss: 6097199104.0000 - val_mape: 97.1504\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 6436810240.0000 - mape: 98.7191 - val_loss: 6096180224.0000 - val_mape: 97.1073\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6435742208.0000 - mape: 98.7001 - val_loss: 6095182848.0000 - val_mape: 97.0651\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 6434695168.0000 - mape: 98.6791 - val_loss: 6094194176.0000 - val_mape: 97.0233\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6433638400.0000 - mape: 98.6606 - val_loss: 6093166592.0000 - val_mape: 96.9799\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6432568320.0000 - mape: 98.6408 - val_loss: 6092163584.0000 - val_mape: 96.9374\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6431495680.0000 - mape: 98.6223 - val_loss: 6091142144.0000 - val_mape: 96.8942\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6430412800.0000 - mape: 98.6031 - val_loss: 6090066944.0000 - val_mape: 96.8487\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6429298688.0000 - mape: 98.5818 - val_loss: 6089025536.0000 - val_mape: 96.8046\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6428205568.0000 - mape: 98.5631 - val_loss: 6087990272.0000 - val_mape: 96.7608\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6427103232.0000 - mape: 98.5451 - val_loss: 6086902784.0000 - val_mape: 96.7148\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 6425981952.0000 - mape: 98.5231 - val_loss: 6085901312.0000 - val_mape: 96.6724\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6424896000.0000 - mape: 98.5046 - val_loss: 6084813824.0000 - val_mape: 96.6264\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6423755776.0000 - mape: 98.4836 - val_loss: 6083717632.0000 - val_mape: 96.5800\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 6422608896.0000 - mape: 98.4631 - val_loss: 6082648576.0000 - val_mape: 96.5347\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6421474304.0000 - mape: 98.4423 - val_loss: 6081549312.0000 - val_mape: 96.4881\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6420310528.0000 - mape: 98.4220 - val_loss: 6080446976.0000 - val_mape: 96.4414\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 6419153408.0000 - mape: 98.4010 - val_loss: 6079343616.0000 - val_mape: 96.3947\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6417994240.0000 - mape: 98.3786 - val_loss: 6078211584.0000 - val_mape: 96.3468\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6416810496.0000 - mape: 98.3574 - val_loss: 6077152256.0000 - val_mape: 96.3019\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6415682560.0000 - mape: 98.3356 - val_loss: 6076014080.0000 - val_mape: 96.2537\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6414489600.0000 - mape: 98.3154 - val_loss: 6074921472.0000 - val_mape: 96.2073\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6413315584.0000 - mape: 98.2950 - val_loss: 6073746944.0000 - val_mape: 96.1576\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 6412099584.0000 - mape: 98.2707 - val_loss: 6072610304.0000 - val_mape: 96.1094\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6410887168.0000 - mape: 98.2500 - val_loss: 6071453184.0000 - val_mape: 96.0603\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6409655808.0000 - mape: 98.2293 - val_loss: 6070262272.0000 - val_mape: 96.0098\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6408427008.0000 - mape: 98.2062 - val_loss: 6069093888.0000 - val_mape: 95.9603\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6407180288.0000 - mape: 98.1848 - val_loss: 6067900416.0000 - val_mape: 95.9097\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6405929984.0000 - mape: 98.1638 - val_loss: 6066707456.0000 - val_mape: 95.8591\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6404682752.0000 - mape: 98.1408 - val_loss: 6065540096.0000 - val_mape: 95.8096\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6403440128.0000 - mape: 98.1187 - val_loss: 6064339968.0000 - val_mape: 95.7586\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6402177536.0000 - mape: 98.0940 - val_loss: 6063127040.0000 - val_mape: 95.7072\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6400899584.0000 - mape: 98.0713 - val_loss: 6061910528.0000 - val_mape: 95.6555\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6399625216.0000 - mape: 98.0469 - val_loss: 6060740608.0000 - val_mape: 95.6059\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6398362624.0000 - mape: 98.0281 - val_loss: 6059486208.0000 - val_mape: 95.5526\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6397062144.0000 - mape: 98.0013 - val_loss: 6058251776.0000 - val_mape: 95.5002\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6395757056.0000 - mape: 97.9810 - val_loss: 6057020416.0000 - val_mape: 95.4480\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6394464768.0000 - mape: 97.9560 - val_loss: 6055794176.0000 - val_mape: 95.3959\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6393164800.0000 - mape: 97.9306 - val_loss: 6054542336.0000 - val_mape: 95.3427\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6391839744.0000 - mape: 97.9065 - val_loss: 6053245952.0000 - val_mape: 95.2876\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6390496768.0000 - mape: 97.8834 - val_loss: 6052010496.0000 - val_mape: 95.2351\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6389170688.0000 - mape: 97.8596 - val_loss: 6050757120.0000 - val_mape: 95.1819\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6387845120.0000 - mape: 97.8366 - val_loss: 6049465344.0000 - val_mape: 95.1270\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6386497536.0000 - mape: 97.8120 - val_loss: 6048201728.0000 - val_mape: 95.0733\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6385155584.0000 - mape: 97.7862 - val_loss: 6046895104.0000 - val_mape: 95.0177\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6383781888.0000 - mape: 97.7615 - val_loss: 6045603840.0000 - val_mape: 94.9628\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6382414336.0000 - mape: 97.7368 - val_loss: 6044285952.0000 - val_mape: 94.9068\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 6381027840.0000 - mape: 97.7105 - val_loss: 6042987520.0000 - val_mape: 94.8516\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6379662336.0000 - mape: 97.6858 - val_loss: 6041676288.0000 - val_mape: 94.7958\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 6378281984.0000 - mape: 97.6616 - val_loss: 6040333312.0000 - val_mape: 94.7387\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6376865280.0000 - mape: 97.6372 - val_loss: 6038973440.0000 - val_mape: 94.6808\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6375448576.0000 - mape: 97.6086 - val_loss: 6037660672.0000 - val_mape: 94.6250\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 6374040064.0000 - mape: 97.5857 - val_loss: 6036301312.0000 - val_mape: 94.5671\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 6372623872.0000 - mape: 97.5584 - val_loss: 6034955264.0000 - val_mape: 94.5098\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6371186176.0000 - mape: 97.5359 - val_loss: 6033523712.0000 - val_mape: 94.4489\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6369709056.0000 - mape: 97.5078 - val_loss: 6032158720.0000 - val_mape: 94.3908\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6368269312.0000 - mape: 97.4795 - val_loss: 6030781440.0000 - val_mape: 94.3321\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6366805504.0000 - mape: 97.4545 - val_loss: 6029397504.0000 - val_mape: 94.2732\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6365344256.0000 - mape: 97.4315 - val_loss: 6028000768.0000 - val_mape: 94.2137\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6363872768.0000 - mape: 97.4026 - val_loss: 6026558464.0000 - val_mape: 94.1522\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 6362385408.0000 - mape: 97.3754 - val_loss: 6025210368.0000 - val_mape: 94.0948\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6360928768.0000 - mape: 97.3494 - val_loss: 6023759360.0000 - val_mape: 94.0329\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6359434240.0000 - mape: 97.3221 - val_loss: 6022389760.0000 - val_mape: 93.9746\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 1s 127ms/step - loss: 6357959168.0000 - mape: 97.2934 - val_loss: 6020946432.0000 - val_mape: 93.9130\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6356453888.0000 - mape: 97.2679 - val_loss: 6019564544.0000 - val_mape: 93.8541\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6354963968.0000 - mape: 97.2418 - val_loss: 6018112512.0000 - val_mape: 93.7922\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6353446400.0000 - mape: 97.2142 - val_loss: 6016644096.0000 - val_mape: 93.7296\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6351903744.0000 - mape: 97.1838 - val_loss: 6015176704.0000 - val_mape: 93.6669\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 6350354432.0000 - mape: 97.1590 - val_loss: 6013692416.0000 - val_mape: 93.6036\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6348818432.0000 - mape: 97.1268 - val_loss: 6012252160.0000 - val_mape: 93.5422\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6347284480.0000 - mape: 97.1008 - val_loss: 6010807296.0000 - val_mape: 93.4805\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6345747456.0000 - mape: 97.0732 - val_loss: 6009283584.0000 - val_mape: 93.4155\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6344158720.0000 - mape: 97.0437 - val_loss: 6007768064.0000 - val_mape: 93.3508\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6342571008.0000 - mape: 97.0154 - val_loss: 6006282752.0000 - val_mape: 93.2874\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6341001216.0000 - mape: 96.9879 - val_loss: 6004827648.0000 - val_mape: 93.2253\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6339451904.0000 - mape: 96.9603 - val_loss: 6003336192.0000 - val_mape: 93.1615\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 6337870848.0000 - mape: 96.9290 - val_loss: 6001801728.0000 - val_mape: 93.0960\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6336261632.0000 - mape: 96.9026 - val_loss: 6000246272.0000 - val_mape: 93.0295\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6334636032.0000 - mape: 96.8732 - val_loss: 5998697472.0000 - val_mape: 92.9633\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 6333011456.0000 - mape: 96.8422 - val_loss: 5997171712.0000 - val_mape: 92.8981\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 6331410432.0000 - mape: 96.8105 - val_loss: 5995652608.0000 - val_mape: 92.8332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_4, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db66b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7efd8bffa3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2.297131\n",
       "dtype: float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_4.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 5, LSTM X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "474fd986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ae6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 4th model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_5 = Sequential()\n",
    "rnn_model_5.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_5.add(LSTM(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_5.add(LSTM(units=20, activation='tanh', return_sequences =True))\n",
    "rnn_model_5.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_5.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c43e359-c428-43f9-9369-f0804a5acdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_5, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_5.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2c33a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, None, 30)          6120      \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, None, 20)          4080      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, None, 1)           11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,462\n",
      "Trainable params: 10,421\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_5.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b13f6eb0-e734-48c4-a455-49949ea62a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 16s 447ms/step - loss: 6508325376.0000 - mape: 99.9999 - val_loss: 6164756992.0000 - val_mape: 99.9999\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6508195840.0000 - mape: 99.9990 - val_loss: 6164599296.0000 - val_mape: 99.9983\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6508064256.0000 - mape: 99.9978 - val_loss: 6164476928.0000 - val_mape: 99.9967\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6507945984.0000 - mape: 99.9967 - val_loss: 6164366848.0000 - val_mape: 99.9952\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6507844096.0000 - mape: 99.9956 - val_loss: 6164276224.0000 - val_mape: 99.9940\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6507753984.0000 - mape: 99.9947 - val_loss: 6164186112.0000 - val_mape: 99.9928\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 6507646976.0000 - mape: 99.9925 - val_loss: 6164096512.0000 - val_mape: 99.9906\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6507546624.0000 - mape: 99.9902 - val_loss: 6164014080.0000 - val_mape: 99.9885\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6507458048.0000 - mape: 99.9888 - val_loss: 6163936256.0000 - val_mape: 99.9862\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6507373568.0000 - mape: 99.9873 - val_loss: 6163857408.0000 - val_mape: 99.9718\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6507285504.0000 - mape: 99.9824 - val_loss: 6163774464.0000 - val_mape: 99.9606\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6507197440.0000 - mape: 99.9805 - val_loss: 6163692544.0000 - val_mape: 99.9562\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6507110912.0000 - mape: 99.9785 - val_loss: 6163609600.0000 - val_mape: 99.9518\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6507019776.0000 - mape: 99.9768 - val_loss: 6163518976.0000 - val_mape: 99.9472\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 6506921472.0000 - mape: 99.9751 - val_loss: 6163420160.0000 - val_mape: 99.9426\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6506815488.0000 - mape: 99.9727 - val_loss: 6163318272.0000 - val_mape: 99.9379\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6506705408.0000 - mape: 99.9711 - val_loss: 6163211264.0000 - val_mape: 99.9332\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6506592256.0000 - mape: 99.9687 - val_loss: 6163103744.0000 - val_mape: 99.9284\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6506479104.0000 - mape: 99.9667 - val_loss: 6162991616.0000 - val_mape: 99.9236\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6506358784.0000 - mape: 99.9647 - val_loss: 6162876928.0000 - val_mape: 99.9186\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6506237952.0000 - mape: 99.9624 - val_loss: 6162762752.0000 - val_mape: 99.9137\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6506114560.0000 - mape: 99.9602 - val_loss: 6162639872.0000 - val_mape: 99.9085\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 6505986048.0000 - mape: 99.9578 - val_loss: 6162515456.0000 - val_mape: 99.9032\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6505853440.0000 - mape: 99.9555 - val_loss: 6162385920.0000 - val_mape: 99.8977\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 6505715712.0000 - mape: 99.9532 - val_loss: 6162262528.0000 - val_mape: 99.8925\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6505584128.0000 - mape: 99.9508 - val_loss: 6162129920.0000 - val_mape: 99.8868\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6505444352.0000 - mape: 99.9483 - val_loss: 6161992704.0000 - val_mape: 99.8810\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6505297920.0000 - mape: 99.9457 - val_loss: 6161852928.0000 - val_mape: 99.8751\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6505149440.0000 - mape: 99.9432 - val_loss: 6161710592.0000 - val_mape: 99.8691\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 6504999424.0000 - mape: 99.9401 - val_loss: 6161569280.0000 - val_mape: 99.8632\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6504846336.0000 - mape: 99.9378 - val_loss: 6161416704.0000 - val_mape: 99.8567\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 6504689152.0000 - mape: 99.9347 - val_loss: 6161264640.0000 - val_mape: 99.8503\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6504525312.0000 - mape: 99.9320 - val_loss: 6161105920.0000 - val_mape: 99.8436\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 6504358912.0000 - mape: 99.9289 - val_loss: 6160952320.0000 - val_mape: 99.8372\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 6504194560.0000 - mape: 99.9262 - val_loss: 6160788480.0000 - val_mape: 99.8302\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 6504022016.0000 - mape: 99.9228 - val_loss: 6160622592.0000 - val_mape: 99.8233\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 6503847936.0000 - mape: 99.9195 - val_loss: 6160460288.0000 - val_mape: 99.8164\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6503672320.0000 - mape: 99.9167 - val_loss: 6160288768.0000 - val_mape: 99.8092\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 6503491072.0000 - mape: 99.9133 - val_loss: 6160113664.0000 - val_mape: 99.8018\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 6503307776.0000 - mape: 99.9097 - val_loss: 6159934976.0000 - val_mape: 99.7943\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6503116800.0000 - mape: 99.9065 - val_loss: 6159757312.0000 - val_mape: 99.7868\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 6502927360.0000 - mape: 99.9032 - val_loss: 6159567360.0000 - val_mape: 99.7788\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 6502729728.0000 - mape: 99.8996 - val_loss: 6159385600.0000 - val_mape: 99.7712\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6502533632.0000 - mape: 99.8958 - val_loss: 6159194624.0000 - val_mape: 99.7631\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 6502330368.0000 - mape: 99.8924 - val_loss: 6158994432.0000 - val_mape: 99.7547\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6502121472.0000 - mape: 99.8887 - val_loss: 6158802944.0000 - val_mape: 99.7467\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6501917696.0000 - mape: 99.8853 - val_loss: 6158601728.0000 - val_mape: 99.7382\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6501705728.0000 - mape: 99.8815 - val_loss: 6158397440.0000 - val_mape: 99.7296\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6501486592.0000 - mape: 99.8779 - val_loss: 6158185984.0000 - val_mape: 99.7207\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6501265920.0000 - mape: 99.8736 - val_loss: 6157972992.0000 - val_mape: 99.7117\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6501042176.0000 - mape: 99.8694 - val_loss: 6157757440.0000 - val_mape: 99.7027\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6500815872.0000 - mape: 99.8652 - val_loss: 6157544960.0000 - val_mape: 99.6937\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6500585984.0000 - mape: 99.8618 - val_loss: 6157321216.0000 - val_mape: 99.6843\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6500349952.0000 - mape: 99.8573 - val_loss: 6157095936.0000 - val_mape: 99.6748\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6500113408.0000 - mape: 99.8529 - val_loss: 6156870656.0000 - val_mape: 99.6653\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 3s 208ms/step - loss: 6499874304.0000 - mape: 99.8486 - val_loss: 6156633600.0000 - val_mape: 99.6554\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6499629568.0000 - mape: 99.8442 - val_loss: 6156412416.0000 - val_mape: 99.6461\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6499388416.0000 - mape: 99.8399 - val_loss: 6156166656.0000 - val_mape: 99.6358\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6499134464.0000 - mape: 99.8352 - val_loss: 6155929088.0000 - val_mape: 99.6257\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6498881024.0000 - mape: 99.8310 - val_loss: 6155692032.0000 - val_mape: 99.6158\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6498627072.0000 - mape: 99.8267 - val_loss: 6155443200.0000 - val_mape: 99.6053\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6498364928.0000 - mape: 99.8217 - val_loss: 6155197440.0000 - val_mape: 99.5950\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6498102784.0000 - mape: 99.8169 - val_loss: 6154946048.0000 - val_mape: 99.5844\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6497840640.0000 - mape: 99.8122 - val_loss: 6154691584.0000 - val_mape: 99.5737\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6497568768.0000 - mape: 99.8076 - val_loss: 6154428416.0000 - val_mape: 99.5626\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6497294336.0000 - mape: 99.8025 - val_loss: 6154164736.0000 - val_mape: 99.5515\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 6497011712.0000 - mape: 99.7977 - val_loss: 6153896960.0000 - val_mape: 99.5403\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6496731648.0000 - mape: 99.7924 - val_loss: 6153638912.0000 - val_mape: 99.5294\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6496457728.0000 - mape: 99.7874 - val_loss: 6153357824.0000 - val_mape: 99.5176\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6496162304.0000 - mape: 99.7824 - val_loss: 6153082368.0000 - val_mape: 99.5060\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6495873024.0000 - mape: 99.7770 - val_loss: 6152805376.0000 - val_mape: 99.4944\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6495578624.0000 - mape: 99.7722 - val_loss: 6152519680.0000 - val_mape: 99.4824\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6495277568.0000 - mape: 99.7664 - val_loss: 6152231424.0000 - val_mape: 99.4703\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6494972416.0000 - mape: 99.7610 - val_loss: 6151937024.0000 - val_mape: 99.4579\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6494664704.0000 - mape: 99.7562 - val_loss: 6151648256.0000 - val_mape: 99.4457\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6494360064.0000 - mape: 99.7506 - val_loss: 6151355392.0000 - val_mape: 99.4334\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6494053888.0000 - mape: 99.7444 - val_loss: 6151061504.0000 - val_mape: 99.4210\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6493739520.0000 - mape: 99.7392 - val_loss: 6150764032.0000 - val_mape: 99.4086\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6493423616.0000 - mape: 99.7331 - val_loss: 6150459904.0000 - val_mape: 99.3957\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6493102080.0000 - mape: 99.7276 - val_loss: 6150151680.0000 - val_mape: 99.3828\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6492773888.0000 - mape: 99.7214 - val_loss: 6149832704.0000 - val_mape: 99.3694\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6492441088.0000 - mape: 99.7160 - val_loss: 6149515264.0000 - val_mape: 99.3560\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6492108800.0000 - mape: 99.7095 - val_loss: 6149207040.0000 - val_mape: 99.3431\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6491777536.0000 - mape: 99.7037 - val_loss: 6148883456.0000 - val_mape: 99.3295\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6491439104.0000 - mape: 99.6980 - val_loss: 6148568064.0000 - val_mape: 99.3162\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6491105792.0000 - mape: 99.6913 - val_loss: 6148247552.0000 - val_mape: 99.3027\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6490759680.0000 - mape: 99.6861 - val_loss: 6147906048.0000 - val_mape: 99.2884\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6490404352.0000 - mape: 99.6797 - val_loss: 6147570688.0000 - val_mape: 99.2743\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6490049024.0000 - mape: 99.6731 - val_loss: 6147231744.0000 - val_mape: 99.2600\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6489692160.0000 - mape: 99.6672 - val_loss: 6146883584.0000 - val_mape: 99.2454\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6489329664.0000 - mape: 99.6601 - val_loss: 6146544640.0000 - val_mape: 99.2311\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6488969728.0000 - mape: 99.6535 - val_loss: 6146187264.0000 - val_mape: 99.2161\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6488597504.0000 - mape: 99.6466 - val_loss: 6145848320.0000 - val_mape: 99.2018\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6488235008.0000 - mape: 99.6402 - val_loss: 6145489920.0000 - val_mape: 99.1868\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6487859200.0000 - mape: 99.6333 - val_loss: 6145141248.0000 - val_mape: 99.1721\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6487488000.0000 - mape: 99.6273 - val_loss: 6144783872.0000 - val_mape: 99.1570\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6487110144.0000 - mape: 99.6201 - val_loss: 6144411136.0000 - val_mape: 99.1414\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6486720512.0000 - mape: 99.6138 - val_loss: 6144042496.0000 - val_mape: 99.1259\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6486328832.0000 - mape: 99.6072 - val_loss: 6143673856.0000 - val_mape: 99.1104\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6485942784.0000 - mape: 99.5999 - val_loss: 6143297024.0000 - val_mape: 99.0945\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6485549568.0000 - mape: 99.5925 - val_loss: 6142928896.0000 - val_mape: 99.0790\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6485155840.0000 - mape: 99.5855 - val_loss: 6142542848.0000 - val_mape: 99.0628\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6484749824.0000 - mape: 99.5783 - val_loss: 6142160384.0000 - val_mape: 99.0467\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 6484347904.0000 - mape: 99.5710 - val_loss: 6141786112.0000 - val_mape: 99.0309\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6483947520.0000 - mape: 99.5643 - val_loss: 6141407232.0000 - val_mape: 99.0150\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6483544064.0000 - mape: 99.5571 - val_loss: 6140997120.0000 - val_mape: 98.9977\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6483119616.0000 - mape: 99.5491 - val_loss: 6140607488.0000 - val_mape: 98.9813\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 6482709504.0000 - mape: 99.5420 - val_loss: 6140214784.0000 - val_mape: 98.9648\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6482291712.0000 - mape: 99.5342 - val_loss: 6139805696.0000 - val_mape: 98.9476\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6481865216.0000 - mape: 99.5267 - val_loss: 6139408384.0000 - val_mape: 98.9309\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6481441792.0000 - mape: 99.5195 - val_loss: 6138999808.0000 - val_mape: 98.9137\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 6481010688.0000 - mape: 99.5117 - val_loss: 6138581504.0000 - val_mape: 98.8960\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6480570880.0000 - mape: 99.5043 - val_loss: 6138164224.0000 - val_mape: 98.8785\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6480137216.0000 - mape: 99.4957 - val_loss: 6137758720.0000 - val_mape: 98.8614\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6479699456.0000 - mape: 99.4885 - val_loss: 6137323520.0000 - val_mape: 98.8431\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6479245824.0000 - mape: 99.4801 - val_loss: 6136901632.0000 - val_mape: 98.8253\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6478800896.0000 - mape: 99.4725 - val_loss: 6136476672.0000 - val_mape: 98.8074\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6478350336.0000 - mape: 99.4640 - val_loss: 6136055296.0000 - val_mape: 98.7897\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6477908992.0000 - mape: 99.4552 - val_loss: 6135635968.0000 - val_mape: 98.7721\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 3s 205ms/step - loss: 6477461504.0000 - mape: 99.4479 - val_loss: 6135193600.0000 - val_mape: 98.7534\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6476995584.0000 - mape: 99.4393 - val_loss: 6134749696.0000 - val_mape: 98.7347\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 3s 233ms/step - loss: 6476528128.0000 - mape: 99.4311 - val_loss: 6134293504.0000 - val_mape: 98.7155\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6476050432.0000 - mape: 99.4234 - val_loss: 6133855744.0000 - val_mape: 98.6971\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6475592704.0000 - mape: 99.4144 - val_loss: 6133415936.0000 - val_mape: 98.6786\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 6475121152.0000 - mape: 99.4064 - val_loss: 6132960768.0000 - val_mape: 98.6594\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6474642944.0000 - mape: 99.3977 - val_loss: 6132496384.0000 - val_mape: 98.6399\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6474153472.0000 - mape: 99.3888 - val_loss: 6132034560.0000 - val_mape: 98.6204\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6473665024.0000 - mape: 99.3796 - val_loss: 6131565568.0000 - val_mape: 98.6006\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 3s 227ms/step - loss: 6473175040.0000 - mape: 99.3714 - val_loss: 6131090944.0000 - val_mape: 98.5807\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 6472676864.0000 - mape: 99.3627 - val_loss: 6130636800.0000 - val_mape: 98.5615\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 3s 209ms/step - loss: 6472190464.0000 - mape: 99.3533 - val_loss: 6130159104.0000 - val_mape: 98.5414\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6471692288.0000 - mape: 99.3445 - val_loss: 6129666560.0000 - val_mape: 98.5207\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6471180288.0000 - mape: 99.3348 - val_loss: 6129202176.0000 - val_mape: 98.5011\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6470683648.0000 - mape: 99.3262 - val_loss: 6128722944.0000 - val_mape: 98.4809\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6470177280.0000 - mape: 99.3180 - val_loss: 6128248320.0000 - val_mape: 98.4609\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 3s 291ms/step - loss: 6469676032.0000 - mape: 99.3087 - val_loss: 6127749120.0000 - val_mape: 98.4398\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 3s 241ms/step - loss: 6469153792.0000 - mape: 99.2986 - val_loss: 6127256064.0000 - val_mape: 98.4191\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 3s 221ms/step - loss: 6468634112.0000 - mape: 99.2897 - val_loss: 6126770176.0000 - val_mape: 98.3986\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 3s 224ms/step - loss: 6468118528.0000 - mape: 99.2808 - val_loss: 6126269440.0000 - val_mape: 98.3775\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6467592192.0000 - mape: 99.2706 - val_loss: 6125771264.0000 - val_mape: 98.3565\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 3s 270ms/step - loss: 6467064320.0000 - mape: 99.2618 - val_loss: 6125261824.0000 - val_mape: 98.3350\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 3s 223ms/step - loss: 6466533376.0000 - mape: 99.2518 - val_loss: 6124754432.0000 - val_mape: 98.3136\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 3s 233ms/step - loss: 6465997824.0000 - mape: 99.2426 - val_loss: 6124254208.0000 - val_mape: 98.2926\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 3s 209ms/step - loss: 6465464832.0000 - mape: 99.2330 - val_loss: 6123723776.0000 - val_mape: 98.2702\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6464907776.0000 - mape: 99.2229 - val_loss: 6123200512.0000 - val_mape: 98.2481\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6464358400.0000 - mape: 99.2131 - val_loss: 6122683392.0000 - val_mape: 98.2263\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6463815680.0000 - mape: 99.2031 - val_loss: 6122165760.0000 - val_mape: 98.2045\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6463267328.0000 - mape: 99.1931 - val_loss: 6121650176.0000 - val_mape: 98.1827\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6462717440.0000 - mape: 99.1838 - val_loss: 6121120256.0000 - val_mape: 98.1604\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6462158336.0000 - mape: 99.1733 - val_loss: 6120583168.0000 - val_mape: 98.1378\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6461597696.0000 - mape: 99.1630 - val_loss: 6120046592.0000 - val_mape: 98.1151\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6461025792.0000 - mape: 99.1536 - val_loss: 6119491072.0000 - val_mape: 98.0917\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 6460449280.0000 - mape: 99.1431 - val_loss: 6118947840.0000 - val_mape: 98.0688\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6459878400.0000 - mape: 99.1323 - val_loss: 6118413824.0000 - val_mape: 98.0462\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6459307520.0000 - mape: 99.1233 - val_loss: 6117840384.0000 - val_mape: 98.0221\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6458718208.0000 - mape: 99.1121 - val_loss: 6117299200.0000 - val_mape: 97.9992\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6458140160.0000 - mape: 99.1019 - val_loss: 6116749312.0000 - val_mape: 97.9760\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 3s 224ms/step - loss: 6457560064.0000 - mape: 99.0919 - val_loss: 6116202496.0000 - val_mape: 97.9530\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 6456983040.0000 - mape: 99.0814 - val_loss: 6115631104.0000 - val_mape: 97.9289\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6456382976.0000 - mape: 99.0699 - val_loss: 6115067904.0000 - val_mape: 97.9051\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 6455785472.0000 - mape: 99.0599 - val_loss: 6114484736.0000 - val_mape: 97.8805\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6455179264.0000 - mape: 99.0484 - val_loss: 6113923072.0000 - val_mape: 97.8568\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 6454583808.0000 - mape: 99.0372 - val_loss: 6113358848.0000 - val_mape: 97.8330\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6453985792.0000 - mape: 99.0265 - val_loss: 6112788992.0000 - val_mape: 97.8089\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6453378560.0000 - mape: 99.0170 - val_loss: 6112207360.0000 - val_mape: 97.7843\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6452764672.0000 - mape: 99.0052 - val_loss: 6111592448.0000 - val_mape: 97.7584\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6452134400.0000 - mape: 98.9943 - val_loss: 6111025664.0000 - val_mape: 97.7345\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 3s 247ms/step - loss: 6451529728.0000 - mape: 98.9827 - val_loss: 6110448640.0000 - val_mape: 97.7101\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 3s 246ms/step - loss: 6450915328.0000 - mape: 98.9715 - val_loss: 6109848576.0000 - val_mape: 97.6848\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6450287104.0000 - mape: 98.9604 - val_loss: 6109261824.0000 - val_mape: 97.6600\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 3s 250ms/step - loss: 6449665024.0000 - mape: 98.9489 - val_loss: 6108642816.0000 - val_mape: 97.6339\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6449018880.0000 - mape: 98.9388 - val_loss: 6108024832.0000 - val_mape: 97.6078\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 3s 240ms/step - loss: 6448374784.0000 - mape: 98.9268 - val_loss: 6107433472.0000 - val_mape: 97.5828\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 3s 263ms/step - loss: 6447747072.0000 - mape: 98.9152 - val_loss: 6106821632.0000 - val_mape: 97.5570\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 3s 261ms/step - loss: 6447101952.0000 - mape: 98.9035 - val_loss: 6106208256.0000 - val_mape: 97.5311\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 3s 245ms/step - loss: 6446455296.0000 - mape: 98.8927 - val_loss: 6105603584.0000 - val_mape: 97.5055\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 3s 242ms/step - loss: 6445820928.0000 - mape: 98.8800 - val_loss: 6104974848.0000 - val_mape: 97.4790\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 3s 237ms/step - loss: 6445156352.0000 - mape: 98.8694 - val_loss: 6104341504.0000 - val_mape: 97.4522\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 6444501504.0000 - mape: 98.8561 - val_loss: 6103733760.0000 - val_mape: 97.4265\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 3s 235ms/step - loss: 6443851776.0000 - mape: 98.8446 - val_loss: 6103105536.0000 - val_mape: 97.4000\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 3s 241ms/step - loss: 6443180032.0000 - mape: 98.8338 - val_loss: 6102471680.0000 - val_mape: 97.3732\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 3s 226ms/step - loss: 6442518016.0000 - mape: 98.8209 - val_loss: 6101830656.0000 - val_mape: 97.3461\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 3s 226ms/step - loss: 6441848832.0000 - mape: 98.8099 - val_loss: 6101212160.0000 - val_mape: 97.3200\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6441191424.0000 - mape: 98.7966 - val_loss: 6100574720.0000 - val_mape: 97.2931\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 6440512512.0000 - mape: 98.7858 - val_loss: 6099928576.0000 - val_mape: 97.2657\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 3s 221ms/step - loss: 6439837696.0000 - mape: 98.7725 - val_loss: 6099271168.0000 - val_mape: 97.2379\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6439146496.0000 - mape: 98.7611 - val_loss: 6098615296.0000 - val_mape: 97.2102\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6438456320.0000 - mape: 98.7476 - val_loss: 6097971200.0000 - val_mape: 97.1830\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 207ms/step - loss: 6437773312.0000 - mape: 98.7360 - val_loss: 6097316864.0000 - val_mape: 97.1553\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6437082624.0000 - mape: 98.7230 - val_loss: 6096643584.0000 - val_mape: 97.1269\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 6436373504.0000 - mape: 98.7108 - val_loss: 6095953920.0000 - val_mape: 97.0977\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6435660800.0000 - mape: 98.6986 - val_loss: 6095315968.0000 - val_mape: 97.0707\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6434980352.0000 - mape: 98.6860 - val_loss: 6094652928.0000 - val_mape: 97.0427\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6434269184.0000 - mape: 98.6733 - val_loss: 6093964288.0000 - val_mape: 97.0136\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6433553920.0000 - mape: 98.6602 - val_loss: 6093282304.0000 - val_mape: 96.9847\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 6432842752.0000 - mape: 98.6466 - val_loss: 6092605952.0000 - val_mape: 96.9561\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6432120320.0000 - mape: 98.6355 - val_loss: 6091924480.0000 - val_mape: 96.9273\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6431404544.0000 - mape: 98.6226 - val_loss: 6091225600.0000 - val_mape: 96.8977\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6430679040.0000 - mape: 98.6089 - val_loss: 6090553856.0000 - val_mape: 96.8693\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 3s 253ms/step - loss: 6429956096.0000 - mape: 98.5965 - val_loss: 6089838080.0000 - val_mape: 96.8390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6dfa3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/2308850997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_5.predict(X_test) \n",
    "print(y_pred)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71da140",
   "metadata": {},
   "source": [
    "### Train model 6, GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "105727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 6th model layers architecture\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_6 = Sequential()\n",
    "rnn_model_6.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_6.add(GRU(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_6.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_6.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d7e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_6, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_6.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4532805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 30)                4680      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,001\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_6.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b164948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 5s 129ms/step - loss: 6508211712.0000 - mape: 99.9973 - val_loss: 6164634624.0000 - val_mape: 99.9925\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6508042240.0000 - mape: 99.9963 - val_loss: 6164467200.0000 - val_mape: 99.9919\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6507843584.0000 - mape: 99.9952 - val_loss: 6164217856.0000 - val_mape: 99.9901\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6507555328.0000 - mape: 99.9932 - val_loss: 6163867648.0000 - val_mape: 99.9878\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6507193856.0000 - mape: 99.9904 - val_loss: 6163424256.0000 - val_mape: 99.9827\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 6506816000.0000 - mape: 99.9868 - val_loss: 6163078144.0000 - val_mape: 99.9791\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6506491904.0000 - mape: 99.9842 - val_loss: 6162754048.0000 - val_mape: 99.9760\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6506182656.0000 - mape: 99.9819 - val_loss: 6162455040.0000 - val_mape: 99.9730\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6505881088.0000 - mape: 99.9794 - val_loss: 6162149376.0000 - val_mape: 99.9700\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6505568768.0000 - mape: 99.9768 - val_loss: 6161831936.0000 - val_mape: 99.9667\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6505258496.0000 - mape: 99.9743 - val_loss: 6161536512.0000 - val_mape: 99.9632\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6504960512.0000 - mape: 99.9716 - val_loss: 6161231872.0000 - val_mape: 99.9592\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504649728.0000 - mape: 99.9688 - val_loss: 6160894976.0000 - val_mape: 99.9548\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504318976.0000 - mape: 99.9659 - val_loss: 6160561152.0000 - val_mape: 99.9501\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6503980544.0000 - mape: 99.9627 - val_loss: 6160230400.0000 - val_mape: 99.9449\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6503644672.0000 - mape: 99.9595 - val_loss: 6159876096.0000 - val_mape: 99.9395\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6503286784.0000 - mape: 99.9558 - val_loss: 6159512576.0000 - val_mape: 99.9339\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6502918144.0000 - mape: 99.9522 - val_loss: 6159133184.0000 - val_mape: 99.9282\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6502407680.0000 - mape: 99.9417 - val_loss: 6158757376.0000 - val_mape: 99.9231\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6501965824.0000 - mape: 99.9346 - val_loss: 6158361600.0000 - val_mape: 99.9176\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6501551104.0000 - mape: 99.9300 - val_loss: 6157952512.0000 - val_mape: 99.9106\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6501090304.0000 - mape: 99.9150 - val_loss: 6157492224.0000 - val_mape: 99.8980\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6500603392.0000 - mape: 99.9062 - val_loss: 6156943360.0000 - val_mape: 99.8589\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6499983360.0000 - mape: 99.8780 - val_loss: 6156482048.0000 - val_mape: 99.7735\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6499341312.0000 - mape: 99.8572 - val_loss: 6155995648.0000 - val_mape: 99.7353\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6498842624.0000 - mape: 99.8481 - val_loss: 6155539456.0000 - val_mape: 99.6960\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6498355200.0000 - mape: 99.8392 - val_loss: 6155056640.0000 - val_mape: 99.6558\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6497838080.0000 - mape: 99.8294 - val_loss: 6154546176.0000 - val_mape: 99.5968\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6497308160.0000 - mape: 99.8169 - val_loss: 6154052608.0000 - val_mape: 99.5534\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6496780288.0000 - mape: 99.8004 - val_loss: 6153535488.0000 - val_mape: 99.5275\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6496231424.0000 - mape: 99.7856 - val_loss: 6152996352.0000 - val_mape: 99.5039\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6495663104.0000 - mape: 99.7741 - val_loss: 6152457216.0000 - val_mape: 99.4806\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6495090688.0000 - mape: 99.7638 - val_loss: 6151896064.0000 - val_mape: 99.4566\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 6494498816.0000 - mape: 99.7536 - val_loss: 6151331328.0000 - val_mape: 99.4327\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6493900288.0000 - mape: 99.7415 - val_loss: 6150763520.0000 - val_mape: 99.4086\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6493298176.0000 - mape: 99.7309 - val_loss: 6150177280.0000 - val_mape: 99.3839\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6492681216.0000 - mape: 99.7201 - val_loss: 6149591040.0000 - val_mape: 99.3592\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6492054528.0000 - mape: 99.7093 - val_loss: 6148975616.0000 - val_mape: 99.3333\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6491406848.0000 - mape: 99.6963 - val_loss: 6148365824.0000 - val_mape: 99.3076\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6490754560.0000 - mape: 99.6867 - val_loss: 6147750912.0000 - val_mape: 99.2818\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6490104320.0000 - mape: 99.6734 - val_loss: 6147119616.0000 - val_mape: 99.2553\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6489435136.0000 - mape: 99.6616 - val_loss: 6146464768.0000 - val_mape: 99.2277\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6488743936.0000 - mape: 99.6503 - val_loss: 6145799680.0000 - val_mape: 99.1997\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6488038400.0000 - mape: 99.6362 - val_loss: 6145108992.0000 - val_mape: 99.1707\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6487322112.0000 - mape: 99.6233 - val_loss: 6144437760.0000 - val_mape: 99.1424\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6486604800.0000 - mape: 99.6104 - val_loss: 6143751680.0000 - val_mape: 99.1136\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6485872128.0000 - mape: 99.5988 - val_loss: 6143042560.0000 - val_mape: 99.0837\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6485126656.0000 - mape: 99.5848 - val_loss: 6142319616.0000 - val_mape: 99.0533\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6484364288.0000 - mape: 99.5706 - val_loss: 6141584384.0000 - val_mape: 99.0224\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6483596800.0000 - mape: 99.5566 - val_loss: 6140859904.0000 - val_mape: 98.9919\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6482822656.0000 - mape: 99.5433 - val_loss: 6140128256.0000 - val_mape: 98.9612\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6482038272.0000 - mape: 99.5301 - val_loss: 6139361280.0000 - val_mape: 98.9289\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6481236480.0000 - mape: 99.5148 - val_loss: 6138595840.0000 - val_mape: 98.8966\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 6480420352.0000 - mape: 99.5015 - val_loss: 6137803776.0000 - val_mape: 98.8633\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6479596544.0000 - mape: 99.4843 - val_loss: 6137031680.0000 - val_mape: 98.8308\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6478767104.0000 - mape: 99.4708 - val_loss: 6136214528.0000 - val_mape: 98.7964\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6477908992.0000 - mape: 99.4559 - val_loss: 6135417344.0000 - val_mape: 98.7628\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6477062144.0000 - mape: 99.4412 - val_loss: 6134595584.0000 - val_mape: 98.7283\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6476193280.0000 - mape: 99.4239 - val_loss: 6133774848.0000 - val_mape: 98.6937\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 6475320832.0000 - mape: 99.4083 - val_loss: 6132915200.0000 - val_mape: 98.6575\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6474423296.0000 - mape: 99.3931 - val_loss: 6132058624.0000 - val_mape: 98.6214\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6473522176.0000 - mape: 99.3777 - val_loss: 6131198976.0000 - val_mape: 98.5852\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6472606720.0000 - mape: 99.3596 - val_loss: 6130316800.0000 - val_mape: 98.5480\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6471676928.0000 - mape: 99.3451 - val_loss: 6129436672.0000 - val_mape: 98.5109\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6470746624.0000 - mape: 99.3261 - val_loss: 6128567808.0000 - val_mape: 98.4743\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6469812224.0000 - mape: 99.3113 - val_loss: 6127654400.0000 - val_mape: 98.4358\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6468856320.0000 - mape: 99.2924 - val_loss: 6126720512.0000 - val_mape: 98.3965\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6467873280.0000 - mape: 99.2765 - val_loss: 6125779968.0000 - val_mape: 98.3569\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6466885120.0000 - mape: 99.2589 - val_loss: 6124849664.0000 - val_mape: 98.3176\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6465903616.0000 - mape: 99.2397 - val_loss: 6123893248.0000 - val_mape: 98.2773\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6464904192.0000 - mape: 99.2229 - val_loss: 6122954752.0000 - val_mape: 98.2378\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6463894528.0000 - mape: 99.2048 - val_loss: 6121978368.0000 - val_mape: 98.1966\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6462860800.0000 - mape: 99.1850 - val_loss: 6120998400.0000 - val_mape: 98.1553\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6461832192.0000 - mape: 99.1669 - val_loss: 6120025088.0000 - val_mape: 98.1142\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6460799488.0000 - mape: 99.1482 - val_loss: 6119008768.0000 - val_mape: 98.0713\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6459728384.0000 - mape: 99.1279 - val_loss: 6117980672.0000 - val_mape: 98.0280\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6458642944.0000 - mape: 99.1093 - val_loss: 6116942848.0000 - val_mape: 97.9842\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6457556480.0000 - mape: 99.0905 - val_loss: 6115907072.0000 - val_mape: 97.9405\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6456460288.0000 - mape: 99.0715 - val_loss: 6114870272.0000 - val_mape: 97.8968\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6455366656.0000 - mape: 99.0514 - val_loss: 6113846272.0000 - val_mape: 97.8535\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6454277120.0000 - mape: 99.0312 - val_loss: 6112775168.0000 - val_mape: 97.8084\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6453141504.0000 - mape: 99.0111 - val_loss: 6111676928.0000 - val_mape: 97.7620\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6451986944.0000 - mape: 98.9897 - val_loss: 6110573568.0000 - val_mape: 97.7154\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6450829312.0000 - mape: 98.9686 - val_loss: 6109471744.0000 - val_mape: 97.6689\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6449666560.0000 - mape: 98.9486 - val_loss: 6108335104.0000 - val_mape: 97.6209\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6448484864.0000 - mape: 98.9273 - val_loss: 6107253760.0000 - val_mape: 97.5752\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6447322624.0000 - mape: 98.9067 - val_loss: 6106108928.0000 - val_mape: 97.5269\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6446103552.0000 - mape: 98.8846 - val_loss: 6104939008.0000 - val_mape: 97.4774\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6444893184.0000 - mape: 98.8638 - val_loss: 6103780352.0000 - val_mape: 97.4285\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6443668992.0000 - mape: 98.8414 - val_loss: 6102649856.0000 - val_mape: 97.3808\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6442475520.0000 - mape: 98.8189 - val_loss: 6101472768.0000 - val_mape: 97.3310\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6441232896.0000 - mape: 98.7979 - val_loss: 6100315648.0000 - val_mape: 97.2821\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 6439998976.0000 - mape: 98.7733 - val_loss: 6099154944.0000 - val_mape: 97.2330\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6438756864.0000 - mape: 98.7532 - val_loss: 6097930752.0000 - val_mape: 97.1813\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6437481984.0000 - mape: 98.7291 - val_loss: 6096718336.0000 - val_mape: 97.1301\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6436197376.0000 - mape: 98.7063 - val_loss: 6095474176.0000 - val_mape: 97.0774\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6434888704.0000 - mape: 98.6848 - val_loss: 6094214656.0000 - val_mape: 97.0242\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6433575424.0000 - mape: 98.6584 - val_loss: 6092983296.0000 - val_mape: 96.9721\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6432267264.0000 - mape: 98.6346 - val_loss: 6091751424.0000 - val_mape: 96.9200\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6430962688.0000 - mape: 98.6143 - val_loss: 6090492928.0000 - val_mape: 96.8668\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6429633536.0000 - mape: 98.5870 - val_loss: 6089175040.0000 - val_mape: 96.8110\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6428260352.0000 - mape: 98.5638 - val_loss: 6087925760.0000 - val_mape: 96.7581\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6426927104.0000 - mape: 98.5411 - val_loss: 6086617088.0000 - val_mape: 96.7027\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6425555968.0000 - mape: 98.5138 - val_loss: 6085335552.0000 - val_mape: 96.6485\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 124ms/step - loss: 6424166400.0000 - mape: 98.4901 - val_loss: 6083976192.0000 - val_mape: 96.5909\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6422757888.0000 - mape: 98.4661 - val_loss: 6082606592.0000 - val_mape: 96.5329\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6421326336.0000 - mape: 98.4403 - val_loss: 6081283072.0000 - val_mape: 96.4769\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6419930112.0000 - mape: 98.4140 - val_loss: 6079962624.0000 - val_mape: 96.4209\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6418513408.0000 - mape: 98.3884 - val_loss: 6078555136.0000 - val_mape: 96.3613\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6417040384.0000 - mape: 98.3641 - val_loss: 6077146624.0000 - val_mape: 96.3017\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6415579648.0000 - mape: 98.3363 - val_loss: 6075779584.0000 - val_mape: 96.2437\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6414133760.0000 - mape: 98.3092 - val_loss: 6074428416.0000 - val_mape: 96.1864\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6412691968.0000 - mape: 98.2814 - val_loss: 6073060864.0000 - val_mape: 96.1285\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6411232256.0000 - mape: 98.2573 - val_loss: 6071585792.0000 - val_mape: 96.0660\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6409705472.0000 - mape: 98.2286 - val_loss: 6070164480.0000 - val_mape: 96.0057\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6408215552.0000 - mape: 98.2012 - val_loss: 6068771840.0000 - val_mape: 95.9466\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6406695424.0000 - mape: 98.1791 - val_loss: 6067277824.0000 - val_mape: 95.8833\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6405156352.0000 - mape: 98.1478 - val_loss: 6065827328.0000 - val_mape: 95.8218\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6403625984.0000 - mape: 98.1188 - val_loss: 6064399360.0000 - val_mape: 95.7611\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 6402102784.0000 - mape: 98.0930 - val_loss: 6062923776.0000 - val_mape: 95.6985\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6400555008.0000 - mape: 98.0643 - val_loss: 6061441536.0000 - val_mape: 95.6356\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6398985216.0000 - mape: 98.0361 - val_loss: 6059893248.0000 - val_mape: 95.5699\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6397365248.0000 - mape: 98.0084 - val_loss: 6058378240.0000 - val_mape: 95.5056\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6395768320.0000 - mape: 97.9791 - val_loss: 6056823296.0000 - val_mape: 95.4396\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6394146304.0000 - mape: 97.9494 - val_loss: 6055345152.0000 - val_mape: 95.3768\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6392554496.0000 - mape: 97.9219 - val_loss: 6053754368.0000 - val_mape: 95.3092\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6390905344.0000 - mape: 97.8900 - val_loss: 6052233216.0000 - val_mape: 95.2446\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6389294592.0000 - mape: 97.8602 - val_loss: 6050713600.0000 - val_mape: 95.1800\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6387662848.0000 - mape: 97.8327 - val_loss: 6049112576.0000 - val_mape: 95.1120\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6385982464.0000 - mape: 97.8032 - val_loss: 6047502848.0000 - val_mape: 95.0436\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6384291328.0000 - mape: 97.7720 - val_loss: 6045906944.0000 - val_mape: 94.9757\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6382619648.0000 - mape: 97.7391 - val_loss: 6044357632.0000 - val_mape: 94.9098\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6380968960.0000 - mape: 97.7087 - val_loss: 6042738176.0000 - val_mape: 94.8410\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6379260416.0000 - mape: 97.6793 - val_loss: 6041067008.0000 - val_mape: 94.7699\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6377503232.0000 - mape: 97.6504 - val_loss: 6039407616.0000 - val_mape: 94.6993\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6375778816.0000 - mape: 97.6185 - val_loss: 6037799424.0000 - val_mape: 94.6309\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6374064128.0000 - mape: 97.5849 - val_loss: 6036166656.0000 - val_mape: 94.5614\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6372328960.0000 - mape: 97.5563 - val_loss: 6034502656.0000 - val_mape: 94.4905\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6370585600.0000 - mape: 97.5222 - val_loss: 6032770048.0000 - val_mape: 94.4168\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6368783360.0000 - mape: 97.4879 - val_loss: 6031115264.0000 - val_mape: 94.3463\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6367031296.0000 - mape: 97.4557 - val_loss: 6029469184.0000 - val_mape: 94.2762\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6365264896.0000 - mape: 97.4261 - val_loss: 6027737088.0000 - val_mape: 94.2024\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6363429888.0000 - mape: 97.3957 - val_loss: 6026048512.0000 - val_mape: 94.1305\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6361663488.0000 - mape: 97.3621 - val_loss: 6024319488.0000 - val_mape: 94.0568\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6359835648.0000 - mape: 97.3268 - val_loss: 6022560256.0000 - val_mape: 93.9818\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6357975040.0000 - mape: 97.2958 - val_loss: 6020725760.0000 - val_mape: 93.9036\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6356091392.0000 - mape: 97.2630 - val_loss: 6018994176.0000 - val_mape: 93.8298\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6354273792.0000 - mape: 97.2287 - val_loss: 6017351168.0000 - val_mape: 93.7597\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6352474624.0000 - mape: 97.1984 - val_loss: 6015517696.0000 - val_mape: 93.6815\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6350583296.0000 - mape: 97.1595 - val_loss: 6013742080.0000 - val_mape: 93.6058\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6348686336.0000 - mape: 97.1284 - val_loss: 6011880448.0000 - val_mape: 93.5263\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6346758144.0000 - mape: 97.0912 - val_loss: 6010066944.0000 - val_mape: 93.4489\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6344843776.0000 - mape: 97.0563 - val_loss: 6008284160.0000 - val_mape: 93.3728\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6342960128.0000 - mape: 97.0206 - val_loss: 6006459904.0000 - val_mape: 93.2949\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6341023232.0000 - mape: 96.9864 - val_loss: 6004575744.0000 - val_mape: 93.2145\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6339052544.0000 - mape: 96.9504 - val_loss: 6002738176.0000 - val_mape: 93.1360\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6337113600.0000 - mape: 96.9151 - val_loss: 6000917504.0000 - val_mape: 93.0582\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6335173632.0000 - mape: 96.8808 - val_loss: 5998990336.0000 - val_mape: 92.9758\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6333147136.0000 - mape: 96.8432 - val_loss: 5997108736.0000 - val_mape: 92.8954\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6331169280.0000 - mape: 96.8103 - val_loss: 5995202048.0000 - val_mape: 92.8139\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6329155072.0000 - mape: 96.7735 - val_loss: 5993335296.0000 - val_mape: 92.7341\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6327190528.0000 - mape: 96.7351 - val_loss: 5991405056.0000 - val_mape: 92.6515\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6325153792.0000 - mape: 96.6988 - val_loss: 5989504512.0000 - val_mape: 92.5702\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6323144704.0000 - mape: 96.6628 - val_loss: 5987554304.0000 - val_mape: 92.4868\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6321076736.0000 - mape: 96.6277 - val_loss: 5985626624.0000 - val_mape: 92.4043\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6319032832.0000 - mape: 96.5909 - val_loss: 5983627776.0000 - val_mape: 92.3187\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6316977152.0000 - mape: 96.5624 - val_loss: 5981710336.0000 - val_mape: 92.2366\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6314912768.0000 - mape: 96.5360 - val_loss: 5979668480.0000 - val_mape: 92.1492\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6312787968.0000 - mape: 96.5087 - val_loss: 5977705472.0000 - val_mape: 92.0651\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6310737920.0000 - mape: 96.4864 - val_loss: 5975773184.0000 - val_mape: 91.9823\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6308651008.0000 - mape: 96.4617 - val_loss: 5973754368.0000 - val_mape: 91.8958\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6306545152.0000 - mape: 96.4315 - val_loss: 5971729408.0000 - val_mape: 91.8090\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6304386048.0000 - mape: 96.4072 - val_loss: 5969629184.0000 - val_mape: 91.7190\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6302209024.0000 - mape: 96.3817 - val_loss: 5967577088.0000 - val_mape: 91.6309\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6300062208.0000 - mape: 96.3525 - val_loss: 5965621248.0000 - val_mape: 91.5471\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6297961472.0000 - mape: 96.3282 - val_loss: 5963559424.0000 - val_mape: 91.4586\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6295784448.0000 - mape: 96.3028 - val_loss: 5961435648.0000 - val_mape: 91.3675\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6293570560.0000 - mape: 96.2726 - val_loss: 5959380480.0000 - val_mape: 91.2793\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6291407360.0000 - mape: 96.2494 - val_loss: 5957314048.0000 - val_mape: 91.1906\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6289189888.0000 - mape: 96.2204 - val_loss: 5955155456.0000 - val_mape: 91.0979\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 6286933504.0000 - mape: 96.1907 - val_loss: 5953028096.0000 - val_mape: 91.0065\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6284728320.0000 - mape: 96.1665 - val_loss: 5950963200.0000 - val_mape: 90.9178\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 6282517504.0000 - mape: 96.1411 - val_loss: 5948808192.0000 - val_mape: 90.8252\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 6280272896.0000 - mape: 96.1091 - val_loss: 5946719232.0000 - val_mape: 90.7354\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6278035456.0000 - mape: 96.0821 - val_loss: 5944541184.0000 - val_mape: 90.6417\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 3s 235ms/step - loss: 6275776000.0000 - mape: 96.0554 - val_loss: 5942462976.0000 - val_mape: 90.5524\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6273534976.0000 - mape: 96.0270 - val_loss: 5940274176.0000 - val_mape: 90.4583\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6271261696.0000 - mape: 96.0005 - val_loss: 5938202112.0000 - val_mape: 90.3691\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6269037568.0000 - mape: 95.9724 - val_loss: 5935952896.0000 - val_mape: 90.2723\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6266678784.0000 - mape: 95.9416 - val_loss: 5933764096.0000 - val_mape: 90.1781\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6264390144.0000 - mape: 95.9147 - val_loss: 5931584000.0000 - val_mape: 90.0842\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6262073344.0000 - mape: 95.8906 - val_loss: 5929377792.0000 - val_mape: 89.9892\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6259736576.0000 - mape: 95.8627 - val_loss: 5927077888.0000 - val_mape: 89.8901\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6257354752.0000 - mape: 95.8250 - val_loss: 5924867072.0000 - val_mape: 89.7949\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 6255002624.0000 - mape: 95.7998 - val_loss: 5922608128.0000 - val_mape: 89.6975\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6252627456.0000 - mape: 95.7692 - val_loss: 5920367616.0000 - val_mape: 89.6009\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6250272256.0000 - mape: 95.7414 - val_loss: 5918111232.0000 - val_mape: 89.5036\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6247875072.0000 - mape: 95.7133 - val_loss: 5915814912.0000 - val_mape: 89.4046\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6245458944.0000 - mape: 95.6858 - val_loss: 5913516544.0000 - val_mape: 89.3054\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6243068416.0000 - mape: 95.6541 - val_loss: 5911335936.0000 - val_mape: 89.2113\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_6, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a7357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    718.29187\n",
       "dtype: float32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_6.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d758d",
   "metadata": {},
   "source": [
    "### Train model 7, GRU X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e91d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 7th model layers architecture\n",
    "> GRU X 2\n",
    "\"\"\"\n",
    "rnn_model_7 = Sequential()\n",
    "rnn_model_7.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_7.add(GRU(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "rnn_model_7.add(GRU(units=20, activation='tanh', return_sequences=True))\n",
    "rnn_model_7.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_7.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15d27674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_7, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_7.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a87bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, None, 30)          4680      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, None, 20)          3120      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, None, 1)           11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,062\n",
      "Trainable params: 8,021\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_7.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7345f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 6s 192ms/step - loss: 6508340736.0000 - mape: 99.9999 - val_loss: 6164800000.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6508286464.0000 - mape: 99.9993 - val_loss: 6164745728.0000 - val_mape: 99.9988\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6508220928.0000 - mape: 99.9987 - val_loss: 6164642816.0000 - val_mape: 99.9978\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6508121600.0000 - mape: 99.9980 - val_loss: 6164547072.0000 - val_mape: 99.9965\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6508022784.0000 - mape: 99.9972 - val_loss: 6164463616.0000 - val_mape: 99.9954\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6507936768.0000 - mape: 99.9965 - val_loss: 6164393984.0000 - val_mape: 99.9944\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6507857920.0000 - mape: 99.9956 - val_loss: 6164306944.0000 - val_mape: 99.9925\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6507777024.0000 - mape: 99.9945 - val_loss: 6164233728.0000 - val_mape: 99.9910\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507706368.0000 - mape: 99.9937 - val_loss: 6164163584.0000 - val_mape: 99.9897\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6507638272.0000 - mape: 99.9929 - val_loss: 6164096000.0000 - val_mape: 99.9884\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6507570176.0000 - mape: 99.9922 - val_loss: 6164026880.0000 - val_mape: 99.9870\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507495424.0000 - mape: 99.9912 - val_loss: 6163943424.0000 - val_mape: 99.9850\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507414016.0000 - mape: 99.9899 - val_loss: 6163870720.0000 - val_mape: 99.9835\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507338752.0000 - mape: 99.9889 - val_loss: 6163795968.0000 - val_mape: 99.9821\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507260928.0000 - mape: 99.9880 - val_loss: 6163718144.0000 - val_mape: 99.9805\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6507180032.0000 - mape: 99.9870 - val_loss: 6163637248.0000 - val_mape: 99.9790\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6507097088.0000 - mape: 99.9861 - val_loss: 6163557376.0000 - val_mape: 99.9774\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6507014144.0000 - mape: 99.9851 - val_loss: 6163469312.0000 - val_mape: 99.9758\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6506925056.0000 - mape: 99.9840 - val_loss: 6163384832.0000 - val_mape: 99.9740\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6506835456.0000 - mape: 99.9829 - val_loss: 6163297280.0000 - val_mape: 99.9722\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6506741760.0000 - mape: 99.9801 - val_loss: 6163199488.0000 - val_mape: 99.9594\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 6506641920.0000 - mape: 99.9777 - val_loss: 6163105280.0000 - val_mape: 99.9525\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6506505728.0000 - mape: 99.9734 - val_loss: 6162980864.0000 - val_mape: 99.9473\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6506353664.0000 - mape: 99.9689 - val_loss: 6162877952.0000 - val_mape: 99.9446\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6506243584.0000 - mape: 99.9672 - val_loss: 6162772992.0000 - val_mape: 99.9415\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6506132992.0000 - mape: 99.9655 - val_loss: 6162663936.0000 - val_mape: 99.9382\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6506018816.0000 - mape: 99.9635 - val_loss: 6162557440.0000 - val_mape: 99.9345\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 6505903104.0000 - mape: 99.9615 - val_loss: 6162443264.0000 - val_mape: 99.9306\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6505783296.0000 - mape: 99.9595 - val_loss: 6162327040.0000 - val_mape: 99.9267\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6505659904.0000 - mape: 99.9574 - val_loss: 6162213888.0000 - val_mape: 99.9225\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6505538048.0000 - mape: 99.9553 - val_loss: 6162095104.0000 - val_mape: 99.9183\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6505414144.0000 - mape: 99.9531 - val_loss: 6161979392.0000 - val_mape: 99.9137\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6505290752.0000 - mape: 99.9474 - val_loss: 6161855488.0000 - val_mape: 99.9095\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6505157120.0000 - mape: 99.9453 - val_loss: 6161723904.0000 - val_mape: 99.8729\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6505018880.0000 - mape: 99.9408 - val_loss: 6161593856.0000 - val_mape: 99.8648\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6504882688.0000 - mape: 99.9380 - val_loss: 6161462272.0000 - val_mape: 99.8589\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504742912.0000 - mape: 99.9357 - val_loss: 6161327616.0000 - val_mape: 99.8528\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6504601088.0000 - mape: 99.9332 - val_loss: 6161193472.0000 - val_mape: 99.8471\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6504458240.0000 - mape: 99.9305 - val_loss: 6161055744.0000 - val_mape: 99.8412\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6504311808.0000 - mape: 99.9277 - val_loss: 6160918016.0000 - val_mape: 99.8353\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6504164864.0000 - mape: 99.9251 - val_loss: 6160771072.0000 - val_mape: 99.8290\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504010240.0000 - mape: 99.9226 - val_loss: 6160622592.0000 - val_mape: 99.8227\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6503854592.0000 - mape: 99.9198 - val_loss: 6160474112.0000 - val_mape: 99.8165\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6503698944.0000 - mape: 99.9168 - val_loss: 6160325632.0000 - val_mape: 99.8102\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503539200.0000 - mape: 99.9143 - val_loss: 6160172032.0000 - val_mape: 99.8037\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6503376896.0000 - mape: 99.9113 - val_loss: 6160019456.0000 - val_mape: 99.7973\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503216128.0000 - mape: 99.9083 - val_loss: 6159863296.0000 - val_mape: 99.7908\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6503049216.0000 - mape: 99.9054 - val_loss: 6159702528.0000 - val_mape: 99.7840\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6502880256.0000 - mape: 99.9020 - val_loss: 6159540224.0000 - val_mape: 99.7772\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6502709248.0000 - mape: 99.8991 - val_loss: 6159379456.0000 - val_mape: 99.7704\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6502535168.0000 - mape: 99.8963 - val_loss: 6159205376.0000 - val_mape: 99.7631\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6502355456.0000 - mape: 99.8926 - val_loss: 6159033344.0000 - val_mape: 99.7559\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6502174720.0000 - mape: 99.8896 - val_loss: 6158865408.0000 - val_mape: 99.7488\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6501993472.0000 - mape: 99.8865 - val_loss: 6158688768.0000 - val_mape: 99.7414\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6501807104.0000 - mape: 99.8833 - val_loss: 6158511104.0000 - val_mape: 99.7340\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6501619200.0000 - mape: 99.8799 - val_loss: 6158329344.0000 - val_mape: 99.7263\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6501428224.0000 - mape: 99.8762 - val_loss: 6158150656.0000 - val_mape: 99.7188\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6501238272.0000 - mape: 99.8730 - val_loss: 6157965824.0000 - val_mape: 99.7110\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6501043200.0000 - mape: 99.8695 - val_loss: 6157777920.0000 - val_mape: 99.7032\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500845056.0000 - mape: 99.8660 - val_loss: 6157587456.0000 - val_mape: 99.6951\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6500642304.0000 - mape: 99.8620 - val_loss: 6157396992.0000 - val_mape: 99.6872\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6500440064.0000 - mape: 99.8587 - val_loss: 6157194240.0000 - val_mape: 99.6787\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6500231168.0000 - mape: 99.8548 - val_loss: 6156998144.0000 - val_mape: 99.6704\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500022784.0000 - mape: 99.8511 - val_loss: 6156804096.0000 - val_mape: 99.6622\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6499815424.0000 - mape: 99.8474 - val_loss: 6156597248.0000 - val_mape: 99.6535\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6499598336.0000 - mape: 99.8438 - val_loss: 6156390400.0000 - val_mape: 99.6449\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6499384320.0000 - mape: 99.8394 - val_loss: 6156190208.0000 - val_mape: 99.6365\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6499167232.0000 - mape: 99.8360 - val_loss: 6155981824.0000 - val_mape: 99.6277\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6498948608.0000 - mape: 99.8315 - val_loss: 6155774976.0000 - val_mape: 99.6190\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6498726912.0000 - mape: 99.8281 - val_loss: 6155556352.0000 - val_mape: 99.6098\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6498498048.0000 - mape: 99.8242 - val_loss: 6155334656.0000 - val_mape: 99.6005\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6498265600.0000 - mape: 99.8196 - val_loss: 6155113472.0000 - val_mape: 99.5912\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6498034176.0000 - mape: 99.8155 - val_loss: 6154892288.0000 - val_mape: 99.5820\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6497800704.0000 - mape: 99.8113 - val_loss: 6154672128.0000 - val_mape: 99.5727\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6497565184.0000 - mape: 99.8069 - val_loss: 6154451456.0000 - val_mape: 99.5634\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6497328128.0000 - mape: 99.8029 - val_loss: 6154214400.0000 - val_mape: 99.5535\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6497083392.0000 - mape: 99.7986 - val_loss: 6153983488.0000 - val_mape: 99.5437\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 2s 205ms/step - loss: 6496836608.0000 - mape: 99.7947 - val_loss: 6153745408.0000 - val_mape: 99.5337\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496588800.0000 - mape: 99.7898 - val_loss: 6153518080.0000 - val_mape: 99.5242\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496348672.0000 - mape: 99.7856 - val_loss: 6153284096.0000 - val_mape: 99.5143\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6496097280.0000 - mape: 99.7812 - val_loss: 6153036288.0000 - val_mape: 99.5039\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6495840768.0000 - mape: 99.7761 - val_loss: 6152793088.0000 - val_mape: 99.4938\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6495583232.0000 - mape: 99.7715 - val_loss: 6152543744.0000 - val_mape: 99.4833\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6495321088.0000 - mape: 99.7672 - val_loss: 6152305664.0000 - val_mape: 99.4733\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6495067136.0000 - mape: 99.7626 - val_loss: 6152055296.0000 - val_mape: 99.4627\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6494800384.0000 - mape: 99.7582 - val_loss: 6151792128.0000 - val_mape: 99.4517\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6494531072.0000 - mape: 99.7531 - val_loss: 6151544832.0000 - val_mape: 99.4413\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6494263808.0000 - mape: 99.7482 - val_loss: 6151287808.0000 - val_mape: 99.4305\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6493992960.0000 - mape: 99.7432 - val_loss: 6151025152.0000 - val_mape: 99.4194\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6493714432.0000 - mape: 99.7383 - val_loss: 6150759936.0000 - val_mape: 99.4083\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6493437952.0000 - mape: 99.7332 - val_loss: 6150505472.0000 - val_mape: 99.3976\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6493167104.0000 - mape: 99.7287 - val_loss: 6150234112.0000 - val_mape: 99.3862\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6492880384.0000 - mape: 99.7234 - val_loss: 6149958656.0000 - val_mape: 99.3746\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6492593152.0000 - mape: 99.7181 - val_loss: 6149687296.0000 - val_mape: 99.3632\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6492306944.0000 - mape: 99.7134 - val_loss: 6149424128.0000 - val_mape: 99.3521\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6492025344.0000 - mape: 99.7084 - val_loss: 6149138944.0000 - val_mape: 99.3401\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6491725312.0000 - mape: 99.7026 - val_loss: 6148849664.0000 - val_mape: 99.3280\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6491425280.0000 - mape: 99.6973 - val_loss: 6148572160.0000 - val_mape: 99.3163\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6491128320.0000 - mape: 99.6926 - val_loss: 6148282368.0000 - val_mape: 99.3041\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6490828800.0000 - mape: 99.6863 - val_loss: 6148003328.0000 - val_mape: 99.2924\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6490530304.0000 - mape: 99.6818 - val_loss: 6147706880.0000 - val_mape: 99.2799\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6490221056.0000 - mape: 99.6758 - val_loss: 6147425792.0000 - val_mape: 99.2681\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489919488.0000 - mape: 99.6707 - val_loss: 6147137024.0000 - val_mape: 99.2560\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6489612288.0000 - mape: 99.6651 - val_loss: 6146836992.0000 - val_mape: 99.2433\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489298432.0000 - mape: 99.6597 - val_loss: 6146542592.0000 - val_mape: 99.2310\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6488986112.0000 - mape: 99.6542 - val_loss: 6146246144.0000 - val_mape: 99.2185\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6488670720.0000 - mape: 99.6485 - val_loss: 6145934336.0000 - val_mape: 99.2054\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6488348160.0000 - mape: 99.6426 - val_loss: 6145631744.0000 - val_mape: 99.1926\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6488026112.0000 - mape: 99.6363 - val_loss: 6145320448.0000 - val_mape: 99.1796\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6487696896.0000 - mape: 99.6308 - val_loss: 6145006592.0000 - val_mape: 99.1663\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6487367680.0000 - mape: 99.6247 - val_loss: 6144690688.0000 - val_mape: 99.1531\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 3s 224ms/step - loss: 6487036416.0000 - mape: 99.6187 - val_loss: 6144380928.0000 - val_mape: 99.1400\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 207ms/step - loss: 6486709760.0000 - mape: 99.6124 - val_loss: 6144065536.0000 - val_mape: 99.1268\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6486368256.0000 - mape: 99.6075 - val_loss: 6143734784.0000 - val_mape: 99.1129\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6486023680.0000 - mape: 99.6012 - val_loss: 6143398912.0000 - val_mape: 99.0988\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6485678592.0000 - mape: 99.5951 - val_loss: 6143091200.0000 - val_mape: 99.0858\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6485347328.0000 - mape: 99.5890 - val_loss: 6142763008.0000 - val_mape: 99.0720\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6484999680.0000 - mape: 99.5825 - val_loss: 6142427136.0000 - val_mape: 99.0579\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6484645376.0000 - mape: 99.5769 - val_loss: 6142093312.0000 - val_mape: 99.0438\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6484298752.0000 - mape: 99.5694 - val_loss: 6141759488.0000 - val_mape: 99.0298\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6483947008.0000 - mape: 99.5636 - val_loss: 6141423104.0000 - val_mape: 99.0156\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6483587072.0000 - mape: 99.5571 - val_loss: 6141078016.0000 - val_mape: 99.0011\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6483226112.0000 - mape: 99.5513 - val_loss: 6140733440.0000 - val_mape: 98.9866\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6482865664.0000 - mape: 99.5443 - val_loss: 6140393472.0000 - val_mape: 98.9723\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6482504192.0000 - mape: 99.5381 - val_loss: 6140046336.0000 - val_mape: 98.9577\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6482135040.0000 - mape: 99.5315 - val_loss: 6139686400.0000 - val_mape: 98.9425\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6481763840.0000 - mape: 99.5241 - val_loss: 6139338752.0000 - val_mape: 98.9279\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6481391616.0000 - mape: 99.5186 - val_loss: 6138976768.0000 - val_mape: 98.9127\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6481013760.0000 - mape: 99.5111 - val_loss: 6138623488.0000 - val_mape: 98.8978\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6480641024.0000 - mape: 99.5044 - val_loss: 6138269696.0000 - val_mape: 98.8829\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6480264704.0000 - mape: 99.4981 - val_loss: 6137910784.0000 - val_mape: 98.8678\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6479885824.0000 - mape: 99.4907 - val_loss: 6137543168.0000 - val_mape: 98.8523\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6479494656.0000 - mape: 99.4842 - val_loss: 6137167872.0000 - val_mape: 98.8365\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6479104512.0000 - mape: 99.4775 - val_loss: 6136788992.0000 - val_mape: 98.8206\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6478711808.0000 - mape: 99.4695 - val_loss: 6136430592.0000 - val_mape: 98.8055\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6478324736.0000 - mape: 99.4633 - val_loss: 6136050688.0000 - val_mape: 98.7895\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6477929472.0000 - mape: 99.4555 - val_loss: 6135675392.0000 - val_mape: 98.7737\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477530112.0000 - mape: 99.4483 - val_loss: 6135296000.0000 - val_mape: 98.7577\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477130240.0000 - mape: 99.4417 - val_loss: 6134906368.0000 - val_mape: 98.7413\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6476723200.0000 - mape: 99.4340 - val_loss: 6134532096.0000 - val_mape: 98.7256\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6476329472.0000 - mape: 99.4268 - val_loss: 6134149120.0000 - val_mape: 98.7094\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6475919360.0000 - mape: 99.4200 - val_loss: 6133744640.0000 - val_mape: 98.6924\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6475498496.0000 - mape: 99.4128 - val_loss: 6133349376.0000 - val_mape: 98.6758\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6475084288.0000 - mape: 99.4054 - val_loss: 6132956160.0000 - val_mape: 98.6592\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6474669056.0000 - mape: 99.3976 - val_loss: 6132567040.0000 - val_mape: 98.6428\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6474255360.0000 - mape: 99.3904 - val_loss: 6132171776.0000 - val_mape: 98.6262\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6473837568.0000 - mape: 99.3823 - val_loss: 6131761664.0000 - val_mape: 98.6089\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6473412096.0000 - mape: 99.3750 - val_loss: 6131362816.0000 - val_mape: 98.5921\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 6472987648.0000 - mape: 99.3669 - val_loss: 6130956288.0000 - val_mape: 98.5749\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6472557568.0000 - mape: 99.3603 - val_loss: 6130549248.0000 - val_mape: 98.5578\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 3s 242ms/step - loss: 6472132096.0000 - mape: 99.3515 - val_loss: 6130147328.0000 - val_mape: 98.5409\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6471698944.0000 - mape: 99.3448 - val_loss: 6129713152.0000 - val_mape: 98.5226\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6471254528.0000 - mape: 99.3361 - val_loss: 6129302016.0000 - val_mape: 98.5052\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 3s 223ms/step - loss: 6470817280.0000 - mape: 99.3281 - val_loss: 6128883200.0000 - val_mape: 98.4876\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6470372864.0000 - mape: 99.3205 - val_loss: 6128456704.0000 - val_mape: 98.4697\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6469926400.0000 - mape: 99.3127 - val_loss: 6128030720.0000 - val_mape: 98.4517\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 3s 253ms/step - loss: 6469479424.0000 - mape: 99.3053 - val_loss: 6127604736.0000 - val_mape: 98.4337\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 3s 257ms/step - loss: 6469031424.0000 - mape: 99.2967 - val_loss: 6127178240.0000 - val_mape: 98.4158\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 3s 208ms/step - loss: 6468581376.0000 - mape: 99.2887 - val_loss: 6126760448.0000 - val_mape: 98.3982\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6468137472.0000 - mape: 99.2809 - val_loss: 6126326784.0000 - val_mape: 98.3799\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6467680768.0000 - mape: 99.2721 - val_loss: 6125899776.0000 - val_mape: 98.3619\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6467228160.0000 - mape: 99.2648 - val_loss: 6125454848.0000 - val_mape: 98.3431\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6466763264.0000 - mape: 99.2553 - val_loss: 6125008384.0000 - val_mape: 98.3243\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6466293760.0000 - mape: 99.2467 - val_loss: 6124583936.0000 - val_mape: 98.3064\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6465838080.0000 - mape: 99.2402 - val_loss: 6124129280.0000 - val_mape: 98.2873\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 6465360896.0000 - mape: 99.2313 - val_loss: 6123678208.0000 - val_mape: 98.2682\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6464889856.0000 - mape: 99.2221 - val_loss: 6123215360.0000 - val_mape: 98.2487\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6464412672.0000 - mape: 99.2134 - val_loss: 6122796032.0000 - val_mape: 98.2311\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6463960576.0000 - mape: 99.2060 - val_loss: 6122342912.0000 - val_mape: 98.2119\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6463479808.0000 - mape: 99.1978 - val_loss: 6121879552.0000 - val_mape: 98.1924\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6462995968.0000 - mape: 99.1884 - val_loss: 6121416192.0000 - val_mape: 98.1729\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6462513152.0000 - mape: 99.1788 - val_loss: 6120978432.0000 - val_mape: 98.1544\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6462044672.0000 - mape: 99.1713 - val_loss: 6120514560.0000 - val_mape: 98.1348\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6461553152.0000 - mape: 99.1620 - val_loss: 6120049664.0000 - val_mape: 98.1152\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6461061120.0000 - mape: 99.1543 - val_loss: 6119570432.0000 - val_mape: 98.0951\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6460563456.0000 - mape: 99.1446 - val_loss: 6119093760.0000 - val_mape: 98.0749\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6460056576.0000 - mape: 99.1356 - val_loss: 6118606848.0000 - val_mape: 98.0544\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6459552768.0000 - mape: 99.1270 - val_loss: 6118130688.0000 - val_mape: 98.0343\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6459054080.0000 - mape: 99.1177 - val_loss: 6117667328.0000 - val_mape: 98.0147\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6458561024.0000 - mape: 99.1088 - val_loss: 6117209600.0000 - val_mape: 97.9954\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6458073088.0000 - mape: 99.1001 - val_loss: 6116715520.0000 - val_mape: 97.9746\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6457556480.0000 - mape: 99.0904 - val_loss: 6116232192.0000 - val_mape: 97.9542\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6457046528.0000 - mape: 99.0812 - val_loss: 6115746816.0000 - val_mape: 97.9337\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6456534528.0000 - mape: 99.0720 - val_loss: 6115257344.0000 - val_mape: 97.9131\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6456018944.0000 - mape: 99.0633 - val_loss: 6114757632.0000 - val_mape: 97.8920\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6455497216.0000 - mape: 99.0532 - val_loss: 6114275328.0000 - val_mape: 97.8716\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6454986752.0000 - mape: 99.0450 - val_loss: 6113780736.0000 - val_mape: 97.8508\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6454462976.0000 - mape: 99.0353 - val_loss: 6113285120.0000 - val_mape: 97.8299\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6453937152.0000 - mape: 99.0260 - val_loss: 6112766976.0000 - val_mape: 97.8080\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6453396992.0000 - mape: 99.0159 - val_loss: 6112254976.0000 - val_mape: 97.7864\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6452864512.0000 - mape: 99.0060 - val_loss: 6111764480.0000 - val_mape: 97.7657\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6452339712.0000 - mape: 98.9964 - val_loss: 6111243264.0000 - val_mape: 97.7437\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6451791360.0000 - mape: 98.9878 - val_loss: 6110729216.0000 - val_mape: 97.7220\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6451253248.0000 - mape: 98.9784 - val_loss: 6110217728.0000 - val_mape: 97.7004\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6450710016.0000 - mape: 98.9674 - val_loss: 6109691904.0000 - val_mape: 97.6782\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6450161664.0000 - mape: 98.9586 - val_loss: 6109177344.0000 - val_mape: 97.6565\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6449615360.0000 - mape: 98.9490 - val_loss: 6108666368.0000 - val_mape: 97.6349\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6449075712.0000 - mape: 98.9388 - val_loss: 6108140544.0000 - val_mape: 97.6127\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6448521216.0000 - mape: 98.9280 - val_loss: 6107608064.0000 - val_mape: 97.5902\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 3s 209ms/step - loss: 6447969792.0000 - mape: 98.9178 - val_loss: 6107094016.0000 - val_mape: 97.5685\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_7, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d1f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[166.99881]\n",
      "  [359.90118]\n",
      "  [420.147  ]\n",
      "  ...\n",
      "  [423.9484 ]\n",
      "  [423.94827]\n",
      "  [423.94815]]\n",
      "\n",
      " [[166.51483]\n",
      "  [359.48444]\n",
      "  [420.10287]\n",
      "  ...\n",
      "  [423.94675]\n",
      "  [423.94656]\n",
      "  [423.9465 ]]\n",
      "\n",
      " [[166.45361]\n",
      "  [359.4035 ]\n",
      "  [420.09558]\n",
      "  ...\n",
      "  [423.94693]\n",
      "  [423.94684]\n",
      "  [423.9467 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[166.50644]\n",
      "  [359.47742]\n",
      "  [420.10526]\n",
      "  ...\n",
      "  [423.94702]\n",
      "  [423.9469 ]\n",
      "  [423.9468 ]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/3491075012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_7.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e588d92b224e11b16adbbadd39936dea13a6488171770263a646fc57f44563d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
