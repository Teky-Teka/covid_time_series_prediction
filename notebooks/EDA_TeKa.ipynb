{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac72ae-9be5-43d4-87d3-b2319a0f757b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Time Series COVID-19 Cases Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a246bb7-0cdf-4939-a4e9-870f871706f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd5297-79e4-4864-bb68-8deb37dbc70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dependencies importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6c99354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reload imported module every time a jupyter cell is executed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de425903-9365-4407-8ae8-9b20a6e41dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import requests\n",
    "import pandas_profiling\n",
    "from typing import overload\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop \n",
    "from covid_time_series_prediction.ml_logic import preprocessor\n",
    "# from ml_logic.country_data import country_output\n",
    "from covid_time_series_prediction.ml_logic.preprocessor import train_test_set\n",
    "from covid_time_series_prediction.dp_logic.RNN_model import model_run, get_RNN_model_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586817c-250d-4813-90a0-a8a859540431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_RNN_model_API('France')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a89aae11-8de5-4100-a9ed-2d28d57a268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332849e5-8682-4079-8266-8a22c70d7d38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee22a40-a282-4e4f-bf18-ce8a7633b59c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e9b20-7339-486d-8ebc-21b0394c5e5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### By country over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c36f91-5db2-4aa7-b598-45ae52f2dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_time_series(feature='stringency', start_date='2020-02-14', end_date='2021-02-14'):\n",
    "    \"\"\"\n",
    "    Get stringency time series for each countries requesting API.\n",
    "    Returns json dict with TS between start_date and end_date like 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    url = f'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/{feature}/date-range/{start_date}/{end_date}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54f6689-5b7f-42e3-891b-57367b4658e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_time_series_api = fetch_time_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bad4c-3c99-4620-aead-b7951a993db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, [c for c in v if c == 'VNM'])  for k, v in countries_time_series_api.items()  if k == 'countries' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59acb5e4-2aee-4aa5-be96-ac8af9d3a96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[([([([(vee)  for kaaa, veee  in vee.items() if kaaa in ['date_value', 'confirmed']  ])  for kaa, vee  in ve.items() if kaa =='VNM'   ])  for ka, ve  in v.items() ])  for k, v in countries_time_series_api.items() if k=='data'   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb51b76-85af-4077-8db0-4b74529104a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[(k, [(ka, [(kaa, vee)  for kaa, vee  in ve.items() if kaa =='USA'   ])  for ka, ve  in v.items() ])  for k, v in countries_time_series_api.items() if k=='data'   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b518b601-27e8-4411-9b10-165c94c89c3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Country data for a specific day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e858c-c9e9-4d78-9d94-804e06fbf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(country='USA', date='2020-08-14'):\n",
    "    \"\"\"\n",
    "    Get stringency data for one country {ALPHA-3} requesting API.\n",
    "    Returns json dict with data for country like 'AAA' and specific date and like 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    url = f'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/actions/{country}/{date}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debbac8-54a8-482a-b339-d5fa249c4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data_api = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c13eee-1681-40ca-b0d2-2ab21437191f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[[';'.join([str(kk) for kk, vv in d.items()]) for i, d in enumerate(v) if type(d) == dict and i == 0] for v in country_data_api.values()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81f0b8-9fc7-4e38-8e24-99408857cb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[[';'.join([str(vv) for kk, vv in d.items()]) for d in v if type(d) == dict] for v in country_data_api.values()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869ee50-edb3-423d-8a75-f05af3f32afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [';'.join([str(vv) for vv in v]) for v in country_data_api.values()][-1]\n",
    "[';'.join([str(kk) for kk in v]) for k, v in country_data_api.items()][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad4771-e464-4eb1-86f5-c5c480cea8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1591b-31c5-4a38-b0fb-4bfaec0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data project directory\n",
    "data_dir = '../data/raw_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7c791-f898-4106-b9a9-f1221fcfb1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Read URL**, **Get CSV files** and **store CSV in local**  *(optional do it at begining or to refresh CSV data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318d9af-8148-4bbc-9dff-7359600f778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_database_to_csv()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42cd6c-5efd-4cad-b74b-d134e2653dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_to_csv(url, csv_list, path='', db_grid=[]) -> list:\n",
    "    \"\"\"\n",
    "    function that take in parameter:\n",
    "     - a root URL (string) to get the CSV data,\n",
    "     - a list of CSV files,\n",
    "     - a path (string) to store CSV in local,\n",
    "     - a grid (list of list) to add in the CSV filename, URL, local path.     \n",
    "    and returns the gird updated with the CSVs of the list\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### Create a database grid (list of list) with all CSVs and associated URLs\n",
    "    # print('db_grid', db_grid)\n",
    "    #### Data project directory (if empty do not store CSV in local)\n",
    "    # print('path', path)\n",
    "    ### Website CSV datasets URL\n",
    "    # print('url', url)\n",
    "    #### List of CSVs of Website to retrieve\n",
    "    # print('csv_list', csv_list)\n",
    "\n",
    "    #### Length of grid aka number of CSVs already stored in grid\n",
    "    len_grid = len(db_grid)\n",
    "\n",
    "    for l in range(len(csv_list)):\n",
    "        sub_list = []       \n",
    "        sub_list.append(csv_list[l]) ## 1st pos°: CSV filename\n",
    "        sub_list.append(url + csv_list[l]) ## 2nd pos°: URL + CSV\n",
    "        if len(data_dir) > 0: ## store CSV in local\n",
    "            sub_list.append(data_dir + csv_list[l]) ## 3rd pos°: local data path + CSV\n",
    "            !curl -L \"{url + csv_list[l]}\" > {data_dir + csv_list[l]} ## curl <URL>/<CSV> => <path>\n",
    "        # print('sub_list', sub_list)\n",
    "        db_grid.append(sub_list)\n",
    "\n",
    "    ### Return a database grid (list of list) with all CSVs and associated URLs\n",
    "    return db_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa98a-c3ba-4108-98bd-5a0b4af0cd17",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Get database to csv** with **get_database_to_csv()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62c742-44a6-46c0-bfca-d5fece235ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Oxford Master data time series URL\n",
    "url_root_oxford = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/'\n",
    "\n",
    "#### List of CSVs of Oxford database Feel free to add more feature...\n",
    "## index_strigency missed!\n",
    "## 'E3;Fiscal measures;' missed!\n",
    "## 'E4;International support;' missed!\n",
    "## 'H3;Contact tracing;' missed!\n",
    "## 'H4;Emergency investment in healthcare!\n",
    "## 'H5;Investment in vaccines missed!\n",
    "## 'V1;Vaccine Prioritisation missed!\n",
    "## 'V2;Vaccine Availability missed!\n",
    "## 'V3;Vaccine Financial Support!\n",
    "## 'V4;Mandatory Vaccination missed!\n",
    "csv_list = ['confirmed_cases.csv', 'confirmed_deaths.csv',\n",
    "            'government_response_index_avg.csv', 'stringency_index_avg.csv', \n",
    "            'containment_health_index_avg.csv', 'economic_support_index.csv',\n",
    "            'c1m_school_closing.csv', 'c2m_workplace_closing.csv',\n",
    "            'c3m_cancel_public_events.csv', 'c4m_restrictions_on_gatherings.csv', \n",
    "            'c5m_close_public_transport.csv', 'c6m_stay_at_home_requirements.csv',\n",
    "            'c7m_movementrestrictions.csv', 'c8ev_internationaltravel.csv',\n",
    "            'e1_income_support.csv', 'e2_debtrelief.csv',\n",
    "            'h1_public_information_campaigns.csv', 'h2_testing_policy.csv',\n",
    "            'h3_contact_tracing.csv', 'h6m_facial_coverings.csv',\n",
    "            'h7_vaccination_policy.csv', 'h8m_protection_of_elderly_ppl.csv'\n",
    "           ] ## ; print('csv_list', csv_list, 'len(csv_list)', len(csv_list))\n",
    "    \n",
    "### Vacinations Dataset URLs\n",
    "url_root_vaccinations = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/'\n",
    "\n",
    "#### List of CSVs of Vaccinations database\n",
    "csv_list_vax = ['vaccinations.csv', 'vaccinations-by-age-group.csv'] ## ; print('csv_list', csv_list_vax, 'len(csv_list)', len(csv_list_vax))\n",
    "\n",
    "### Create a database grid all CSVs and associated URLs from Oxford website\n",
    "db_grid = get_database_to_csv(url_root_oxford, csv_list, data_dir)\n",
    "### Insert into database grid all CSVs and associated URLs from vaccinations website\n",
    "db_grid = get_database_to_csv(url_root_vaccinations, csv_list_vax, data_dir, db_grid)\n",
    "# print('db_grid', db_grid)\n",
    "\n",
    "# Stack all csl in the list\n",
    "csv_list += csv_list_vax ## ; print('csv_list', csv_list)\n",
    "\n",
    "# transform list into dict:\n",
    "csv = dict(zip(csv_list, [v[0] for v in enumerate(csv_list)])) ## ; print ('csv', csv) ## if v[1] == 'containment_health_index_avg.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b3bb-c12b-43a3-9dda-7bbd905f3622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d5c4a-c489-4204-a782-08cfb9a127c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc67099-9098-47c4-9e1d-b2754e07cd70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Read CSV** and **Set raw dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d45ada",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Read raw CSV** and **Set dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_gov_response = pd.read_csv(data_dir + 'government_response_index_avg.csv')\n",
    "df_raw_health = pd.read_csv(data_dir + 'containment_health_index_avg.csv')\n",
    "df_raw_economic = pd.read_csv(data_dir + 'economic_support_index.csv')\n",
    "\n",
    "#### Vaccination\n",
    "df_raw_vaccination = pd.read_csv(data_dir + 'vaccinations.csv')\n",
    "df_raw_ages = pd.read_csv(data_dir + 'vaccinations-by-age-group.csv')\n",
    "\n",
    "\n",
    "#### Data Frame target\n",
    "df_raw_cases = pd.read_csv(data_dir + 'confirmed_cases.csv')\n",
    "df_raw_deaths = pd.read_csv(data_dir + 'confirmed_deaths.csv')\n",
    "\n",
    "#### Data multiple\n",
    "df_raw_school_closing=pd.read_csv(data_dir + 'c1m_school_closing.csv')\n",
    "df_raw_workplace_closing=pd.read_csv(data_dir + 'c2m_workplace_closing.csv')\n",
    "df_raw_cancel_public_event=pd.read_csv(data_dir + 'c3m_cancel_public_events.csv')\n",
    "df_raw_restriction_on_gathering=pd.read_csv(data_dir + 'c4m_restrictions_on_gatherings.csv')\n",
    "df_raw_stay_at_home=pd.read_csv(data_dir + 'c6m_stay_at_home_requirements.csv')\n",
    "df_raw_international_travel=pd.read_csv(data_dir + 'c6m_stay_at_home_requirements.csv')\n",
    "df_raw_goverment_response=pd.read_csv(data_dir + 'government_response_index_avg.csv')\n",
    "df_raw_facial_covering=pd.read_csv(data_dir + 'h6m_facial_coverings.csv')\n",
    "df_raw_vacination_policy=pd.read_csv(data_dir + 'h7_vaccination_policy.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f43d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Read out CSV** and **Set dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647dd56-8eaf-4f56-b52f-173e818e245c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### CSV Data out project directory\n",
    "csv_dir = '../data/csv_out/'\n",
    "# ! unzip {csv_dir}usa_index\n",
    "# ! unzip {csv_dir}usa_indicator\n",
    "# ! rm ECG_data.zip\n",
    "df_usa_index =  pd.read_csv(csv_dir + 'usa_index.csv')\n",
    "df_usa_indicator =  pd.read_csv(csv_dir + 'usa_indicator.csv')\n",
    "df_usa_index, df_usa_indicator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaecf123-5e8d-4e1e-b337-e0164c78fa4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DataFrames setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ed76f-6978-4e8d-84a6-41b4cd8cb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumedha csv test\n",
    "df_ts_usa_index =  df_usa_index.copy()\n",
    "df_ts_usa_indicator =  df_usa_indicator.copy()\n",
    "\n",
    "df_ts_usa_index.head(), df_ts_usa_indicator.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780e433-b780-4736-9011-36e69a97910d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Time series analysing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b05ade-58d9-4909-85f7-270a32dc2480",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Time Series Analysis *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9490e09-78eb-4563-881a-ea7d9f40b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases = df_raw_cases.drop(columns=['country_name','region_code','region_name','jurisdiction','Unnamed: 0'])\n",
    "ts_cases = ts_cases.groupby('country_code').agg('sum')\n",
    "ts_cases.transpose()\n",
    "ts_cases.columns.name = 'Dates'\n",
    "ts_cases = ts_cases.fillna(0)\n",
    "# ts_cases.index = pd.to_datetime(ts_cases.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9518a-a848-45c5-b0cf-a1dc2363b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases = ts_cases.transpose()\n",
    "ts_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e0fb8-f976-4dbe-8243-7a69be38b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5ecb3-6362-430f-bdc8-f8f2298c120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_ts_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54395e-6a75-45b7-a389-7f739b5c35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_ts_cases = vn_ts_cases.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca627f6-23e4-4243-8a68-e0b3108b21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_cases.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75735fc5-e04d-46ae-9d96-92dccdc1aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get VietNam country dataset\n",
    "vn_data = df_raw_cases.loc[df_raw_cases['country_code'] == 'VNM'].copy()\n",
    "\n",
    "vn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f336df2-286a-4819-a011-c1ae0d7cdb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vn_data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122911e-70c8-4131-bf2f-8db270b3ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TENSORFLOW & RNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636afbb-aab0-4500-897b-311961289203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recurrent Neural Network (sequences data) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f081b-7ee6-405b-8e13-04bf267ac2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427dc3-25b3-4a5a-abba-20e92d67ab1b",
   "metadata": {},
   "source": [
    "X.shape = (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES) and y = RNN(X) where $X_{i,j}^{t}$\n",
    "\n",
    "with $_{i}$ is the sample/sequence, $_{j}$ is the feature measured and  $^{t}$ is the time at which the observation is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8151dc-1f0e-4981-91a0-c1164230af26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- **retrieve dataset** from Sumedha & Alberto\n",
    "\n",
    "    - **clean dataset**: \n",
    "        \n",
    "        - **drop first lines == 0** *(before Covid arrived)*\n",
    "        \n",
    "        - **check Nan**: \n",
    "- **strategy 1 country by country** sequences split as follow:\n",
    "\n",
    "- **strategy 2 one sequence per country**:\n",
    "    - **split X train, set** \n",
    "    - **Pad sequences**\n",
    "    - **create one csv per country**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848eaa-a889-47ee-972c-e9f2e54214cc",
   "metadata": {},
   "source": [
    "## Training strategies:\n",
    "- Get NB dataset (cleaned) from Alberto & Sumedha\n",
    "- 1/ Indicator in precentage %\n",
    "- 2/ Indicator as categorical labels\n",
    "- Run same RNN model in parallel with Kim & Thomas\n",
    "- Identify best dataset\n",
    "- Parameters to fit:\n",
    "    - increase **nb of sequences**\n",
    "    - train series modulation (ex: [50, 150, 200, 300, 400 nb of days = n_obs]) < take time to compute\n",
    "    - **learning_rate** in Optimizer(parameters)\n",
    "    - model layers architecture (**simple** -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "        > LSTM\n",
    "        > Dense\n",
    "       (> LSTM\n",
    "        > LSTM\n",
    "        > Dense)\n",
    "     >> **try to overfit** the model with the loss (train over val) or (early_stopping)\n",
    "     >> **(X_val, y_val)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4cdbb-56aa-4522-84a6-833471a7b34a",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ee15a-c507-4049-9489-1f546a1f0a8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series **Analysis** & **Preparation** to training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775435d-8b7a-4742-bcb2-a76e1c2ffb1c",
   "metadata": {},
   "source": [
    "# Alberto's RNN model #4 Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648961ed-ee19-4342-b137-8f5029082404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alberto train set\n",
    "n_seq = 200 ## nb of sequences (samples)\n",
    "n_obs = 70 # maxi = 96 (stay around 70 or more test_split)\n",
    "n_feat = 20 #  X_train.shape[1] # 20 feature:\n",
    "n_pred = 1 # nb of days where we can predict new daily deaths\n",
    "n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d337-90d7-47b1-a4ee-b508e8731c39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test datasplit the dataset into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alberto train set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('France', split_train=0.7, split_val=0.9)\n",
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4fba2-9562-49eb-9b94-7c5ab57fd163",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create sequences (`X`,`y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e6437-7538-4c29-b8d9-d00d4d804064",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Generates an entire dataset of multiple subsamples with shape $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence(X, y, X_len, y_len) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length` (eg n_obs).\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    X_y_len = X_len + y_len\n",
    "    print('_len + y_len',  X_len,  y_len)\n",
    "    print('X.shape[0] >= X_y_len', X.shape[0], X_y_len)\n",
    "    if X.shape[0] >= X_y_len:\n",
    "        last_possible = X.shape[0] - X_y_len\n",
    "    else:\n",
    "        last_possible = X.shape[0]\n",
    "        print('X_y_len = ?', X.shape[0])\n",
    "    # How to split sequences? we could do it manually...\n",
    "    print('X.shape[0]', X.shape[0])\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    # X start and y end\n",
    "    X_sample = X[random_start : random_start + X_len]\n",
    "    y_sample = y[random_start + X_len : (random_start + X_y_len)]\n",
    "    print(\"X[random_start : random_start + X_len]\", f\"X[{random_start} : {random_start + X_len}]\")\n",
    "    print(\"y[random_start : random_start + X_y_len]\", f\"y[{random_start} : {(random_start + X_y_len)}]\")\n",
    "    \n",
    "    return np.array(X_sample), np.array(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a54ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993c868-606c-4d31-ac8d-997bc85d63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "(X_sample, y_sample) = subsample_sequence(X_train, y_train, X_len=n_obs, y_len=n_pred) # n_seq = 200, n_obs = 150\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb0927-e52b-4d41-8dcb-6ca6f204a1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_X_y(df, n_sequences, length)**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no use\n",
    "def get_X_y(df, n_sequences, length, feature='VNM') -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = split_subsample_sequence(df, length, feature=feature)\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15128c-961a-471e-b864-589455120c15",
   "metadata": {},
   "source": [
    "def get_X_y(X_train, y_train, X_test, y_test, X_len, y_len, n_seq_train, X_val=np.array(range(1)), y_val=np.array(range(1))) -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X_train_list = X_val_list = X_test_list = y_list = y_val = y_test = []\n",
    "    # train sequences splitting\n",
    "    for i in range(n_sequences):\n",
    "        # print('X_len', X_len, 'y_len', y_len)\n",
    "        (x_tainr_i, y_train_i) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "        X_list.append(xi)\n",
    "        y_list.append(yi)\n",
    "    X_train_array = np.array(X_list)\n",
    "    y_train_array = np.array(y_list) \n",
    "\n",
    "    if X_val==np.array(range(1)) and y_val==np.array(range(1)):\n",
    "        # val sequences splitting (train / val /test)        \n",
    "        n_seq_val = n_seq_train // 5 # number of sequences in test set ?\n",
    "        n_seq_test = n_seq_train // 10 # number of sequences in test set ?\n",
    "        for i in range(n_s_train):\n",
    "            # print('X_len', X_len, 'y_len', y_len)\n",
    "            (xi, yi) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "            X_list.append(xi)\n",
    "            y_list.append(yi)\n",
    "        X_val_array = np.array(X_list)\n",
    "        y_val_array = np.n_seq_test(y_list) \n",
    "        for i in range(n_seq_val):\n",
    "            # print('X_len', X_len, 'y_len', y_len)\n",
    "            (xi, yi) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "            X_list.append(xi)\n",
    "            y_list.append(yi)\n",
    "        X_test_array = np.array(X_list)\n",
    "        y_test_array = np.array(y_list) \n",
    "        return X_train_array, y_train_array, X_val_array, y_val_array, X_test_array, y_test_array\n",
    "    else:\n",
    "        # test sequences splitting (train / test)\n",
    "        n_seq_test = n_seq_train // 5 # number of sequences in test set ?\n",
    "        for i in range(n_sequences):\n",
    "            # print('X_len', X_len, 'y_len', y_len)\n",
    "            (x_tainr_i, y_train_i) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "            X_list.append(xi)\n",
    "            y_list.append(yi)\n",
    "        X_test_array = np.array(X_list)\n",
    "        y_test_array = np.array(y_list)        \n",
    "        return X_train_array, y_train_array, X_test_array, y_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde1f94-64b6-472d-8007-5a91ec085172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_2(X, y, X_len, y_len, n_sequences) -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "        X_list.append(xi)\n",
    "        y_list.append(yi)\n",
    "        \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9096e3f-477e-45ae-a9a2-bdb1c8163ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq_val = n_seq // 5 # number of sequences in test set ?\n",
    "n_seq_test = n_seq // 10 # number of sequences in test set ?\n",
    "n_seq, n_seq_val, n_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682240c-818e-4e96-9a2a-8b596cd235bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af4e7e-d602-4fcd-a36a-942ab4dc0b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_X_y_2(X_test, y_test, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n",
    "print('X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat')\n",
    "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497196a7-2d6a-4663-82a9-c8873c4ad0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "print('X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat')\n",
    "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a422c04-e3ef-4daf-b870-060c559ad2bb",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('United States', split_train=0.7, split_val=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e2fa4-2cf1-4ad1-bd39-6b48cd2d478d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_test, y_test, X_len=n_obs, y_len=n_pred, n_seq_train=n_seq_test, X_val=X_val, y_val=y_val)\n",
    "X_train.shape, y_train.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826c700-4273-4cd3-b7d7-4c3cca2b3582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y_2(X_train, y_train, X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n",
    "X_train.shape, y_train.shape, n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2a397-4b64-4358-8355-2a243e990afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "X_val.shape, y_val.shape, n_seq_val, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661159e6-4eb3-48cc-b11d-fff40fb81a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alberto train set\n",
    "# 3. Training\n",
    "def train_rnn_model(model, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 1,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "# print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe229ed-38be-485e-9314-5a3c04da8847",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### How to split sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49128bf-f5e8-4319-8635-fa1f63c29ceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- randomly or\n",
    "\n",
    "- manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867c99-9dd9-4dc5-9cda-de741bf57ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **train_rnn_model(model, patience=2, epochs=200):**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ece011-197d-439e-b405-ee2370c9773d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model #4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc24c32-f615-4696-b4e0-01e7d7281ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=20, epochs=200):\n",
    "    \"\"\" function that train a RNN model with hyperparameters:\n",
    "    - patience by default 2 to early stop\n",
    "    - epochs by default 200 to train over several epochs\n",
    "    - valisation data by default (X_val, y_val)=(0, 0) in case of auto split\n",
    "    \"\"\"\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "             # Auto split for validation data\n",
    "            # [print(f'validation_data=(X_val, y_val),') if (X_val!=0 or y_val!=0) else print(f'validation_split=0.1,')],\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "# print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alberto model #4 test \n",
    "# Normalization layer not necessary as X_train already train\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_4 = Sequential()\n",
    "# rnn_model_4.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_4.add(LSTM(units=20, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_4.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_4.add(Dense(n_pred, activation = 'linear'))\n",
    "#ValueError: Input 0 of layer \"lstm_1\" is incompatible with the layer:\n",
    "#     >>> expected ndim=3, found ndim=2. Full shape received: (None, 20)#\n",
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_4.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_4.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "history = train_rnn_model(model=rnn_model_4, epochs=200, patience=3)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "\n",
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "# y_pred = rnn_model.predict(X_test) \n",
    "# print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "# pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f87f4-a522-43df-9486-ac6e4930ae60",
   "metadata": {},
   "source": [
    "# Alberto model #4 End Of the Road..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884eb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=20, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "# print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50f5c4-faa0-4355-a1fd-a28b28062623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. The Normalization Layer\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901218-eb1c-4d85-b40e-9beda6c3c548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model #1 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1267f1-8a86-4f8b-82f5-01478bc0d350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c615d0-d83c-4b7d-b076-b914179fb0ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series Forecasting with model #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a685f57-01a6-4405-8b38-0cc3c7fa5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571254c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_rnn_model(rnn_model, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trai### Train model #1n_series = [50, 150, 200, 300, 400]\n",
    "overfit_es =   [2, 6, 6, 5, 6 ]\n",
    "print('type(overfit_es), overfit_es', type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "for i in range(len(train_series)):\n",
    "    history = train_rnn_model(model=rnn_model_2, epochs=train_series[i], patience=overfit_es[i])\n",
    "    plt.plot(history.history['mape'])\n",
    "    plt.plot(history.history['val_mape'])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1950356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b325a69",
   "metadata": {},
   "source": [
    "#### 🚀 The **LSTM (= Long Short Term Memory)** with their ability to *avoid the vanishing gradient problem*, should be preferred over a SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae1226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1470a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1673f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model #3 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92739ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c972bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model #3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cc51a-a8a3-4a61-9c9e-96317f159d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecee49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_3, patience=2, epochs=200, (X_val, y_val)=(0, 0)):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_3.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "print(type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "# history = train_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923f50a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min(history.history['mape'])\n",
    "\n",
    "# print(\"adjust early stopping\")\n",
    "# overfit_es = [d[0]+1 for d in enumerate(history.history['mape']) if d[1] == min(history.history['mape'])][0]\n",
    "# overfit_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43e359-c428-43f9-9369-f0804a5acdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min(history.history['mape']), max(history.history['mape']), history.history['mape'] # blue line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f6eb0-e734-48c4-a455-49949ea62a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(history.history['val_mape']), history.history['val_mape'] # orange line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba3da5-6613-418e-9919-2f7d50e0078e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model #1 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed1ac5-3b8a-4049-abe8-9431de2121c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32232f29-fc87-49fc-9f6f-11dcb25a9f92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Series Forecasting with model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a68310-d512-4825-a2c4-21cb68e5f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your code below\n",
    "assert y_pred.shape == (n_seq_test, n_pred)\n",
    "# Distribution of the real values y_train\n",
    "pd.DataFrame(y_train).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cad73-73e8-4211-908a-3789b7a03497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the real values y_train\n",
    "pd.DataFrame(y_train).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd17d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trai### Train model #1n_series = [50, 150, 200, 300, 400]\n",
    "overfit_es =   [2, 6, 6, 5, 6 ]\n",
    "print('type(overfit_es), overfit_es', type(overfit_es), overfit_es)\n",
    "# if overfit_es:\n",
    "#     print(\"early stopping\")\n",
    "#     history = train_rnn_model(patience=overfit_es)\n",
    "# else:\n",
    "# print(\"No early stopping\")\n",
    "for i in range(len(train_series)):\n",
    "    history = train_rnn_model(model=rnn_model_2, epochs=train_series[i], patience=overfit_es[i])\n",
    "    plt.plot(history.history['mape'])\n",
    "    plt.plot(history.history['val_mape'])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f576952-8105-43b3-9d97-39d19277c207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3557ab-55e0-4ae2-a289-212d608dc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(X_val=0, y_val=0):\n",
    "    [print(f'validation_data=(X_val, y_val),') if (X_val!=0 or y_val!=0) else print(f'validation_split=0.1,')]\n",
    "    return True\n",
    "\n",
    "train_rnn(),train_rnn((1),(0))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e588d92b224e11b16adbbadd39936dea13a6488171770263a646fc57f44563d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
