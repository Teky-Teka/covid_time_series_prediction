{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac72ae-9be5-43d4-87d3-b2319a0f757b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Time Series COVID-19 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a246bb7-0cdf-4939-a4e9-870f871706f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd5297-79e4-4864-bb68-8deb37dbc70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c99354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload imported module every time a jupyter cell is executed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de425903-9365-4407-8ae8-9b20a6e41dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 17:12:57.553009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-14 17:12:57.553077: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "import seaborn as sns \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import requests\n",
    "import pandas_profiling\n",
    "from typing import overload\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop \n",
    "from covid_time_series_prediction.ml_logic import preprocessor\n",
    "# from ml_logic.country_data import country_output\n",
    "from covid_time_series_prediction.ml_logic.preprocessor import train_test_set, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad4771-e464-4eb1-86f5-c5c480cea8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e1591b-31c5-4a38-b0fb-4bfaec0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Data project directory\n",
    "# data_dir = '../data/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760eea31",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed3547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, figsize=(17,7))\n",
    "# plt.plot(          ['new_deaths']);\n",
    "# ax.set_title(\"Covid 19 calculation for different countries\", size=10)\n",
    "# ax.set_ylabel(\"Number of death cases\", size=10)\n",
    "# ax.set_xlabel(\"Date\", size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b3bb-c12b-43a3-9dda-7bbd905f3622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122911e-70c8-4131-bf2f-8db270b3ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TENSORFLOW & RNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636afbb-aab0-4500-897b-311961289203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recurrent Neural Network (sequences data) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f081b-7ee6-405b-8e13-04bf267ac2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427dc3-25b3-4a5a-abba-20e92d67ab1b",
   "metadata": {},
   "source": [
    "X.shape = (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES) and y = RNN(X) where $X_{i,j}^{t}$\n",
    "\n",
    "with $_{i}$ is the sample/sequence, $_{j}$ is the feature measured and  $^{t}$ is the time at which the observation is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8151dc-1f0e-4981-91a0-c1164230af26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- **retrieve dataset** from Sumedha & Alberto\n",
    "\n",
    "    - **clean dataset**: \n",
    "        \n",
    "        - **drop first lines == 0** *(before Covid arrived)*\n",
    "        \n",
    "        - **check Nan**: \n",
    "- **strategy 1 country by country** sequences split as follow:\n",
    "\n",
    "- **strategy 2 one sequence per country**:\n",
    "    - **split X train, set** \n",
    "    - **Pad sequences**\n",
    "    - **create one csv per country**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848eaa-a889-47ee-972c-e9f2e54214cc",
   "metadata": {},
   "source": [
    "## Training strategies:\n",
    "- Get NB dataset (cleaned) from Alberto & Sumedha\n",
    "- 1/ Indicator in precentage %\n",
    "- 2/ Indicator as categorical labels\n",
    "- Run same RNN model in parallel with Kim & Thomas\n",
    "- Identify best dataset\n",
    "- Parameters to fit:\n",
    "    - increase **nb of sequences**\n",
    "    - train series modulation (ex: [50, 150, 200, 300, 400 nb of days = n_obs]) < take time to compute\n",
    "    - **learning_rate** in Optimizer(parameters)\n",
    "    - model layers architecture (**simple** -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "        > LSTM\n",
    "        > Dense\n",
    "       (> LSTM\n",
    "        > LSTM\n",
    "        > Dense)\n",
    "     >> **try to overfit** the model with the loss (train over val) or (early_stopping)\n",
    "     >> **(X_val, y_val)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4cdbb-56aa-4522-84a6-833471a7b34a",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775435d-8b7a-4742-bcb2-a76e1c2ffb1c",
   "metadata": {},
   "source": [
    "# RNN model Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648961ed-ee19-4342-b137-8f5029082404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "n_seq = 200 ## nb of sequences (samples)\n",
    "n_obs = 61 # maxi = 96 (stay around 70 or more test_split)\n",
    "n_feat = 20 #  X_train.shape[1] # 20 feature:\n",
    "n_pred = 10 # nb of days where we can predict new daily deaths\n",
    "n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d337-90d7-47b1-a4ee-b508e8731c39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test datasplit the dataset into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 1, 2, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('France', split_train=0.7, split_val=0.9)\n",
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4fba2-9562-49eb-9b94-7c5ab57fd163",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create sequences (`X`,`y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e6437-7538-4c29-b8d9-d00d4d804064",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Generates an entire dataset of multiple subsamples with shape $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b0cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence(X, y, X_len, y_len) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length` (eg n_obs).\n",
    "    This shorter sequence should be selected at random\n",
    "    \"\"\"\n",
    "    X_y_len = X_len + y_len\n",
    "    print('_len + y_len',  X_len,  y_len)\n",
    "    print('X.shape[0] >= X_y_len', X.shape[0], X_y_len)\n",
    "    if X.shape[0] >= X_y_len:\n",
    "        last_possible = X.shape[0] - X_y_len\n",
    "    else:\n",
    "        last_possible = X.shape[0]\n",
    "        print('X_y_len = ?', X.shape[0])\n",
    "    # How to split sequences? we could do it manually...\n",
    "    print('X.shape[0]', X.shape[0])\n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    # X start and y end\n",
    "    X_sample = X[random_start : random_start + X_len]\n",
    "    y_sample = y[random_start + X_len : (random_start + X_y_len)]\n",
    "    print(\"X[random_start : random_start + X_len]\", f\"X[{random_start} : {random_start + X_len}]\")\n",
    "    print(\"y[random_start : random_start + X_y_len]\", f\"y[{random_start} : {(random_start + X_y_len)}]\")\n",
    "    \n",
    "    return np.array(X_sample), np.array(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a54ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1993c868-606c-4d31-ac8d-997bc85d63d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[447 : 508]\n",
      "y[random_start : random_start + X_y_len] y[447 : 518]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((61, 20), (10,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsample sequence\n",
    "(X_sample, y_sample) = subsample_sequence(X_train, y_train, X_len=n_obs, y_len=n_pred)\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb0927-e52b-4d41-8dcb-6ca6f204a1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_X_y(df, n_sequences, length)**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cde1f94-64b6-472d-8007-5a91ec085172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(X, y, X_len, y_len, n_sequences) -> tuple:\n",
    "    '''Return a list of samples (X, y)'''\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        (xi, yi) = subsample_sequence(X, y, X_len=X_len, y_len=y_len)\n",
    "        X_list.append(xi)\n",
    "        y_list.append(yi)\n",
    "        \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9096e3f-477e-45ae-a9a2-bdb1c8163ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 40, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq_val = n_seq // 5 # number of sequences in test set ?\n",
    "n_seq_test = n_seq // 10 # number of sequences in test set ?\n",
    "n_seq, n_seq_val, n_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1682240c-818e-4e96-9a2a-8b596cd235bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 20), (97,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3af4e7e-d602-4fcd-a36a-942ab4dc0b72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 82]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 82]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 82]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 72]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[12 : 73]\n",
      "y[random_start : random_start + X_y_len] y[12 : 83]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[13 : 74]\n",
      "y[random_start : random_start + X_y_len] y[13 : 84]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[17 : 78]\n",
      "y[random_start : random_start + X_y_len] y[17 : 88]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[12 : 73]\n",
      "y[random_start : random_start + X_y_len] y[12 : 83]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[19 : 80]\n",
      "y[random_start : random_start + X_y_len] y[19 : 90]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[23 : 84]\n",
      "y[random_start : random_start + X_y_len] y[23 : 94]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[2 : 63]\n",
      "y[random_start : random_start + X_y_len] y[2 : 73]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[15 : 76]\n",
      "y[random_start : random_start + X_y_len] y[15 : 86]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[11 : 72]\n",
      "y[random_start : random_start + X_y_len] y[11 : 82]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[9 : 70]\n",
      "y[random_start : random_start + X_y_len] y[9 : 80]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 77]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 77]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 72]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[2 : 63]\n",
      "y[random_start : random_start + X_y_len] y[2 : 73]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[22 : 83]\n",
      "y[random_start : random_start + X_y_len] y[22 : 93]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 97 71\n",
      "X.shape[0] 97\n",
      "X[random_start : random_start + X_len] X[14 : 75]\n",
      "y[random_start : random_start + X_y_len] y[14 : 85]\n",
      "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20, 61, 20), (20, 10), 20, 61, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_X_y(X_test, y_test, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n",
    "print('X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat')\n",
    "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497196a7-2d6a-4663-82a9-c8873c4ad0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[38 : 99]\n",
      "y[random_start : random_start + X_y_len] y[38 : 109]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[101 : 162]\n",
      "y[random_start : random_start + X_y_len] y[101 : 172]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[88 : 149]\n",
      "y[random_start : random_start + X_y_len] y[88 : 159]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[61 : 122]\n",
      "y[random_start : random_start + X_y_len] y[61 : 132]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[46 : 107]\n",
      "y[random_start : random_start + X_y_len] y[46 : 117]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[68 : 129]\n",
      "y[random_start : random_start + X_y_len] y[68 : 139]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[83 : 144]\n",
      "y[random_start : random_start + X_y_len] y[83 : 154]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[81 : 142]\n",
      "y[random_start : random_start + X_y_len] y[81 : 152]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[22 : 83]\n",
      "y[random_start : random_start + X_y_len] y[22 : 93]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[98 : 159]\n",
      "y[random_start : random_start + X_y_len] y[98 : 169]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[103 : 164]\n",
      "y[random_start : random_start + X_y_len] y[103 : 174]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[87 : 148]\n",
      "y[random_start : random_start + X_y_len] y[87 : 158]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[47 : 108]\n",
      "y[random_start : random_start + X_y_len] y[47 : 118]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[5 : 66]\n",
      "y[random_start : random_start + X_y_len] y[5 : 76]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[84 : 145]\n",
      "y[random_start : random_start + X_y_len] y[84 : 155]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[57 : 118]\n",
      "y[random_start : random_start + X_y_len] y[57 : 128]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[30 : 91]\n",
      "y[random_start : random_start + X_y_len] y[30 : 101]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[90 : 151]\n",
      "y[random_start : random_start + X_y_len] y[90 : 161]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[99 : 160]\n",
      "y[random_start : random_start + X_y_len] y[99 : 170]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[10 : 71]\n",
      "y[random_start : random_start + X_y_len] y[10 : 81]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[52 : 113]\n",
      "y[random_start : random_start + X_y_len] y[52 : 123]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[56 : 117]\n",
      "y[random_start : random_start + X_y_len] y[56 : 127]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[109 : 170]\n",
      "y[random_start : random_start + X_y_len] y[109 : 180]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[25 : 86]\n",
      "y[random_start : random_start + X_y_len] y[25 : 96]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 77]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[108 : 169]\n",
      "y[random_start : random_start + X_y_len] y[108 : 179]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[24 : 85]\n",
      "y[random_start : random_start + X_y_len] y[24 : 95]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[14 : 75]\n",
      "y[random_start : random_start + X_y_len] y[14 : 85]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[22 : 83]\n",
      "y[random_start : random_start + X_y_len] y[22 : 93]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[83 : 144]\n",
      "y[random_start : random_start + X_y_len] y[83 : 154]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[63 : 124]\n",
      "y[random_start : random_start + X_y_len] y[63 : 134]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[61 : 122]\n",
      "y[random_start : random_start + X_y_len] y[61 : 132]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[55 : 116]\n",
      "y[random_start : random_start + X_y_len] y[55 : 126]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[39 : 100]\n",
      "y[random_start : random_start + X_y_len] y[39 : 110]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[90 : 151]\n",
      "y[random_start : random_start + X_y_len] y[90 : 161]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[12 : 73]\n",
      "y[random_start : random_start + X_y_len] y[12 : 83]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 72]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[67 : 128]\n",
      "y[random_start : random_start + X_y_len] y[67 : 138]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[6 : 67]\n",
      "y[random_start : random_start + X_y_len] y[6 : 77]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 192 71\n",
      "X.shape[0] 192\n",
      "X[random_start : random_start + X_len] X[73 : 134]\n",
      "y[random_start : random_start + X_y_len] y[73 : 144]\n",
      "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((40, 61, 20), (40, 10), 200, 40, 20, 61, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = get_X_y(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "print('X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat')\n",
    "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a422c04-e3ef-4daf-b870-060c559ad2bb",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('United States', split_train=0.7, split_val=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e2fa4-2cf1-4ad1-bd39-6b48cd2d478d",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_test, y_test, X_len=n_obs, y_len=n_pred, n_seq_train=n_seq_test, X_val=X_val, y_val=y_val)\n",
    "X_train.shape, y_train.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7826c700-4273-4cd3-b7d7-4c3cca2b3582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[560 : 621]\n",
      "y[random_start : random_start + X_y_len] y[560 : 631]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[53 : 114]\n",
      "y[random_start : random_start + X_y_len] y[53 : 124]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[589 : 650]\n",
      "y[random_start : random_start + X_y_len] y[589 : 660]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[319 : 380]\n",
      "y[random_start : random_start + X_y_len] y[319 : 390]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[234 : 295]\n",
      "y[random_start : random_start + X_y_len] y[234 : 305]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[225 : 286]\n",
      "y[random_start : random_start + X_y_len] y[225 : 296]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[154 : 215]\n",
      "y[random_start : random_start + X_y_len] y[154 : 225]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[197 : 258]\n",
      "y[random_start : random_start + X_y_len] y[197 : 268]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[66 : 127]\n",
      "y[random_start : random_start + X_y_len] y[66 : 137]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[399 : 460]\n",
      "y[random_start : random_start + X_y_len] y[399 : 470]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[215 : 276]\n",
      "y[random_start : random_start + X_y_len] y[215 : 286]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[305 : 366]\n",
      "y[random_start : random_start + X_y_len] y[305 : 376]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[38 : 99]\n",
      "y[random_start : random_start + X_y_len] y[38 : 109]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[119 : 180]\n",
      "y[random_start : random_start + X_y_len] y[119 : 190]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[19 : 80]\n",
      "y[random_start : random_start + X_y_len] y[19 : 90]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[237 : 298]\n",
      "y[random_start : random_start + X_y_len] y[237 : 308]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[72 : 133]\n",
      "y[random_start : random_start + X_y_len] y[72 : 143]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[242 : 303]\n",
      "y[random_start : random_start + X_y_len] y[242 : 313]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[414 : 475]\n",
      "y[random_start : random_start + X_y_len] y[414 : 485]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[175 : 236]\n",
      "y[random_start : random_start + X_y_len] y[175 : 246]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[351 : 412]\n",
      "y[random_start : random_start + X_y_len] y[351 : 422]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[334 : 395]\n",
      "y[random_start : random_start + X_y_len] y[334 : 405]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[521 : 582]\n",
      "y[random_start : random_start + X_y_len] y[521 : 592]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[57 : 118]\n",
      "y[random_start : random_start + X_y_len] y[57 : 128]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[279 : 340]\n",
      "y[random_start : random_start + X_y_len] y[279 : 350]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[255 : 316]\n",
      "y[random_start : random_start + X_y_len] y[255 : 326]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[153 : 214]\n",
      "y[random_start : random_start + X_y_len] y[153 : 224]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[381 : 442]\n",
      "y[random_start : random_start + X_y_len] y[381 : 452]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[136 : 197]\n",
      "y[random_start : random_start + X_y_len] y[136 : 207]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[124 : 185]\n",
      "y[random_start : random_start + X_y_len] y[124 : 195]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[198 : 259]\n",
      "y[random_start : random_start + X_y_len] y[198 : 269]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[208 : 269]\n",
      "y[random_start : random_start + X_y_len] y[208 : 279]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[43 : 104]\n",
      "y[random_start : random_start + X_y_len] y[43 : 114]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[48 : 109]\n",
      "y[random_start : random_start + X_y_len] y[48 : 119]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[193 : 254]\n",
      "y[random_start : random_start + X_y_len] y[193 : 264]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[72 : 133]\n",
      "y[random_start : random_start + X_y_len] y[72 : 143]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[399 : 460]\n",
      "y[random_start : random_start + X_y_len] y[399 : 470]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[425 : 486]\n",
      "y[random_start : random_start + X_y_len] y[425 : 496]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[240 : 301]\n",
      "y[random_start : random_start + X_y_len] y[240 : 311]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[439 : 500]\n",
      "y[random_start : random_start + X_y_len] y[439 : 510]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[116 : 177]\n",
      "y[random_start : random_start + X_y_len] y[116 : 187]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[155 : 216]\n",
      "y[random_start : random_start + X_y_len] y[155 : 226]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[510 : 571]\n",
      "y[random_start : random_start + X_y_len] y[510 : 581]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[351 : 412]\n",
      "y[random_start : random_start + X_y_len] y[351 : 422]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[291 : 352]\n",
      "y[random_start : random_start + X_y_len] y[291 : 362]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[152 : 213]\n",
      "y[random_start : random_start + X_y_len] y[152 : 223]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[435 : 496]\n",
      "y[random_start : random_start + X_y_len] y[435 : 506]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[314 : 375]\n",
      "y[random_start : random_start + X_y_len] y[314 : 385]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[231 : 292]\n",
      "y[random_start : random_start + X_y_len] y[231 : 302]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[548 : 609]\n",
      "y[random_start : random_start + X_y_len] y[548 : 619]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[498 : 559]\n",
      "y[random_start : random_start + X_y_len] y[498 : 569]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[30 : 91]\n",
      "y[random_start : random_start + X_y_len] y[30 : 101]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[41 : 102]\n",
      "y[random_start : random_start + X_y_len] y[41 : 112]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[65 : 126]\n",
      "y[random_start : random_start + X_y_len] y[65 : 136]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[285 : 346]\n",
      "y[random_start : random_start + X_y_len] y[285 : 356]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[302 : 363]\n",
      "y[random_start : random_start + X_y_len] y[302 : 373]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[197 : 258]\n",
      "y[random_start : random_start + X_y_len] y[197 : 268]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[189 : 250]\n",
      "y[random_start : random_start + X_y_len] y[189 : 260]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[295 : 356]\n",
      "y[random_start : random_start + X_y_len] y[295 : 366]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[150 : 211]\n",
      "y[random_start : random_start + X_y_len] y[150 : 221]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[139 : 200]\n",
      "y[random_start : random_start + X_y_len] y[139 : 210]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[235 : 296]\n",
      "y[random_start : random_start + X_y_len] y[235 : 306]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[587 : 648]\n",
      "y[random_start : random_start + X_y_len] y[587 : 658]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[161 : 222]\n",
      "y[random_start : random_start + X_y_len] y[161 : 232]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[479 : 540]\n",
      "y[random_start : random_start + X_y_len] y[479 : 550]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[291 : 352]\n",
      "y[random_start : random_start + X_y_len] y[291 : 362]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[378 : 439]\n",
      "y[random_start : random_start + X_y_len] y[378 : 449]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[475 : 536]\n",
      "y[random_start : random_start + X_y_len] y[475 : 546]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[569 : 630]\n",
      "y[random_start : random_start + X_y_len] y[569 : 640]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[265 : 326]\n",
      "y[random_start : random_start + X_y_len] y[265 : 336]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[233 : 294]\n",
      "y[random_start : random_start + X_y_len] y[233 : 304]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[569 : 630]\n",
      "y[random_start : random_start + X_y_len] y[569 : 640]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[94 : 155]\n",
      "y[random_start : random_start + X_y_len] y[94 : 165]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[477 : 538]\n",
      "y[random_start : random_start + X_y_len] y[477 : 548]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[515 : 576]\n",
      "y[random_start : random_start + X_y_len] y[515 : 586]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[353 : 414]\n",
      "y[random_start : random_start + X_y_len] y[353 : 424]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[574 : 635]\n",
      "y[random_start : random_start + X_y_len] y[574 : 645]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[186 : 247]\n",
      "y[random_start : random_start + X_y_len] y[186 : 257]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[508 : 569]\n",
      "y[random_start : random_start + X_y_len] y[508 : 579]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[585 : 646]\n",
      "y[random_start : random_start + X_y_len] y[585 : 656]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[381 : 442]\n",
      "y[random_start : random_start + X_y_len] y[381 : 452]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[590 : 651]\n",
      "y[random_start : random_start + X_y_len] y[590 : 661]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[221 : 282]\n",
      "y[random_start : random_start + X_y_len] y[221 : 292]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[40 : 101]\n",
      "y[random_start : random_start + X_y_len] y[40 : 111]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[4 : 65]\n",
      "y[random_start : random_start + X_y_len] y[4 : 75]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[471 : 532]\n",
      "y[random_start : random_start + X_y_len] y[471 : 542]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[597 : 658]\n",
      "y[random_start : random_start + X_y_len] y[597 : 668]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[188 : 249]\n",
      "y[random_start : random_start + X_y_len] y[188 : 259]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[273 : 334]\n",
      "y[random_start : random_start + X_y_len] y[273 : 344]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[262 : 323]\n",
      "y[random_start : random_start + X_y_len] y[262 : 333]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[404 : 465]\n",
      "y[random_start : random_start + X_y_len] y[404 : 475]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[21 : 82]\n",
      "y[random_start : random_start + X_y_len] y[21 : 92]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[468 : 529]\n",
      "y[random_start : random_start + X_y_len] y[468 : 539]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[371 : 432]\n",
      "y[random_start : random_start + X_y_len] y[371 : 442]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[601 : 662]\n",
      "y[random_start : random_start + X_y_len] y[601 : 672]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[507 : 568]\n",
      "y[random_start : random_start + X_y_len] y[507 : 578]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[132 : 193]\n",
      "y[random_start : random_start + X_y_len] y[132 : 203]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[453 : 514]\n",
      "y[random_start : random_start + X_y_len] y[453 : 524]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[472 : 533]\n",
      "y[random_start : random_start + X_y_len] y[472 : 543]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[3 : 64]\n",
      "y[random_start : random_start + X_y_len] y[3 : 74]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[333 : 394]\n",
      "y[random_start : random_start + X_y_len] y[333 : 404]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[407 : 468]\n",
      "y[random_start : random_start + X_y_len] y[407 : 478]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[435 : 496]\n",
      "y[random_start : random_start + X_y_len] y[435 : 506]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[451 : 512]\n",
      "y[random_start : random_start + X_y_len] y[451 : 522]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[98 : 159]\n",
      "y[random_start : random_start + X_y_len] y[98 : 169]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[555 : 616]\n",
      "y[random_start : random_start + X_y_len] y[555 : 626]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[571 : 632]\n",
      "y[random_start : random_start + X_y_len] y[571 : 642]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[71 : 132]\n",
      "y[random_start : random_start + X_y_len] y[71 : 142]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[143 : 204]\n",
      "y[random_start : random_start + X_y_len] y[143 : 214]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[165 : 226]\n",
      "y[random_start : random_start + X_y_len] y[165 : 236]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[265 : 326]\n",
      "y[random_start : random_start + X_y_len] y[265 : 336]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[294 : 355]\n",
      "y[random_start : random_start + X_y_len] y[294 : 365]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[135 : 196]\n",
      "y[random_start : random_start + X_y_len] y[135 : 206]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[361 : 422]\n",
      "y[random_start : random_start + X_y_len] y[361 : 432]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[25 : 86]\n",
      "y[random_start : random_start + X_y_len] y[25 : 96]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[466 : 527]\n",
      "y[random_start : random_start + X_y_len] y[466 : 537]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[575 : 636]\n",
      "y[random_start : random_start + X_y_len] y[575 : 646]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[507 : 568]\n",
      "y[random_start : random_start + X_y_len] y[507 : 578]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[430 : 491]\n",
      "y[random_start : random_start + X_y_len] y[430 : 501]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[76 : 137]\n",
      "y[random_start : random_start + X_y_len] y[76 : 147]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[378 : 439]\n",
      "y[random_start : random_start + X_y_len] y[378 : 449]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[483 : 544]\n",
      "y[random_start : random_start + X_y_len] y[483 : 554]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[562 : 623]\n",
      "y[random_start : random_start + X_y_len] y[562 : 633]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[147 : 208]\n",
      "y[random_start : random_start + X_y_len] y[147 : 218]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[598 : 659]\n",
      "y[random_start : random_start + X_y_len] y[598 : 669]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[274 : 335]\n",
      "y[random_start : random_start + X_y_len] y[274 : 345]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 72]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[470 : 531]\n",
      "y[random_start : random_start + X_y_len] y[470 : 541]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[149 : 210]\n",
      "y[random_start : random_start + X_y_len] y[149 : 220]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[271 : 332]\n",
      "y[random_start : random_start + X_y_len] y[271 : 342]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[313 : 374]\n",
      "y[random_start : random_start + X_y_len] y[313 : 384]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[217 : 278]\n",
      "y[random_start : random_start + X_y_len] y[217 : 288]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[421 : 482]\n",
      "y[random_start : random_start + X_y_len] y[421 : 492]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[435 : 496]\n",
      "y[random_start : random_start + X_y_len] y[435 : 506]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[591 : 652]\n",
      "y[random_start : random_start + X_y_len] y[591 : 662]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[275 : 336]\n",
      "y[random_start : random_start + X_y_len] y[275 : 346]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[441 : 502]\n",
      "y[random_start : random_start + X_y_len] y[441 : 512]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[407 : 468]\n",
      "y[random_start : random_start + X_y_len] y[407 : 478]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[366 : 427]\n",
      "y[random_start : random_start + X_y_len] y[366 : 437]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[182 : 243]\n",
      "y[random_start : random_start + X_y_len] y[182 : 253]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[13 : 74]\n",
      "y[random_start : random_start + X_y_len] y[13 : 84]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[326 : 387]\n",
      "y[random_start : random_start + X_y_len] y[326 : 397]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[110 : 171]\n",
      "y[random_start : random_start + X_y_len] y[110 : 181]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[278 : 339]\n",
      "y[random_start : random_start + X_y_len] y[278 : 349]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[70 : 131]\n",
      "y[random_start : random_start + X_y_len] y[70 : 141]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[184 : 245]\n",
      "y[random_start : random_start + X_y_len] y[184 : 255]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[173 : 234]\n",
      "y[random_start : random_start + X_y_len] y[173 : 244]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[403 : 464]\n",
      "y[random_start : random_start + X_y_len] y[403 : 474]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[564 : 625]\n",
      "y[random_start : random_start + X_y_len] y[564 : 635]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[569 : 630]\n",
      "y[random_start : random_start + X_y_len] y[569 : 640]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[322 : 383]\n",
      "y[random_start : random_start + X_y_len] y[322 : 393]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[328 : 389]\n",
      "y[random_start : random_start + X_y_len] y[328 : 399]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[273 : 334]\n",
      "y[random_start : random_start + X_y_len] y[273 : 344]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[469 : 530]\n",
      "y[random_start : random_start + X_y_len] y[469 : 540]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[42 : 103]\n",
      "y[random_start : random_start + X_y_len] y[42 : 113]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[552 : 613]\n",
      "y[random_start : random_start + X_y_len] y[552 : 623]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[562 : 623]\n",
      "y[random_start : random_start + X_y_len] y[562 : 633]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[206 : 267]\n",
      "y[random_start : random_start + X_y_len] y[206 : 277]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[316 : 377]\n",
      "y[random_start : random_start + X_y_len] y[316 : 387]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[358 : 419]\n",
      "y[random_start : random_start + X_y_len] y[358 : 429]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[286 : 347]\n",
      "y[random_start : random_start + X_y_len] y[286 : 357]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[157 : 218]\n",
      "y[random_start : random_start + X_y_len] y[157 : 228]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[297 : 358]\n",
      "y[random_start : random_start + X_y_len] y[297 : 368]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[152 : 213]\n",
      "y[random_start : random_start + X_y_len] y[152 : 223]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[77 : 138]\n",
      "y[random_start : random_start + X_y_len] y[77 : 148]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[333 : 394]\n",
      "y[random_start : random_start + X_y_len] y[333 : 404]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[35 : 96]\n",
      "y[random_start : random_start + X_y_len] y[35 : 106]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[99 : 160]\n",
      "y[random_start : random_start + X_y_len] y[99 : 170]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[270 : 331]\n",
      "y[random_start : random_start + X_y_len] y[270 : 341]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[352 : 413]\n",
      "y[random_start : random_start + X_y_len] y[352 : 423]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[237 : 298]\n",
      "y[random_start : random_start + X_y_len] y[237 : 308]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[508 : 569]\n",
      "y[random_start : random_start + X_y_len] y[508 : 579]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[244 : 305]\n",
      "y[random_start : random_start + X_y_len] y[244 : 315]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[136 : 197]\n",
      "y[random_start : random_start + X_y_len] y[136 : 207]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[191 : 252]\n",
      "y[random_start : random_start + X_y_len] y[191 : 262]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[408 : 469]\n",
      "y[random_start : random_start + X_y_len] y[408 : 479]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[26 : 87]\n",
      "y[random_start : random_start + X_y_len] y[26 : 97]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[523 : 584]\n",
      "y[random_start : random_start + X_y_len] y[523 : 594]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[329 : 390]\n",
      "y[random_start : random_start + X_y_len] y[329 : 400]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[1 : 62]\n",
      "y[random_start : random_start + X_y_len] y[1 : 72]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[191 : 252]\n",
      "y[random_start : random_start + X_y_len] y[191 : 262]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[586 : 647]\n",
      "y[random_start : random_start + X_y_len] y[586 : 657]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[252 : 313]\n",
      "y[random_start : random_start + X_y_len] y[252 : 323]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[61 : 122]\n",
      "y[random_start : random_start + X_y_len] y[61 : 132]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[226 : 287]\n",
      "y[random_start : random_start + X_y_len] y[226 : 297]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[163 : 224]\n",
      "y[random_start : random_start + X_y_len] y[163 : 234]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[439 : 500]\n",
      "y[random_start : random_start + X_y_len] y[439 : 510]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[128 : 189]\n",
      "y[random_start : random_start + X_y_len] y[128 : 199]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[389 : 450]\n",
      "y[random_start : random_start + X_y_len] y[389 : 460]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[489 : 550]\n",
      "y[random_start : random_start + X_y_len] y[489 : 560]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[76 : 137]\n",
      "y[random_start : random_start + X_y_len] y[76 : 147]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[133 : 194]\n",
      "y[random_start : random_start + X_y_len] y[133 : 204]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[411 : 472]\n",
      "y[random_start : random_start + X_y_len] y[411 : 482]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[341 : 402]\n",
      "y[random_start : random_start + X_y_len] y[341 : 412]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[107 : 168]\n",
      "y[random_start : random_start + X_y_len] y[107 : 178]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[529 : 590]\n",
      "y[random_start : random_start + X_y_len] y[529 : 600]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[369 : 430]\n",
      "y[random_start : random_start + X_y_len] y[369 : 440]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[139 : 200]\n",
      "y[random_start : random_start + X_y_len] y[139 : 210]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[318 : 379]\n",
      "y[random_start : random_start + X_y_len] y[318 : 389]\n",
      "_len + y_len 61 10\n",
      "X.shape[0] >= X_y_len 673 71\n",
      "X.shape[0] 673\n",
      "X[random_start : random_start + X_len] X[52 : 113]\n",
      "y[random_start : random_start + X_y_len] y[52 : 123]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200, 61, 20), (200, 10), 200, 61, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n",
    "X_train.shape, y_train.shape, n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2a397-4b64-4358-8355-2a243e990afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "X_val.shape, y_val.shape, n_seq_val, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec27cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661159e6-4eb3-48cc-b11d-fff40fb81a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Train set\n",
    "# def train_rnn_model(model, patience=7, epochs=200):\n",
    "#     es = EarlyStopping(monitor = 'val_loss',\n",
    "#                     patience = patience,\n",
    "#                     verbose = 1,\n",
    "#                     restore_best_weights = True)\n",
    "#     # Fit\n",
    "#     history =  model.fit(X_train, y_train, \n",
    "#             validation_split=0.1, # Auto split for validation data\n",
    "#             batch_size = 16,\n",
    "#             epochs = epochs,\n",
    "#             callbacks = [es],\n",
    "#             verbose=1)\n",
    "#     return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe229ed-38be-485e-9314-5a3c04da8847",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### How to split sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49128bf-f5e8-4319-8635-fa1f63c29ceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- randomly or\n",
    "\n",
    "- manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867c99-9dd9-4dc5-9cda-de741bf57ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **train_rnn_model(model, patience=2, epochs=200):**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ece011-197d-439e-b405-ee2370c9773d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RNN model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc24c32-f615-4696-b4e0-01e7d7281ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model(model, patience=7, epochs=200):\n",
    "    \"\"\" function that train a RNN model with hyperparameters:\n",
    "    - patience by default 7 to early stop\n",
    "    - epochs by default 200 to train over several epochs\n",
    "    - validation data by default (X_val, y_val)\n",
    "    \"\"\"\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  model.fit(X_train,\n",
    "            y_train, \n",
    "             # Auto split for validation data\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f844bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 20)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a7bd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 17:14:35.742114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-14 17:14:35.742245: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-14 17:14:35.742296: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-S0MTM0QT): /proc/driver/nvidia/version does not exist\n",
      "2022-09-14 17:14:35.742817: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,600\n",
      "Trainable params: 3,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    " \n",
    "# Normalization layer\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "\n",
    "# 1. The Architecture\n",
    "\"\"\"   - model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model = Sequential()\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model.add(LSTM(units=20, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model.add(Dense(n_pred, activation = 'linear'))\n",
    "\n",
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8cd997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 7s 150ms/step - loss: 5914614784.0000 - mape: 99.9981 - val_loss: 18694172672.0000 - val_mape: 97.7052\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5659205120.0000 - mape: 95.6358 - val_loss: 17642268672.0000 - val_mape: 88.1514\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5257539584.0000 - mape: 89.9160 - val_loss: 16463112192.0000 - val_mape: 78.1755\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 4845002240.0000 - mape: 82.5277 - val_loss: 14682759168.0000 - val_mape: 78.5960\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 4426439168.0000 - mape: 77.5383 - val_loss: 13491181568.0000 - val_mape: 66.3618\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 3974489600.0000 - mape: 70.5143 - val_loss: 12303913984.0000 - val_mape: 62.8960\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 3601258240.0000 - mape: 66.5532 - val_loss: 10491168768.0000 - val_mape: 58.2411\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 3131032320.0000 - mape: 60.7984 - val_loss: 9989159936.0000 - val_mape: 56.7334\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 2668130816.0000 - mape: 55.9984 - val_loss: 7575045120.0000 - val_mape: 47.8984\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 2270192896.0000 - mape: 51.6882 - val_loss: 7596401664.0000 - val_mape: 52.1585\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 1860170880.0000 - mape: 46.8220 - val_loss: 5087522816.0000 - val_mape: 39.3852\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 1477409664.0000 - mape: 42.0011 - val_loss: 4071875840.0000 - val_mape: 35.0537\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 1189304064.0000 - mape: 37.1698 - val_loss: 4432800256.0000 - val_mape: 39.5908\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 1s 112ms/step - loss: 993095424.0000 - mape: 34.3725 - val_loss: 3794827776.0000 - val_mape: 38.0305\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 1s 114ms/step - loss: 724143232.0000 - mape: 30.9909 - val_loss: 1944175232.0000 - val_mape: 22.8491\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 550204352.0000 - mape: 26.5615 - val_loss: 1328284928.0000 - val_mape: 19.0272\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 490514144.0000 - mape: 26.0577 - val_loss: 962118016.0000 - val_mape: 17.8081\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 389230496.0000 - mape: 25.3371 - val_loss: 2516267520.0000 - val_mape: 31.3454\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 247146208.0000 - mape: 21.5954 - val_loss: 2121106688.0000 - val_mape: 28.4827\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 298375648.0000 - mape: 22.5497 - val_loss: 1215290752.0000 - val_mape: 21.0863\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluating\n",
    "history = train_rnn_model(rnn_model, epochs=200, patience=3)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e336f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    164911.828125\n",
       "3    148396.359375\n",
       "6    145096.625000\n",
       "1    144698.515625\n",
       "7    141783.156250\n",
       "4    137216.546875\n",
       "2    133035.156250\n",
       "5    129908.773438\n",
       "9     96255.007812\n",
       "8     73174.906250\n",
       "dtype: float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Prediction\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 2 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_2 = Sequential()\n",
    "rnn_model_2.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_2.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_2.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_2.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_2.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b589e",
   "metadata": {},
   "source": [
    "## RNN model 2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03d90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "def train_rnn_model_2(rnn_model_2, patience=20, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_2.fit(X_train,\n",
    "            y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901218-eb1c-4d85-b40e-9beda6c3c548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model 2 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db1267f1-8a86-4f8b-82f5-01478bc0d350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.858835\n",
       "9    0.268253\n",
       "4    0.259531\n",
       "5   -0.012228\n",
       "0   -0.062479\n",
       "3   -0.169380\n",
       "2   -0.239516\n",
       "6   -0.314345\n",
       "8   -0.498923\n",
       "7   -0.518477\n",
       "dtype: float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 2 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,581\n",
      "Trainable params: 6,540\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_2.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571254c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 9s 246ms/step - loss: 5914933248.0000 - mape: 99.9996 - val_loss: 19172200448.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 5914892800.0000 - mape: 99.9990 - val_loss: 19172182016.0000 - val_mape: 99.9999\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 5914838528.0000 - mape: 99.9983 - val_loss: 19172159488.0000 - val_mape: 99.9999\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5914774016.0000 - mape: 99.9974 - val_loss: 19172157440.0000 - val_mape: 99.9998\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 108ms/step - loss: 5914692096.0000 - mape: 99.9963 - val_loss: 19172141056.0000 - val_mape: 99.9998\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5914605056.0000 - mape: 99.9951 - val_loss: 19172114432.0000 - val_mape: 99.9997\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5914516480.0000 - mape: 99.9940 - val_loss: 19172085760.0000 - val_mape: 99.9997\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5914401280.0000 - mape: 99.9925 - val_loss: 19172052992.0000 - val_mape: 99.9996\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5914256384.0000 - mape: 99.9906 - val_loss: 19172038656.0000 - val_mape: 99.9995\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 5914097664.0000 - mape: 99.9858 - val_loss: 19172032512.0000 - val_mape: 99.9995\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 5913943040.0000 - mape: 99.9820 - val_loss: 19172014080.0000 - val_mape: 99.9995\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5913784320.0000 - mape: 99.9785 - val_loss: 19171993600.0000 - val_mape: 99.9994\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5913621504.0000 - mape: 99.9752 - val_loss: 19171964928.0000 - val_mape: 99.9994\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 5913451008.0000 - mape: 99.9717 - val_loss: 19171936256.0000 - val_mape: 99.9993\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5913271296.0000 - mape: 99.9685 - val_loss: 19171897344.0000 - val_mape: 99.9992\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5913079296.0000 - mape: 99.9649 - val_loss: 19171852288.0000 - val_mape: 99.9991\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5912880640.0000 - mape: 99.9610 - val_loss: 19171809280.0000 - val_mape: 99.9989\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5912668160.0000 - mape: 99.9573 - val_loss: 19171758080.0000 - val_mape: 99.9988\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 5912451072.0000 - mape: 99.9530 - val_loss: 19171698688.0000 - val_mape: 99.9986\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5912223232.0000 - mape: 99.9486 - val_loss: 19171637248.0000 - val_mape: 99.9985\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5911983104.0000 - mape: 99.9442 - val_loss: 19171559424.0000 - val_mape: 99.9983\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5911730688.0000 - mape: 99.9394 - val_loss: 19171471360.0000 - val_mape: 99.9980\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5911471104.0000 - mape: 99.9348 - val_loss: 19171375104.0000 - val_mape: 99.9978\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5911204352.0000 - mape: 99.9294 - val_loss: 19171272704.0000 - val_mape: 99.9975\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5910929920.0000 - mape: 99.9242 - val_loss: 19171153920.0000 - val_mape: 99.9972\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5910638592.0000 - mape: 99.9190 - val_loss: 19171065856.0000 - val_mape: 99.9970\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5910338560.0000 - mape: 99.9131 - val_loss: 19170996224.0000 - val_mape: 99.9968\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5910022656.0000 - mape: 99.9072 - val_loss: 19170924544.0000 - val_mape: 99.9966\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5909692416.0000 - mape: 99.9015 - val_loss: 19170619392.0000 - val_mape: 99.9958\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5909350912.0000 - mape: 99.8952 - val_loss: 19170357248.0000 - val_mape: 99.9951\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5909004800.0000 - mape: 99.8887 - val_loss: 19170226176.0000 - val_mape: 99.9948\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 5908654080.0000 - mape: 99.8822 - val_loss: 19170093056.0000 - val_mape: 99.9945\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5908291072.0000 - mape: 99.8753 - val_loss: 19169949696.0000 - val_mape: 99.9941\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 5907916800.0000 - mape: 99.8685 - val_loss: 19169816576.0000 - val_mape: 99.9937\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5907535872.0000 - mape: 99.8614 - val_loss: 19169685504.0000 - val_mape: 99.9934\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5907147776.0000 - mape: 99.8539 - val_loss: 19169564672.0000 - val_mape: 99.9931\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5906748416.0000 - mape: 99.8467 - val_loss: 19169458176.0000 - val_mape: 99.9928\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5906338816.0000 - mape: 99.8388 - val_loss: 19169351680.0000 - val_mape: 99.9925\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5905921536.0000 - mape: 99.8312 - val_loss: 19169290240.0000 - val_mape: 99.9924\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5905489408.0000 - mape: 99.8230 - val_loss: 19169226752.0000 - val_mape: 99.9922\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 1s 115ms/step - loss: 5905053184.0000 - mape: 99.8150 - val_loss: 19169181696.0000 - val_mape: 99.9921\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5904598528.0000 - mape: 99.8063 - val_loss: 19169144832.0000 - val_mape: 99.9920\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5904139776.0000 - mape: 99.7983 - val_loss: 19169138688.0000 - val_mape: 99.9920\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5903667712.0000 - mape: 99.7892 - val_loss: 19169134592.0000 - val_mape: 99.9920\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 5903192064.0000 - mape: 99.7805 - val_loss: 19169148928.0000 - val_mape: 99.9920\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 5902705664.0000 - mape: 99.7711 - val_loss: 19169161216.0000 - val_mape: 99.9920\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 5902206464.0000 - mape: 99.7616 - val_loss: 19169161216.0000 - val_mape: 99.9920\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 5901696512.0000 - mape: 99.7525 - val_loss: 19168933888.0000 - val_mape: 99.9914\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 5901177344.0000 - mape: 99.7426 - val_loss: 19168727040.0000 - val_mape: 99.9909\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 5900645888.0000 - mape: 99.7333 - val_loss: 19168423936.0000 - val_mape: 99.9901\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5900109312.0000 - mape: 99.7231 - val_loss: 19168071680.0000 - val_mape: 99.9892\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5899560448.0000 - mape: 99.7130 - val_loss: 19167690752.0000 - val_mape: 99.9882\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5898997760.0000 - mape: 99.7027 - val_loss: 19167379456.0000 - val_mape: 99.9874\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 5898432000.0000 - mape: 99.6911 - val_loss: 19167176704.0000 - val_mape: 99.9869\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5897860608.0000 - mape: 99.6804 - val_loss: 19166943232.0000 - val_mape: 99.9863\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5897275392.0000 - mape: 99.6698 - val_loss: 19166701568.0000 - val_mape: 99.9856\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5896680960.0000 - mape: 99.6583 - val_loss: 19166494720.0000 - val_mape: 99.9851\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 1s 112ms/step - loss: 5896086016.0000 - mape: 99.6466 - val_loss: 19166308352.0000 - val_mape: 99.9846\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5895463424.0000 - mape: 99.6362 - val_loss: 19166119936.0000 - val_mape: 99.9841\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5894841344.0000 - mape: 99.6236 - val_loss: 19165925376.0000 - val_mape: 99.9836\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 6s 480ms/step - loss: 5894211072.0000 - mape: 99.6112 - val_loss: 19165734912.0000 - val_mape: 99.9831\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 5893573632.0000 - mape: 99.6001 - val_loss: 19165538304.0000 - val_mape: 99.9826\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 5892912128.0000 - mape: 99.5888 - val_loss: 19165331456.0000 - val_mape: 99.9820\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5892241408.0000 - mape: 99.5757 - val_loss: 19165124608.0000 - val_mape: 99.9815\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5891564032.0000 - mape: 99.5624 - val_loss: 19164919808.0000 - val_mape: 99.9810\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 5890884096.0000 - mape: 99.5503 - val_loss: 19164712960.0000 - val_mape: 99.9804\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5890199552.0000 - mape: 99.5369 - val_loss: 19164499968.0000 - val_mape: 99.9799\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5889495552.0000 - mape: 99.5238 - val_loss: 19164282880.0000 - val_mape: 99.9793\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5888775168.0000 - mape: 99.5111 - val_loss: 19164057600.0000 - val_mape: 99.9787\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 5888050688.0000 - mape: 99.4965 - val_loss: 19163844608.0000 - val_mape: 99.9781\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5887323136.0000 - mape: 99.4837 - val_loss: 19163619328.0000 - val_mape: 99.9776\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5886574080.0000 - mape: 99.4701 - val_loss: 19163383808.0000 - val_mape: 99.9769\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5885816320.0000 - mape: 99.4547 - val_loss: 19163158528.0000 - val_mape: 99.9764\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5885062656.0000 - mape: 99.4413 - val_loss: 19162923008.0000 - val_mape: 99.9757\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5884289536.0000 - mape: 99.4263 - val_loss: 19162691584.0000 - val_mape: 99.9751\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5883516928.0000 - mape: 99.4122 - val_loss: 19162447872.0000 - val_mape: 99.9745\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 1s 108ms/step - loss: 5882723840.0000 - mape: 99.3970 - val_loss: 19162210304.0000 - val_mape: 99.9739\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 1s 116ms/step - loss: 5881935872.0000 - mape: 99.3820 - val_loss: 19161964544.0000 - val_mape: 99.9732\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5881122816.0000 - mape: 99.3671 - val_loss: 19161716736.0000 - val_mape: 99.9726\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5880296960.0000 - mape: 99.3523 - val_loss: 19161460736.0000 - val_mape: 99.9719\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5879452672.0000 - mape: 99.3374 - val_loss: 19161206784.0000 - val_mape: 99.9713\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5878611968.0000 - mape: 99.3214 - val_loss: 19160946688.0000 - val_mape: 99.9706\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5877757952.0000 - mape: 99.3049 - val_loss: 19160690688.0000 - val_mape: 99.9699\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5876905472.0000 - mape: 99.2890 - val_loss: 19160426496.0000 - val_mape: 99.9692\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5876032000.0000 - mape: 99.2721 - val_loss: 19160160256.0000 - val_mape: 99.9685\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 5875158016.0000 - mape: 99.2555 - val_loss: 19159891968.0000 - val_mape: 99.9678\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 5874265088.0000 - mape: 99.2396 - val_loss: 19159617536.0000 - val_mape: 99.9671\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5873363968.0000 - mape: 99.2225 - val_loss: 19159347200.0000 - val_mape: 99.9664\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5872460288.0000 - mape: 99.2059 - val_loss: 19159064576.0000 - val_mape: 99.9657\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5871534592.0000 - mape: 99.1887 - val_loss: 19158788096.0000 - val_mape: 99.9650\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5870623744.0000 - mape: 99.1699 - val_loss: 19158511616.0000 - val_mape: 99.9642\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5869703680.0000 - mape: 99.1532 - val_loss: 19158220800.0000 - val_mape: 99.9635\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5868738560.0000 - mape: 99.1359 - val_loss: 19157927936.0000 - val_mape: 99.9627\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5867781632.0000 - mape: 99.1174 - val_loss: 19157630976.0000 - val_mape: 99.9619\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5866805760.0000 - mape: 99.0995 - val_loss: 19157338112.0000 - val_mape: 99.9612\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 5865840128.0000 - mape: 99.0810 - val_loss: 19157045248.0000 - val_mape: 99.9604\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 1s 114ms/step - loss: 5864850944.0000 - mape: 99.0634 - val_loss: 19156740096.0000 - val_mape: 99.9596\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 5863856128.0000 - mape: 99.0441 - val_loss: 19156439040.0000 - val_mape: 99.9588\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 2s 128ms/step - loss: 5862859776.0000 - mape: 99.0243 - val_loss: 19156133888.0000 - val_mape: 99.9580\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5861849088.0000 - mape: 99.0054 - val_loss: 19155822592.0000 - val_mape: 99.9572\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5860822016.0000 - mape: 98.9868 - val_loss: 19155511296.0000 - val_mape: 99.9564\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5859784192.0000 - mape: 98.9675 - val_loss: 19155193856.0000 - val_mape: 99.9556\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5858741248.0000 - mape: 98.9479 - val_loss: 19154874368.0000 - val_mape: 99.9547\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5857689600.0000 - mape: 98.9277 - val_loss: 19154554880.0000 - val_mape: 99.9539\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5856626688.0000 - mape: 98.9081 - val_loss: 19154231296.0000 - val_mape: 99.9530\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5855566848.0000 - mape: 98.8887 - val_loss: 19153905664.0000 - val_mape: 99.9522\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5854488576.0000 - mape: 98.8675 - val_loss: 19153571840.0000 - val_mape: 99.9513\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5853392384.0000 - mape: 98.8477 - val_loss: 19153238016.0000 - val_mape: 99.9505\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5852288000.0000 - mape: 98.8256 - val_loss: 19152902144.0000 - val_mape: 99.9496\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5851171328.0000 - mape: 98.8066 - val_loss: 19152558080.0000 - val_mape: 99.9487\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5850041344.0000 - mape: 98.7846 - val_loss: 19152216064.0000 - val_mape: 99.9478\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5848924672.0000 - mape: 98.7626 - val_loss: 19151882240.0000 - val_mape: 99.9469\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5847798784.0000 - mape: 98.7419 - val_loss: 19151532032.0000 - val_mape: 99.9460\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5846633472.0000 - mape: 98.7224 - val_loss: 19151171584.0000 - val_mape: 99.9451\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5845480960.0000 - mape: 98.6976 - val_loss: 19150827520.0000 - val_mape: 99.9442\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5844324352.0000 - mape: 98.6768 - val_loss: 19150473216.0000 - val_mape: 99.9432\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5843143168.0000 - mape: 98.6564 - val_loss: 19150106624.0000 - val_mape: 99.9423\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5841942528.0000 - mape: 98.6320 - val_loss: 19149737984.0000 - val_mape: 99.9413\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 5840748544.0000 - mape: 98.6094 - val_loss: 19149375488.0000 - val_mape: 99.9404\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5839532032.0000 - mape: 98.5885 - val_loss: 19149004800.0000 - val_mape: 99.9394\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5838316544.0000 - mape: 98.5650 - val_loss: 19148632064.0000 - val_mape: 99.9384\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5837075456.0000 - mape: 98.5427 - val_loss: 19148253184.0000 - val_mape: 99.9374\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5835835392.0000 - mape: 98.5199 - val_loss: 19147882496.0000 - val_mape: 99.9365\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 5834622976.0000 - mape: 98.4939 - val_loss: 19147511808.0000 - val_mape: 99.9355\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 5833384448.0000 - mape: 98.4709 - val_loss: 19147122688.0000 - val_mape: 99.9345\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5832104960.0000 - mape: 98.4480 - val_loss: 19146735616.0000 - val_mape: 99.9335\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5830836224.0000 - mape: 98.4226 - val_loss: 19146346496.0000 - val_mape: 99.9325\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5829555712.0000 - mape: 98.4002 - val_loss: 19145961472.0000 - val_mape: 99.9315\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5828270592.0000 - mape: 98.3759 - val_loss: 19145562112.0000 - val_mape: 99.9304\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 1s 111ms/step - loss: 5826964992.0000 - mape: 98.3489 - val_loss: 19145166848.0000 - val_mape: 99.9294\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5825644544.0000 - mape: 98.3264 - val_loss: 19144759296.0000 - val_mape: 99.9283\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5824312320.0000 - mape: 98.3008 - val_loss: 19144355840.0000 - val_mape: 99.9273\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5822983168.0000 - mape: 98.2759 - val_loss: 19143948288.0000 - val_mape: 99.9262\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 5821645312.0000 - mape: 98.2512 - val_loss: 19143544832.0000 - val_mape: 99.9251\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5820317696.0000 - mape: 98.2246 - val_loss: 19143141376.0000 - val_mape: 99.9241\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5818967040.0000 - mape: 98.2001 - val_loss: 19142717440.0000 - val_mape: 99.9230\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5817584128.0000 - mape: 98.1732 - val_loss: 19142299648.0000 - val_mape: 99.9219\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5816215040.0000 - mape: 98.1475 - val_loss: 19141885952.0000 - val_mape: 99.9208\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5814846976.0000 - mape: 98.1216 - val_loss: 19141455872.0000 - val_mape: 99.9197\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 5813427712.0000 - mape: 98.0957 - val_loss: 19141023744.0000 - val_mape: 99.9186\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 2s 140ms/step - loss: 5812014080.0000 - mape: 98.0668 - val_loss: 19140595712.0000 - val_mape: 99.9174\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 1s 112ms/step - loss: 5810583552.0000 - mape: 98.0424 - val_loss: 19140149248.0000 - val_mape: 99.9163\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5809144320.0000 - mape: 98.0147 - val_loss: 19139721216.0000 - val_mape: 99.9151\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 5807726080.0000 - mape: 97.9878 - val_loss: 19139287040.0000 - val_mape: 99.9140\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 1s 108ms/step - loss: 5806284288.0000 - mape: 97.9608 - val_loss: 19138846720.0000 - val_mape: 99.9129\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 5804823552.0000 - mape: 97.9344 - val_loss: 19138390016.0000 - val_mape: 99.9117\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 5803350016.0000 - mape: 97.9059 - val_loss: 19137947648.0000 - val_mape: 99.9105\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 1s 111ms/step - loss: 5801880576.0000 - mape: 97.8771 - val_loss: 19137505280.0000 - val_mape: 99.9093\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5800415744.0000 - mape: 97.8499 - val_loss: 19137048576.0000 - val_mape: 99.9082\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 5798923264.0000 - mape: 97.8231 - val_loss: 19136585728.0000 - val_mape: 99.9070\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 1s 108ms/step - loss: 5797411328.0000 - mape: 97.7909 - val_loss: 19136135168.0000 - val_mape: 99.9058\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 5795891712.0000 - mape: 97.7660 - val_loss: 19135655936.0000 - val_mape: 99.9045\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 5794355712.0000 - mape: 97.7361 - val_loss: 19135199232.0000 - val_mape: 99.9033\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 5792847872.0000 - mape: 97.7064 - val_loss: 19134738432.0000 - val_mape: 99.9021\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5791321088.0000 - mape: 97.6784 - val_loss: 19134269440.0000 - val_mape: 99.9009\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5789765632.0000 - mape: 97.6496 - val_loss: 19133784064.0000 - val_mape: 99.8996\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5788184064.0000 - mape: 97.6191 - val_loss: 19133300736.0000 - val_mape: 99.8984\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5786599936.0000 - mape: 97.5903 - val_loss: 19132813312.0000 - val_mape: 99.8971\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5785031168.0000 - mape: 97.5592 - val_loss: 19132340224.0000 - val_mape: 99.8959\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5783457792.0000 - mape: 97.5297 - val_loss: 19131858944.0000 - val_mape: 99.8946\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5781855232.0000 - mape: 97.4997 - val_loss: 19131363328.0000 - val_mape: 99.8933\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5780253696.0000 - mape: 97.4690 - val_loss: 19130886144.0000 - val_mape: 99.8921\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5778664448.0000 - mape: 97.4379 - val_loss: 19130392576.0000 - val_mape: 99.8908\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5777038848.0000 - mape: 97.4085 - val_loss: 19129884672.0000 - val_mape: 99.8894\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5775383040.0000 - mape: 97.3766 - val_loss: 19129380864.0000 - val_mape: 99.8881\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 1s 106ms/step - loss: 5773738496.0000 - mape: 97.3460 - val_loss: 19128879104.0000 - val_mape: 99.8868\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5772102656.0000 - mape: 97.3143 - val_loss: 19128383488.0000 - val_mape: 99.8855\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 5770465280.0000 - mape: 97.2827 - val_loss: 19127875584.0000 - val_mape: 99.8842\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5768787456.0000 - mape: 97.2531 - val_loss: 19127363584.0000 - val_mape: 99.8829\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 5767099904.0000 - mape: 97.2203 - val_loss: 19126839296.0000 - val_mape: 99.8815\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5765396992.0000 - mape: 97.1870 - val_loss: 19126325248.0000 - val_mape: 99.8801\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5763710976.0000 - mape: 97.1558 - val_loss: 19125803008.0000 - val_mape: 99.8788\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 5762027520.0000 - mape: 97.1221 - val_loss: 19125293056.0000 - val_mape: 99.8774\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5760301568.0000 - mape: 97.0915 - val_loss: 19124752384.0000 - val_mape: 99.8760\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5758556672.0000 - mape: 97.0592 - val_loss: 19124232192.0000 - val_mape: 99.8747\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5756846080.0000 - mape: 97.0263 - val_loss: 19123701760.0000 - val_mape: 99.8733\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5755104768.0000 - mape: 96.9920 - val_loss: 19123165184.0000 - val_mape: 99.8719\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 1s 118ms/step - loss: 5753361408.0000 - mape: 96.9581 - val_loss: 19122624512.0000 - val_mape: 99.8705\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 5751579648.0000 - mape: 96.9247 - val_loss: 19122079744.0000 - val_mape: 99.8690\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5749784064.0000 - mape: 96.8929 - val_loss: 19121526784.0000 - val_mape: 99.8676\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5747998720.0000 - mape: 96.8564 - val_loss: 19120986112.0000 - val_mape: 99.8662\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5746226688.0000 - mape: 96.8238 - val_loss: 19120445440.0000 - val_mape: 99.8648\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 1s 101ms/step - loss: 5744453120.0000 - mape: 96.7904 - val_loss: 19119898624.0000 - val_mape: 99.8633\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 5742636032.0000 - mape: 96.7561 - val_loss: 19119331328.0000 - val_mape: 99.8619\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 5740808704.0000 - mape: 96.7216 - val_loss: 19118788608.0000 - val_mape: 99.8605\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 5739032576.0000 - mape: 96.6872 - val_loss: 19118225408.0000 - val_mape: 99.8590\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5737165824.0000 - mape: 96.6541 - val_loss: 19117664256.0000 - val_mape: 99.8575\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5735340032.0000 - mape: 96.6197 - val_loss: 19117090816.0000 - val_mape: 99.8560\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 1s 115ms/step - loss: 5733466624.0000 - mape: 96.5816 - val_loss: 19116527616.0000 - val_mape: 99.8545\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5731642368.0000 - mape: 96.5445 - val_loss: 19115964416.0000 - val_mape: 99.8531\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 5729779200.0000 - mape: 96.5094 - val_loss: 19115382784.0000 - val_mape: 99.8515\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 5727875072.0000 - mape: 96.4763 - val_loss: 19114792960.0000 - val_mape: 99.8500\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 5725984768.0000 - mape: 96.4375 - val_loss: 19114225664.0000 - val_mape: 99.8485\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5724107776.0000 - mape: 96.4032 - val_loss: 19113639936.0000 - val_mape: 99.8470\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5722188800.0000 - mape: 96.3680 - val_loss: 19113056256.0000 - val_mape: 99.8455\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 5720297984.0000 - mape: 96.3281 - val_loss: 19112470528.0000 - val_mape: 99.8439\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 5718376448.0000 - mape: 96.2926 - val_loss: 19111882752.0000 - val_mape: 99.8424\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 5716429312.0000 - mape: 96.2562 - val_loss: 19111280640.0000 - val_mape: 99.8408\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 1s 103ms/step - loss: 5714491392.0000 - mape: 96.2196 - val_loss: 19110670336.0000 - val_mape: 99.8392\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 5712504320.0000 - mape: 96.1829 - val_loss: 19110072320.0000 - val_mape: 99.8377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_2, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78dfb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9    245.335205\n",
       "1    238.461334\n",
       "4    235.891678\n",
       "3    232.757248\n",
       "2    231.189041\n",
       "0    229.336716\n",
       "6    228.176239\n",
       "7    223.850266\n",
       "8    221.876434\n",
       "5    220.185379\n",
       "dtype: float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1950356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 3, LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2ad1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(20, activation = 'relu'))\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae1226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47f878a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,581\n",
      "Trainable params: 6,540\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b737ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 13s 291ms/step - loss: 5914933760.0000 - mape: 99.9998 - val_loss: 19172208640.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 5914905600.0000 - mape: 99.9995 - val_loss: 19172192256.0000 - val_mape: 99.9999\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5914854912.0000 - mape: 99.9991 - val_loss: 19172169728.0000 - val_mape: 99.9999\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 5914783232.0000 - mape: 99.9983 - val_loss: 19172136960.0000 - val_mape: 99.9998\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 5914685440.0000 - mape: 99.9971 - val_loss: 19172132864.0000 - val_mape: 99.9998\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 5914581504.0000 - mape: 99.9958 - val_loss: 19172112384.0000 - val_mape: 99.9997\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5914464768.0000 - mape: 99.9944 - val_loss: 19172093952.0000 - val_mape: 99.9997\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5914337792.0000 - mape: 99.9929 - val_loss: 19172075520.0000 - val_mape: 99.9996\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5914209280.0000 - mape: 99.9910 - val_loss: 19172055040.0000 - val_mape: 99.9996\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5914144768.0000 - mape: 99.9884 - val_loss: 19172034560.0000 - val_mape: 99.9995\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5914049024.0000 - mape: 99.9846 - val_loss: 19172005888.0000 - val_mape: 99.9995\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5913893888.0000 - mape: 99.9802 - val_loss: 19172179968.0000 - val_mape: 99.9999\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5913648128.0000 - mape: 99.9745 - val_loss: 19171835904.0000 - val_mape: 99.9990\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5913331712.0000 - mape: 99.9705 - val_loss: 19171799040.0000 - val_mape: 99.9989\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5913126912.0000 - mape: 99.9666 - val_loss: 19171762176.0000 - val_mape: 99.9988\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5912915456.0000 - mape: 99.9623 - val_loss: 19171731456.0000 - val_mape: 99.9987\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5912692224.0000 - mape: 99.9582 - val_loss: 19171688448.0000 - val_mape: 99.9986\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5912453632.0000 - mape: 99.9541 - val_loss: 19171645440.0000 - val_mape: 99.9985\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5912205824.0000 - mape: 99.9492 - val_loss: 19171602432.0000 - val_mape: 99.9984\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5911947264.0000 - mape: 99.9443 - val_loss: 19171565568.0000 - val_mape: 99.9983\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5911677440.0000 - mape: 99.9395 - val_loss: 19171534848.0000 - val_mape: 99.9982\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 5911393792.0000 - mape: 99.9342 - val_loss: 19171518464.0000 - val_mape: 99.9982\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 5911101440.0000 - mape: 99.9287 - val_loss: 19171516416.0000 - val_mape: 99.9982\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5910799360.0000 - mape: 99.9229 - val_loss: 19171532800.0000 - val_mape: 99.9982\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5910488576.0000 - mape: 99.9166 - val_loss: 19171557376.0000 - val_mape: 99.9983\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 5910164480.0000 - mape: 99.9112 - val_loss: 19171586048.0000 - val_mape: 99.9984\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 5909819392.0000 - mape: 99.9047 - val_loss: 19171610624.0000 - val_mape: 99.9984\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5909465088.0000 - mape: 99.8986 - val_loss: 19171643392.0000 - val_mape: 99.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_3, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c144bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    3.755394\n",
       "9    3.462556\n",
       "8    2.883344\n",
       "4    2.827268\n",
       "7    2.684329\n",
       "1    2.472190\n",
       "5    2.268114\n",
       "0    1.761166\n",
       "2    1.683639\n",
       "6    0.890642\n",
       "dtype: float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_3.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 4 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_4 = Sequential()\n",
    "rnn_model_4.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_4.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_4.add(Dense(15, activation = 'relu'))\n",
    "rnn_model_4.add(Dense(5, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_4.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 4 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 80        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,766\n",
      "Trainable params: 6,725\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_4.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cc51a-a8a3-4a61-9c9e-96317f159d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ecee49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_4, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_4.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "                ## validation_data = (X_val, y_val), # To be created manually if needed\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d401da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 10s 293ms/step - loss: 5969966592.0000 - mape: 99.9998 - val_loss: 5419558400.0000 - val_mape: 99.9996\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 5969914368.0000 - mape: 99.9994 - val_loss: 5419499520.0000 - val_mape: 99.9993\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 5969830912.0000 - mape: 99.9989 - val_loss: 5419404800.0000 - val_mape: 99.9986\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 5969710080.0000 - mape: 99.9979 - val_loss: 5419279360.0000 - val_mape: 99.9973\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 5969557504.0000 - mape: 99.9958 - val_loss: 5419134464.0000 - val_mape: 99.9947\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 5969406464.0000 - mape: 99.9937 - val_loss: 5418984960.0000 - val_mape: 99.9923\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5969246208.0000 - mape: 99.9917 - val_loss: 5418830848.0000 - val_mape: 99.9899\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5969074176.0000 - mape: 99.9896 - val_loss: 5418648576.0000 - val_mape: 99.9872\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 5968875008.0000 - mape: 99.9873 - val_loss: 5418449920.0000 - val_mape: 99.9843\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5968655360.0000 - mape: 99.9846 - val_loss: 5418235392.0000 - val_mape: 99.9813\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5968414208.0000 - mape: 99.9816 - val_loss: 5417996288.0000 - val_mape: 99.9779\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 5968223232.0000 - mape: 99.9784 - val_loss: 5417888768.0000 - val_mape: 99.9733\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 5967987200.0000 - mape: 99.9736 - val_loss: 5417432064.0000 - val_mape: 99.9675\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 5967675904.0000 - mape: 99.9696 - val_loss: 5417137152.0000 - val_mape: 99.9630\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5967364608.0000 - mape: 99.9656 - val_loss: 5416790528.0000 - val_mape: 99.9576\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5967005696.0000 - mape: 99.9610 - val_loss: 5416411648.0000 - val_mape: 99.9515\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5966628864.0000 - mape: 99.9555 - val_loss: 5416034304.0000 - val_mape: 99.9453\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5966228480.0000 - mape: 99.9503 - val_loss: 5415586816.0000 - val_mape: 99.9378\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5965778944.0000 - mape: 99.9443 - val_loss: 5415097344.0000 - val_mape: 99.9295\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 5965331456.0000 - mape: 99.9378 - val_loss: 5414985728.0000 - val_mape: 99.9219\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 5964799488.0000 - mape: 99.9308 - val_loss: 5414466048.0000 - val_mape: 99.9124\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5964293632.0000 - mape: 99.9219 - val_loss: 5413843968.0000 - val_mape: 99.8974\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5963551232.0000 - mape: 99.8976 - val_loss: 5413241856.0000 - val_mape: 99.8866\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5962886656.0000 - mape: 99.8775 - val_loss: 5412610048.0000 - val_mape: 99.8754\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 5962888192.0000 - mape: 99.8636 - val_loss: 5412572160.0000 - val_mape: 99.8684\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5962081280.0000 - mape: 99.8507 - val_loss: 5411929088.0000 - val_mape: 99.8564\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5961265152.0000 - mape: 99.8353 - val_loss: 5411270144.0000 - val_mape: 99.8442\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5960455168.0000 - mape: 99.8202 - val_loss: 5410460160.0000 - val_mape: 99.8298\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5959612416.0000 - mape: 99.8033 - val_loss: 5409278976.0000 - val_mape: 99.8133\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5958536704.0000 - mape: 99.7870 - val_loss: 5408155136.0000 - val_mape: 99.7958\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5957313536.0000 - mape: 99.7643 - val_loss: 5407305728.0000 - val_mape: 99.7807\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5956350976.0000 - mape: 99.7426 - val_loss: 5406338560.0000 - val_mape: 99.7635\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 5955305472.0000 - mape: 99.7188 - val_loss: 5405331456.0000 - val_mape: 99.7455\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 5954211328.0000 - mape: 99.6946 - val_loss: 5404301312.0000 - val_mape: 99.7271\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5953075200.0000 - mape: 99.6692 - val_loss: 5403208192.0000 - val_mape: 99.7076\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5951883264.0000 - mape: 99.6440 - val_loss: 5402068992.0000 - val_mape: 99.6874\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5950979584.0000 - mape: 99.6223 - val_loss: 5400918528.0000 - val_mape: 99.6669\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5949480448.0000 - mape: 99.5978 - val_loss: 5399664640.0000 - val_mape: 99.6444\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5948003328.0000 - mape: 99.5702 - val_loss: 5398355968.0000 - val_mape: 99.6210\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5946583552.0000 - mape: 99.5403 - val_loss: 5396995072.0000 - val_mape: 99.5967\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5945123328.0000 - mape: 99.5101 - val_loss: 5395649536.0000 - val_mape: 99.5728\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 5943622144.0000 - mape: 99.4826 - val_loss: 5394178048.0000 - val_mape: 99.5465\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5942034944.0000 - mape: 99.4496 - val_loss: 5392671744.0000 - val_mape: 99.5196\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5940408832.0000 - mape: 99.4161 - val_loss: 5391095296.0000 - val_mape: 99.4915\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5938671104.0000 - mape: 99.3840 - val_loss: 5389463552.0000 - val_mape: 99.4625\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5936928256.0000 - mape: 99.3458 - val_loss: 5387833856.0000 - val_mape: 99.4334\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5935112704.0000 - mape: 99.3134 - val_loss: 5386010112.0000 - val_mape: 99.4008\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 3s 279ms/step - loss: 5933175808.0000 - mape: 99.2739 - val_loss: 5384206848.0000 - val_mape: 99.3685\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5931214336.0000 - mape: 99.2359 - val_loss: 5382372352.0000 - val_mape: 99.3357\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5929199104.0000 - mape: 99.1943 - val_loss: 5380431872.0000 - val_mape: 99.3010\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 3s 231ms/step - loss: 5927085056.0000 - mape: 99.1558 - val_loss: 5378462720.0000 - val_mape: 99.2658\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5924942848.0000 - mape: 99.1120 - val_loss: 5376372736.0000 - val_mape: 99.2285\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5922699264.0000 - mape: 99.0638 - val_loss: 5374305280.0000 - val_mape: 99.1914\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5921228288.0000 - mape: 99.0239 - val_loss: 5372033536.0000 - val_mape: 99.1507\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5918166016.0000 - mape: 98.9722 - val_loss: 5369773056.0000 - val_mape: 99.1102\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5915541504.0000 - mape: 98.9240 - val_loss: 5367606272.0000 - val_mape: 99.0714\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5913148928.0000 - mape: 98.8747 - val_loss: 5365287936.0000 - val_mape: 99.0299\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5910596096.0000 - mape: 98.8264 - val_loss: 5362699264.0000 - val_mape: 98.9835\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5907845632.0000 - mape: 98.7683 - val_loss: 5360205312.0000 - val_mape: 98.9389\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5905103360.0000 - mape: 98.7147 - val_loss: 5357616640.0000 - val_mape: 98.8926\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5902265856.0000 - mape: 98.6609 - val_loss: 5354877952.0000 - val_mape: 98.8437\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5899332608.0000 - mape: 98.6006 - val_loss: 5352093696.0000 - val_mape: 98.7938\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5896290304.0000 - mape: 98.5452 - val_loss: 5349181952.0000 - val_mape: 98.7417\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5893208064.0000 - mape: 98.4753 - val_loss: 5346399232.0000 - val_mape: 98.6922\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5890135040.0000 - mape: 98.4183 - val_loss: 5343241216.0000 - val_mape: 98.6354\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5886785024.0000 - mape: 98.3527 - val_loss: 5340144640.0000 - val_mape: 98.5799\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5883480576.0000 - mape: 98.2812 - val_loss: 5337187328.0000 - val_mape: 98.5269\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5880174080.0000 - mape: 98.2156 - val_loss: 5333909504.0000 - val_mape: 98.4682\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5876623872.0000 - mape: 98.1464 - val_loss: 5330532864.0000 - val_mape: 98.4076\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5873005568.0000 - mape: 98.0815 - val_loss: 5327211520.0000 - val_mape: 98.3481\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5869382656.0000 - mape: 98.0053 - val_loss: 5323684352.0000 - val_mape: 98.2847\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5865557504.0000 - mape: 97.9297 - val_loss: 5320163328.0000 - val_mape: 98.2213\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5861729280.0000 - mape: 97.8489 - val_loss: 5316471296.0000 - val_mape: 98.1548\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5857758720.0000 - mape: 97.7712 - val_loss: 5312849920.0000 - val_mape: 98.0897\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5853790720.0000 - mape: 97.6906 - val_loss: 5308976128.0000 - val_mape: 98.0201\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5849652224.0000 - mape: 97.6084 - val_loss: 5305152512.0000 - val_mape: 97.9513\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5845463552.0000 - mape: 97.5234 - val_loss: 5301122048.0000 - val_mape: 97.8783\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5841114624.0000 - mape: 97.4353 - val_loss: 5296966656.0000 - val_mape: 97.8033\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5836642304.0000 - mape: 97.3522 - val_loss: 5292728832.0000 - val_mape: 97.7266\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5832034304.0000 - mape: 97.2549 - val_loss: 5288370176.0000 - val_mape: 97.6478\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5827406336.0000 - mape: 97.1621 - val_loss: 5284159488.0000 - val_mape: 97.5719\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5821553664.0000 - mape: 97.0653 - val_loss: 5279547392.0000 - val_mape: 97.4881\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5816563200.0000 - mape: 96.9723 - val_loss: 5274944512.0000 - val_mape: 97.4046\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5811565568.0000 - mape: 96.8650 - val_loss: 5270316032.0000 - val_mape: 97.3207\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5806502912.0000 - mape: 96.7610 - val_loss: 5265471488.0000 - val_mape: 97.2328\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5801270272.0000 - mape: 96.6592 - val_loss: 5260729344.0000 - val_mape: 97.1465\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5796067328.0000 - mape: 96.5566 - val_loss: 5255758336.0000 - val_mape: 97.0562\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5790691328.0000 - mape: 96.4424 - val_loss: 5250897408.0000 - val_mape: 96.9679\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5785270272.0000 - mape: 96.3327 - val_loss: 5245787648.0000 - val_mape: 96.8749\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 5779681792.0000 - mape: 96.2307 - val_loss: 5240358400.0000 - val_mape: 96.7761\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5773907456.0000 - mape: 96.1107 - val_loss: 5235140096.0000 - val_mape: 96.6809\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5768160256.0000 - mape: 95.9849 - val_loss: 5229976064.0000 - val_mape: 96.5870\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5762432512.0000 - mape: 95.8762 - val_loss: 5224226816.0000 - val_mape: 96.4817\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5756296192.0000 - mape: 95.7524 - val_loss: 5218805248.0000 - val_mape: 96.3827\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5750275072.0000 - mape: 95.6443 - val_loss: 5212825600.0000 - val_mape: 96.2731\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5743852544.0000 - mape: 95.5175 - val_loss: 5207124992.0000 - val_mape: 96.1688\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5737618432.0000 - mape: 95.3950 - val_loss: 5201220608.0000 - val_mape: 96.0608\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5731057664.0000 - mape: 95.2749 - val_loss: 5194869760.0000 - val_mape: 95.9444\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5724302336.0000 - mape: 95.1439 - val_loss: 5188907008.0000 - val_mape: 95.8351\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5717658112.0000 - mape: 95.0243 - val_loss: 5182503424.0000 - val_mape: 95.7179\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5710751744.0000 - mape: 94.8913 - val_loss: 5176249344.0000 - val_mape: 95.6028\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5703933952.0000 - mape: 94.7478 - val_loss: 5169892864.0000 - val_mape: 95.4858\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5696905216.0000 - mape: 94.6182 - val_loss: 5163067392.0000 - val_mape: 95.3598\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5689553920.0000 - mape: 94.4763 - val_loss: 5156529664.0000 - val_mape: 95.2397\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5682372096.0000 - mape: 94.3488 - val_loss: 5149725696.0000 - val_mape: 95.1139\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5674916864.0000 - mape: 94.2199 - val_loss: 5142617600.0000 - val_mape: 94.9824\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 5667262976.0000 - mape: 94.0784 - val_loss: 5135686656.0000 - val_mape: 94.8542\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5659667968.0000 - mape: 93.9402 - val_loss: 5128436736.0000 - val_mape: 94.7199\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5651767808.0000 - mape: 93.8029 - val_loss: 5121135104.0000 - val_mape: 94.5845\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5643818496.0000 - mape: 93.6639 - val_loss: 5113934848.0000 - val_mape: 94.4512\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5635805696.0000 - mape: 93.5389 - val_loss: 5106290688.0000 - val_mape: 94.3093\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 5627697152.0000 - mape: 93.3893 - val_loss: 5098672640.0000 - val_mape: 94.1677\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 5619345408.0000 - mape: 93.2474 - val_loss: 5090755584.0000 - val_mape: 94.0203\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 5610799616.0000 - mape: 93.1094 - val_loss: 5082980864.0000 - val_mape: 93.8755\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5602358272.0000 - mape: 92.9767 - val_loss: 5075099136.0000 - val_mape: 93.7282\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5593675776.0000 - mape: 92.8477 - val_loss: 5066876416.0000 - val_mape: 93.5749\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5584733696.0000 - mape: 92.7159 - val_loss: 5058732032.0000 - val_mape: 93.4220\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5575844864.0000 - mape: 92.5811 - val_loss: 5050526720.0000 - val_mape: 93.2688\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 5566819328.0000 - mape: 92.4501 - val_loss: 5041837056.0000 - val_mape: 93.1055\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5557501952.0000 - mape: 92.3131 - val_loss: 5033367040.0000 - val_mape: 92.9464\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 5548186624.0000 - mape: 92.1655 - val_loss: 5024593408.0000 - val_mape: 92.7812\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5538625536.0000 - mape: 92.0331 - val_loss: 5015898112.0000 - val_mape: 92.6178\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 5529134592.0000 - mape: 91.8941 - val_loss: 5007221760.0000 - val_mape: 92.4547\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5519629312.0000 - mape: 91.7557 - val_loss: 4997925888.0000 - val_mape: 92.2784\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 5509557760.0000 - mape: 91.6156 - val_loss: 4989124608.0000 - val_mape: 92.1116\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 5499773952.0000 - mape: 91.4849 - val_loss: 4979786240.0000 - val_mape: 91.9354\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 5489818112.0000 - mape: 91.3376 - val_loss: 4970384896.0000 - val_mape: 91.7569\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 5479443968.0000 - mape: 91.1927 - val_loss: 4960578560.0000 - val_mape: 91.5709\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5449381376.0000 - mape: 90.9714 - val_loss: 4914297856.0000 - val_mape: 91.2524\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 5429394944.0000 - mape: 90.7768 - val_loss: 4903409664.0000 - val_mape: 91.0573\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5417974272.0000 - mape: 90.6169 - val_loss: 4892711424.0000 - val_mape: 90.8655\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5406482432.0000 - mape: 90.4695 - val_loss: 4881905664.0000 - val_mape: 90.6713\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5394982400.0000 - mape: 90.3085 - val_loss: 4871045632.0000 - val_mape: 90.4760\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5383233536.0000 - mape: 90.1505 - val_loss: 4859842048.0000 - val_mape: 90.2741\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5371422720.0000 - mape: 89.9869 - val_loss: 4848993792.0000 - val_mape: 90.0783\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5359615488.0000 - mape: 89.8336 - val_loss: 4837416960.0000 - val_mape: 89.8691\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5347476480.0000 - mape: 89.6710 - val_loss: 4826045440.0000 - val_mape: 89.6632\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5335268864.0000 - mape: 89.5066 - val_loss: 4814369280.0000 - val_mape: 89.4515\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5322792960.0000 - mape: 89.3411 - val_loss: 4802225152.0000 - val_mape: 89.2309\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 5310063616.0000 - mape: 89.1698 - val_loss: 4791003136.0000 - val_mape: 89.0267\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 5297668608.0000 - mape: 89.0129 - val_loss: 4779134464.0000 - val_mape: 88.8104\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 5285015040.0000 - mape: 88.8508 - val_loss: 4766522368.0000 - val_mape: 88.5802\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5271720448.0000 - mape: 88.6742 - val_loss: 4754444288.0000 - val_mape: 88.3593\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 5258687488.0000 - mape: 88.4968 - val_loss: 4741483520.0000 - val_mape: 88.1218\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 5244967936.0000 - mape: 88.3361 - val_loss: 4728787968.0000 - val_mape: 87.8888\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 5231641088.0000 - mape: 88.1565 - val_loss: 4716509696.0000 - val_mape: 87.6630\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5218337792.0000 - mape: 87.9711 - val_loss: 4703886336.0000 - val_mape: 87.4305\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5204649472.0000 - mape: 87.8045 - val_loss: 4691004416.0000 - val_mape: 87.1927\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5190894080.0000 - mape: 87.6362 - val_loss: 4677778432.0000 - val_mape: 86.9482\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5186214400.0000 - mape: 87.3443 - val_loss: 4664115200.0000 - val_mape: 86.6950\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5173230592.0000 - mape: 86.8565 - val_loss: 4651019264.0000 - val_mape: 86.4519\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 5153670656.0000 - mape: 86.5207 - val_loss: 4637359616.0000 - val_mape: 86.1978\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5137783296.0000 - mape: 86.3467 - val_loss: 4623417856.0000 - val_mape: 85.9379\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5118836736.0000 - mape: 86.1239 - val_loss: 4609374208.0000 - val_mape: 85.6756\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 5104754688.0000 - mape: 85.9008 - val_loss: 4595829248.0000 - val_mape: 85.4220\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 17s 2s/step - loss: 5089012736.0000 - mape: 85.7334 - val_loss: 4581921792.0000 - val_mape: 85.1612\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 5073945088.0000 - mape: 85.5211 - val_loss: 4567716352.0000 - val_mape: 84.8941\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5058761216.0000 - mape: 85.3028 - val_loss: 4552777728.0000 - val_mape: 84.6127\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 47s 4s/step - loss: 5043071488.0000 - mape: 85.0836 - val_loss: 4538117120.0000 - val_mape: 84.3359\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5027530752.0000 - mape: 84.8157 - val_loss: 4524098560.0000 - val_mape: 84.0706\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 5012066816.0000 - mape: 84.5756 - val_loss: 4509058560.0000 - val_mape: 83.7854\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 4996147200.0000 - mape: 84.2903 - val_loss: 4494341120.0000 - val_mape: 83.5056\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 4980295680.0000 - mape: 84.1542 - val_loss: 4479356416.0000 - val_mape: 83.2201\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 4964187136.0000 - mape: 83.7127 - val_loss: 4463814144.0000 - val_mape: 82.9233\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 4950932992.0000 - mape: 84.0412 - val_loss: 4448270336.0000 - val_mape: 82.6257\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 4931073024.0000 - mape: 83.5931 - val_loss: 4432778240.0000 - val_mape: 82.3284\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 4914568192.0000 - mape: 83.3370 - val_loss: 4416886784.0000 - val_mape: 82.0226\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 4897827328.0000 - mape: 83.0350 - val_loss: 4401598976.0000 - val_mape: 81.7277\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 1s 128ms/step - loss: 4881280512.0000 - mape: 82.7281 - val_loss: 4386262528.0000 - val_mape: 81.4312\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 4864563712.0000 - mape: 82.4270 - val_loss: 4370112000.0000 - val_mape: 81.1181\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 4847272960.0000 - mape: 82.1184 - val_loss: 4353816576.0000 - val_mape: 80.8014\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 4829914112.0000 - mape: 81.8268 - val_loss: 4338095104.0000 - val_mape: 80.4951\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 4812951552.0000 - mape: 81.6549 - val_loss: 4321591808.0000 - val_mape: 80.1726\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 4795094016.0000 - mape: 81.2398 - val_loss: 4304500224.0000 - val_mape: 79.8378\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 4776956928.0000 - mape: 80.7555 - val_loss: 4288267520.0000 - val_mape: 79.5189\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 4759491072.0000 - mape: 80.7363 - val_loss: 4271271168.0000 - val_mape: 79.1841\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4740993024.0000 - mape: 80.2506 - val_loss: 4254840064.0000 - val_mape: 78.8595\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 1s 127ms/step - loss: 4723090432.0000 - mape: 79.8258 - val_loss: 4237153536.0000 - val_mape: 78.5091\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4704600064.0000 - mape: 79.4254 - val_loss: 4220546304.0000 - val_mape: 78.1791\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4686653440.0000 - mape: 79.1860 - val_loss: 4202295296.0000 - val_mape: 77.8154\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 4667337728.0000 - mape: 78.9986 - val_loss: 4185669632.0000 - val_mape: 77.4831\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4649211392.0000 - mape: 78.5343 - val_loss: 4168280064.0000 - val_mape: 77.1345\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 216ms/step - loss: 4630301696.0000 - mape: 78.1230 - val_loss: 4150299904.0000 - val_mape: 76.7729\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 3s 249ms/step - loss: 4611234816.0000 - mape: 77.7735 - val_loss: 4132361728.0000 - val_mape: 76.4111\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4591806976.0000 - mape: 77.4710 - val_loss: 4114110464.0000 - val_mape: 76.0418\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 4572414976.0000 - mape: 77.0503 - val_loss: 4096936704.0000 - val_mape: 75.6932\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 4553518592.0000 - mape: 76.8151 - val_loss: 4078513408.0000 - val_mape: 75.3181\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4533806080.0000 - mape: 76.2328 - val_loss: 4060005120.0000 - val_mape: 74.9399\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 4513978880.0000 - mape: 75.9803 - val_loss: 4041854464.0000 - val_mape: 74.5679\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 4494066688.0000 - mape: 75.6459 - val_loss: 4023342336.0000 - val_mape: 74.1872\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 4474366976.0000 - mape: 75.2664 - val_loss: 4004724736.0000 - val_mape: 73.8030\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 4453990400.0000 - mape: 74.9881 - val_loss: 3985463808.0000 - val_mape: 73.4042\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4433559040.0000 - mape: 74.6764 - val_loss: 3965978112.0000 - val_mape: 72.9993\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4412873728.0000 - mape: 74.1468 - val_loss: 3947461376.0000 - val_mape: 72.6131\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4392536064.0000 - mape: 73.9125 - val_loss: 3927900672.0000 - val_mape: 72.2037\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 4371551232.0000 - mape: 73.5634 - val_loss: 3908448256.0000 - val_mape: 71.7951\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 4350789632.0000 - mape: 73.1183 - val_loss: 3889894144.0000 - val_mape: 71.4039\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 4330358784.0000 - mape: 72.7432 - val_loss: 3870331648.0000 - val_mape: 70.9900\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 4309394432.0000 - mape: 72.8046 - val_loss: 3850118400.0000 - val_mape: 70.5606\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4287428864.0000 - mape: 72.1406 - val_loss: 3830207232.0000 - val_mape: 70.1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_4, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db66b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f12663b3790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9    756.453491\n",
       "5    753.715088\n",
       "2    719.322144\n",
       "4    712.748169\n",
       "0    691.983032\n",
       "8    674.045654\n",
       "3    653.919678\n",
       "7    651.653687\n",
       "6    637.685913\n",
       "1    624.480774\n",
       "dtype: float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_4.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 5, LSTM X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "474fd986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 4th model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_5 = Sequential()\n",
    "rnn_model_5.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_5.add(LSTM(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_5.add(LSTM(units=20, activation='tanh', return_sequences =True))\n",
    "rnn_model_5.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_5.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c43e359-c428-43f9-9369-f0804a5acdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_5, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_5.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2c33a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, None, 30)          6120      \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, None, 20)          4080      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, None, 10)          110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,561\n",
      "Trainable params: 10,520\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_5.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b13f6eb0-e734-48c4-a455-49949ea62a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [16,10] vs. [16,61,10]\n\t [[node sub\n (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py:1372)\n]] [Op:__inference_train_function_65713]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sub:\nIn[0] IteratorGetNext (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py:866)\t\nIn[1] sequential_4/dense_11/BiasAdd (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/core/dense.py:210)\n\nOperation defined at: (most recent call last)\n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_25168/4090895566.py\", line 1, in <module>\n>>>     history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n>>> \n>>>   File \"/tmp/ipykernel_25168/3069528292.py\", line 10, in train_rnn_model\n>>>     history =  rnn_model_5.fit(X_train, y_train,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 725, in update_state\n>>>     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 1372, in mean_absolute_percentage_error\n>>>     (y_true - y_pred) / backend.maximum(tf.abs(y_true),\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25168/4090895566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25168/3069528292.py\u001b[0m in \u001b[0;36mtrain_rnn_model\u001b[0;34m(rnn_model_5, patience, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m                     restore_best_weights = True)\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# The fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     history =  rnn_model_5.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Auto split for validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [16,10] vs. [16,61,10]\n\t [[node sub\n (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py:1372)\n]] [Op:__inference_train_function_65713]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sub:\nIn[0] IteratorGetNext (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py:866)\t\nIn[1] sequential_4/dense_11/BiasAdd (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/core/dense.py:210)\n\nOperation defined at: (most recent call last)\n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_25168/4090895566.py\", line 1, in <module>\n>>>     history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n>>> \n>>>   File \"/tmp/ipykernel_25168/3069528292.py\", line 10, in train_rnn_model\n>>>     history =  rnn_model_5.fit(X_train, y_train,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 725, in update_state\n>>>     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 1372, in mean_absolute_percentage_error\n>>>     (y_true - y_pred) / backend.maximum(tf.abs(y_true),\n>>> "
     ]
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/2308850997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_5.predict(X_test) \n",
    "print(y_pred)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71da140",
   "metadata": {},
   "source": [
    "### Train model 6, GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 6th model layers architecture\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_6 = Sequential()\n",
    "rnn_model_6.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_6.add(GRU(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_6.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_6.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_6, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_6.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4532805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 30)                4680      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,001\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_6.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b164948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 5s 129ms/step - loss: 6508211712.0000 - mape: 99.9973 - val_loss: 6164634624.0000 - val_mape: 99.9925\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6508042240.0000 - mape: 99.9963 - val_loss: 6164467200.0000 - val_mape: 99.9919\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6507843584.0000 - mape: 99.9952 - val_loss: 6164217856.0000 - val_mape: 99.9901\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6507555328.0000 - mape: 99.9932 - val_loss: 6163867648.0000 - val_mape: 99.9878\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6507193856.0000 - mape: 99.9904 - val_loss: 6163424256.0000 - val_mape: 99.9827\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 6506816000.0000 - mape: 99.9868 - val_loss: 6163078144.0000 - val_mape: 99.9791\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6506491904.0000 - mape: 99.9842 - val_loss: 6162754048.0000 - val_mape: 99.9760\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6506182656.0000 - mape: 99.9819 - val_loss: 6162455040.0000 - val_mape: 99.9730\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6505881088.0000 - mape: 99.9794 - val_loss: 6162149376.0000 - val_mape: 99.9700\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6505568768.0000 - mape: 99.9768 - val_loss: 6161831936.0000 - val_mape: 99.9667\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6505258496.0000 - mape: 99.9743 - val_loss: 6161536512.0000 - val_mape: 99.9632\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6504960512.0000 - mape: 99.9716 - val_loss: 6161231872.0000 - val_mape: 99.9592\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504649728.0000 - mape: 99.9688 - val_loss: 6160894976.0000 - val_mape: 99.9548\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504318976.0000 - mape: 99.9659 - val_loss: 6160561152.0000 - val_mape: 99.9501\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6503980544.0000 - mape: 99.9627 - val_loss: 6160230400.0000 - val_mape: 99.9449\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6503644672.0000 - mape: 99.9595 - val_loss: 6159876096.0000 - val_mape: 99.9395\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6503286784.0000 - mape: 99.9558 - val_loss: 6159512576.0000 - val_mape: 99.9339\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6502918144.0000 - mape: 99.9522 - val_loss: 6159133184.0000 - val_mape: 99.9282\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6502407680.0000 - mape: 99.9417 - val_loss: 6158757376.0000 - val_mape: 99.9231\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6501965824.0000 - mape: 99.9346 - val_loss: 6158361600.0000 - val_mape: 99.9176\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6501551104.0000 - mape: 99.9300 - val_loss: 6157952512.0000 - val_mape: 99.9106\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6501090304.0000 - mape: 99.9150 - val_loss: 6157492224.0000 - val_mape: 99.8980\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6500603392.0000 - mape: 99.9062 - val_loss: 6156943360.0000 - val_mape: 99.8589\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6499983360.0000 - mape: 99.8780 - val_loss: 6156482048.0000 - val_mape: 99.7735\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6499341312.0000 - mape: 99.8572 - val_loss: 6155995648.0000 - val_mape: 99.7353\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6498842624.0000 - mape: 99.8481 - val_loss: 6155539456.0000 - val_mape: 99.6960\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6498355200.0000 - mape: 99.8392 - val_loss: 6155056640.0000 - val_mape: 99.6558\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6497838080.0000 - mape: 99.8294 - val_loss: 6154546176.0000 - val_mape: 99.5968\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6497308160.0000 - mape: 99.8169 - val_loss: 6154052608.0000 - val_mape: 99.5534\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6496780288.0000 - mape: 99.8004 - val_loss: 6153535488.0000 - val_mape: 99.5275\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6496231424.0000 - mape: 99.7856 - val_loss: 6152996352.0000 - val_mape: 99.5039\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6495663104.0000 - mape: 99.7741 - val_loss: 6152457216.0000 - val_mape: 99.4806\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6495090688.0000 - mape: 99.7638 - val_loss: 6151896064.0000 - val_mape: 99.4566\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 6494498816.0000 - mape: 99.7536 - val_loss: 6151331328.0000 - val_mape: 99.4327\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6493900288.0000 - mape: 99.7415 - val_loss: 6150763520.0000 - val_mape: 99.4086\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6493298176.0000 - mape: 99.7309 - val_loss: 6150177280.0000 - val_mape: 99.3839\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6492681216.0000 - mape: 99.7201 - val_loss: 6149591040.0000 - val_mape: 99.3592\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6492054528.0000 - mape: 99.7093 - val_loss: 6148975616.0000 - val_mape: 99.3333\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6491406848.0000 - mape: 99.6963 - val_loss: 6148365824.0000 - val_mape: 99.3076\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6490754560.0000 - mape: 99.6867 - val_loss: 6147750912.0000 - val_mape: 99.2818\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6490104320.0000 - mape: 99.6734 - val_loss: 6147119616.0000 - val_mape: 99.2553\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6489435136.0000 - mape: 99.6616 - val_loss: 6146464768.0000 - val_mape: 99.2277\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6488743936.0000 - mape: 99.6503 - val_loss: 6145799680.0000 - val_mape: 99.1997\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6488038400.0000 - mape: 99.6362 - val_loss: 6145108992.0000 - val_mape: 99.1707\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6487322112.0000 - mape: 99.6233 - val_loss: 6144437760.0000 - val_mape: 99.1424\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6486604800.0000 - mape: 99.6104 - val_loss: 6143751680.0000 - val_mape: 99.1136\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6485872128.0000 - mape: 99.5988 - val_loss: 6143042560.0000 - val_mape: 99.0837\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6485126656.0000 - mape: 99.5848 - val_loss: 6142319616.0000 - val_mape: 99.0533\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6484364288.0000 - mape: 99.5706 - val_loss: 6141584384.0000 - val_mape: 99.0224\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6483596800.0000 - mape: 99.5566 - val_loss: 6140859904.0000 - val_mape: 98.9919\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6482822656.0000 - mape: 99.5433 - val_loss: 6140128256.0000 - val_mape: 98.9612\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6482038272.0000 - mape: 99.5301 - val_loss: 6139361280.0000 - val_mape: 98.9289\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6481236480.0000 - mape: 99.5148 - val_loss: 6138595840.0000 - val_mape: 98.8966\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 6480420352.0000 - mape: 99.5015 - val_loss: 6137803776.0000 - val_mape: 98.8633\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6479596544.0000 - mape: 99.4843 - val_loss: 6137031680.0000 - val_mape: 98.8308\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6478767104.0000 - mape: 99.4708 - val_loss: 6136214528.0000 - val_mape: 98.7964\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6477908992.0000 - mape: 99.4559 - val_loss: 6135417344.0000 - val_mape: 98.7628\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6477062144.0000 - mape: 99.4412 - val_loss: 6134595584.0000 - val_mape: 98.7283\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6476193280.0000 - mape: 99.4239 - val_loss: 6133774848.0000 - val_mape: 98.6937\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 6475320832.0000 - mape: 99.4083 - val_loss: 6132915200.0000 - val_mape: 98.6575\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6474423296.0000 - mape: 99.3931 - val_loss: 6132058624.0000 - val_mape: 98.6214\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6473522176.0000 - mape: 99.3777 - val_loss: 6131198976.0000 - val_mape: 98.5852\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6472606720.0000 - mape: 99.3596 - val_loss: 6130316800.0000 - val_mape: 98.5480\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6471676928.0000 - mape: 99.3451 - val_loss: 6129436672.0000 - val_mape: 98.5109\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6470746624.0000 - mape: 99.3261 - val_loss: 6128567808.0000 - val_mape: 98.4743\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6469812224.0000 - mape: 99.3113 - val_loss: 6127654400.0000 - val_mape: 98.4358\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6468856320.0000 - mape: 99.2924 - val_loss: 6126720512.0000 - val_mape: 98.3965\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6467873280.0000 - mape: 99.2765 - val_loss: 6125779968.0000 - val_mape: 98.3569\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6466885120.0000 - mape: 99.2589 - val_loss: 6124849664.0000 - val_mape: 98.3176\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6465903616.0000 - mape: 99.2397 - val_loss: 6123893248.0000 - val_mape: 98.2773\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6464904192.0000 - mape: 99.2229 - val_loss: 6122954752.0000 - val_mape: 98.2378\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6463894528.0000 - mape: 99.2048 - val_loss: 6121978368.0000 - val_mape: 98.1966\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6462860800.0000 - mape: 99.1850 - val_loss: 6120998400.0000 - val_mape: 98.1553\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6461832192.0000 - mape: 99.1669 - val_loss: 6120025088.0000 - val_mape: 98.1142\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6460799488.0000 - mape: 99.1482 - val_loss: 6119008768.0000 - val_mape: 98.0713\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6459728384.0000 - mape: 99.1279 - val_loss: 6117980672.0000 - val_mape: 98.0280\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6458642944.0000 - mape: 99.1093 - val_loss: 6116942848.0000 - val_mape: 97.9842\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6457556480.0000 - mape: 99.0905 - val_loss: 6115907072.0000 - val_mape: 97.9405\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6456460288.0000 - mape: 99.0715 - val_loss: 6114870272.0000 - val_mape: 97.8968\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6455366656.0000 - mape: 99.0514 - val_loss: 6113846272.0000 - val_mape: 97.8535\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6454277120.0000 - mape: 99.0312 - val_loss: 6112775168.0000 - val_mape: 97.8084\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6453141504.0000 - mape: 99.0111 - val_loss: 6111676928.0000 - val_mape: 97.7620\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6451986944.0000 - mape: 98.9897 - val_loss: 6110573568.0000 - val_mape: 97.7154\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6450829312.0000 - mape: 98.9686 - val_loss: 6109471744.0000 - val_mape: 97.6689\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6449666560.0000 - mape: 98.9486 - val_loss: 6108335104.0000 - val_mape: 97.6209\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6448484864.0000 - mape: 98.9273 - val_loss: 6107253760.0000 - val_mape: 97.5752\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6447322624.0000 - mape: 98.9067 - val_loss: 6106108928.0000 - val_mape: 97.5269\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6446103552.0000 - mape: 98.8846 - val_loss: 6104939008.0000 - val_mape: 97.4774\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6444893184.0000 - mape: 98.8638 - val_loss: 6103780352.0000 - val_mape: 97.4285\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6443668992.0000 - mape: 98.8414 - val_loss: 6102649856.0000 - val_mape: 97.3808\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6442475520.0000 - mape: 98.8189 - val_loss: 6101472768.0000 - val_mape: 97.3310\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6441232896.0000 - mape: 98.7979 - val_loss: 6100315648.0000 - val_mape: 97.2821\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 6439998976.0000 - mape: 98.7733 - val_loss: 6099154944.0000 - val_mape: 97.2330\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6438756864.0000 - mape: 98.7532 - val_loss: 6097930752.0000 - val_mape: 97.1813\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6437481984.0000 - mape: 98.7291 - val_loss: 6096718336.0000 - val_mape: 97.1301\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6436197376.0000 - mape: 98.7063 - val_loss: 6095474176.0000 - val_mape: 97.0774\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6434888704.0000 - mape: 98.6848 - val_loss: 6094214656.0000 - val_mape: 97.0242\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6433575424.0000 - mape: 98.6584 - val_loss: 6092983296.0000 - val_mape: 96.9721\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6432267264.0000 - mape: 98.6346 - val_loss: 6091751424.0000 - val_mape: 96.9200\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6430962688.0000 - mape: 98.6143 - val_loss: 6090492928.0000 - val_mape: 96.8668\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6429633536.0000 - mape: 98.5870 - val_loss: 6089175040.0000 - val_mape: 96.8110\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6428260352.0000 - mape: 98.5638 - val_loss: 6087925760.0000 - val_mape: 96.7581\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6426927104.0000 - mape: 98.5411 - val_loss: 6086617088.0000 - val_mape: 96.7027\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6425555968.0000 - mape: 98.5138 - val_loss: 6085335552.0000 - val_mape: 96.6485\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 124ms/step - loss: 6424166400.0000 - mape: 98.4901 - val_loss: 6083976192.0000 - val_mape: 96.5909\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6422757888.0000 - mape: 98.4661 - val_loss: 6082606592.0000 - val_mape: 96.5329\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6421326336.0000 - mape: 98.4403 - val_loss: 6081283072.0000 - val_mape: 96.4769\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6419930112.0000 - mape: 98.4140 - val_loss: 6079962624.0000 - val_mape: 96.4209\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6418513408.0000 - mape: 98.3884 - val_loss: 6078555136.0000 - val_mape: 96.3613\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6417040384.0000 - mape: 98.3641 - val_loss: 6077146624.0000 - val_mape: 96.3017\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6415579648.0000 - mape: 98.3363 - val_loss: 6075779584.0000 - val_mape: 96.2437\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6414133760.0000 - mape: 98.3092 - val_loss: 6074428416.0000 - val_mape: 96.1864\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6412691968.0000 - mape: 98.2814 - val_loss: 6073060864.0000 - val_mape: 96.1285\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6411232256.0000 - mape: 98.2573 - val_loss: 6071585792.0000 - val_mape: 96.0660\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6409705472.0000 - mape: 98.2286 - val_loss: 6070164480.0000 - val_mape: 96.0057\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6408215552.0000 - mape: 98.2012 - val_loss: 6068771840.0000 - val_mape: 95.9466\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6406695424.0000 - mape: 98.1791 - val_loss: 6067277824.0000 - val_mape: 95.8833\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6405156352.0000 - mape: 98.1478 - val_loss: 6065827328.0000 - val_mape: 95.8218\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6403625984.0000 - mape: 98.1188 - val_loss: 6064399360.0000 - val_mape: 95.7611\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 6402102784.0000 - mape: 98.0930 - val_loss: 6062923776.0000 - val_mape: 95.6985\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6400555008.0000 - mape: 98.0643 - val_loss: 6061441536.0000 - val_mape: 95.6356\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6398985216.0000 - mape: 98.0361 - val_loss: 6059893248.0000 - val_mape: 95.5699\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6397365248.0000 - mape: 98.0084 - val_loss: 6058378240.0000 - val_mape: 95.5056\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6395768320.0000 - mape: 97.9791 - val_loss: 6056823296.0000 - val_mape: 95.4396\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6394146304.0000 - mape: 97.9494 - val_loss: 6055345152.0000 - val_mape: 95.3768\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6392554496.0000 - mape: 97.9219 - val_loss: 6053754368.0000 - val_mape: 95.3092\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6390905344.0000 - mape: 97.8900 - val_loss: 6052233216.0000 - val_mape: 95.2446\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6389294592.0000 - mape: 97.8602 - val_loss: 6050713600.0000 - val_mape: 95.1800\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6387662848.0000 - mape: 97.8327 - val_loss: 6049112576.0000 - val_mape: 95.1120\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6385982464.0000 - mape: 97.8032 - val_loss: 6047502848.0000 - val_mape: 95.0436\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6384291328.0000 - mape: 97.7720 - val_loss: 6045906944.0000 - val_mape: 94.9757\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6382619648.0000 - mape: 97.7391 - val_loss: 6044357632.0000 - val_mape: 94.9098\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6380968960.0000 - mape: 97.7087 - val_loss: 6042738176.0000 - val_mape: 94.8410\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6379260416.0000 - mape: 97.6793 - val_loss: 6041067008.0000 - val_mape: 94.7699\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6377503232.0000 - mape: 97.6504 - val_loss: 6039407616.0000 - val_mape: 94.6993\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6375778816.0000 - mape: 97.6185 - val_loss: 6037799424.0000 - val_mape: 94.6309\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6374064128.0000 - mape: 97.5849 - val_loss: 6036166656.0000 - val_mape: 94.5614\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6372328960.0000 - mape: 97.5563 - val_loss: 6034502656.0000 - val_mape: 94.4905\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6370585600.0000 - mape: 97.5222 - val_loss: 6032770048.0000 - val_mape: 94.4168\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6368783360.0000 - mape: 97.4879 - val_loss: 6031115264.0000 - val_mape: 94.3463\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6367031296.0000 - mape: 97.4557 - val_loss: 6029469184.0000 - val_mape: 94.2762\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6365264896.0000 - mape: 97.4261 - val_loss: 6027737088.0000 - val_mape: 94.2024\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6363429888.0000 - mape: 97.3957 - val_loss: 6026048512.0000 - val_mape: 94.1305\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6361663488.0000 - mape: 97.3621 - val_loss: 6024319488.0000 - val_mape: 94.0568\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6359835648.0000 - mape: 97.3268 - val_loss: 6022560256.0000 - val_mape: 93.9818\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6357975040.0000 - mape: 97.2958 - val_loss: 6020725760.0000 - val_mape: 93.9036\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6356091392.0000 - mape: 97.2630 - val_loss: 6018994176.0000 - val_mape: 93.8298\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6354273792.0000 - mape: 97.2287 - val_loss: 6017351168.0000 - val_mape: 93.7597\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6352474624.0000 - mape: 97.1984 - val_loss: 6015517696.0000 - val_mape: 93.6815\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6350583296.0000 - mape: 97.1595 - val_loss: 6013742080.0000 - val_mape: 93.6058\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6348686336.0000 - mape: 97.1284 - val_loss: 6011880448.0000 - val_mape: 93.5263\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6346758144.0000 - mape: 97.0912 - val_loss: 6010066944.0000 - val_mape: 93.4489\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6344843776.0000 - mape: 97.0563 - val_loss: 6008284160.0000 - val_mape: 93.3728\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6342960128.0000 - mape: 97.0206 - val_loss: 6006459904.0000 - val_mape: 93.2949\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6341023232.0000 - mape: 96.9864 - val_loss: 6004575744.0000 - val_mape: 93.2145\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6339052544.0000 - mape: 96.9504 - val_loss: 6002738176.0000 - val_mape: 93.1360\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6337113600.0000 - mape: 96.9151 - val_loss: 6000917504.0000 - val_mape: 93.0582\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6335173632.0000 - mape: 96.8808 - val_loss: 5998990336.0000 - val_mape: 92.9758\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6333147136.0000 - mape: 96.8432 - val_loss: 5997108736.0000 - val_mape: 92.8954\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6331169280.0000 - mape: 96.8103 - val_loss: 5995202048.0000 - val_mape: 92.8139\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6329155072.0000 - mape: 96.7735 - val_loss: 5993335296.0000 - val_mape: 92.7341\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6327190528.0000 - mape: 96.7351 - val_loss: 5991405056.0000 - val_mape: 92.6515\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6325153792.0000 - mape: 96.6988 - val_loss: 5989504512.0000 - val_mape: 92.5702\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6323144704.0000 - mape: 96.6628 - val_loss: 5987554304.0000 - val_mape: 92.4868\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6321076736.0000 - mape: 96.6277 - val_loss: 5985626624.0000 - val_mape: 92.4043\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6319032832.0000 - mape: 96.5909 - val_loss: 5983627776.0000 - val_mape: 92.3187\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6316977152.0000 - mape: 96.5624 - val_loss: 5981710336.0000 - val_mape: 92.2366\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6314912768.0000 - mape: 96.5360 - val_loss: 5979668480.0000 - val_mape: 92.1492\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6312787968.0000 - mape: 96.5087 - val_loss: 5977705472.0000 - val_mape: 92.0651\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6310737920.0000 - mape: 96.4864 - val_loss: 5975773184.0000 - val_mape: 91.9823\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6308651008.0000 - mape: 96.4617 - val_loss: 5973754368.0000 - val_mape: 91.8958\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6306545152.0000 - mape: 96.4315 - val_loss: 5971729408.0000 - val_mape: 91.8090\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6304386048.0000 - mape: 96.4072 - val_loss: 5969629184.0000 - val_mape: 91.7190\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6302209024.0000 - mape: 96.3817 - val_loss: 5967577088.0000 - val_mape: 91.6309\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6300062208.0000 - mape: 96.3525 - val_loss: 5965621248.0000 - val_mape: 91.5471\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6297961472.0000 - mape: 96.3282 - val_loss: 5963559424.0000 - val_mape: 91.4586\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6295784448.0000 - mape: 96.3028 - val_loss: 5961435648.0000 - val_mape: 91.3675\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6293570560.0000 - mape: 96.2726 - val_loss: 5959380480.0000 - val_mape: 91.2793\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6291407360.0000 - mape: 96.2494 - val_loss: 5957314048.0000 - val_mape: 91.1906\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6289189888.0000 - mape: 96.2204 - val_loss: 5955155456.0000 - val_mape: 91.0979\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 6286933504.0000 - mape: 96.1907 - val_loss: 5953028096.0000 - val_mape: 91.0065\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6284728320.0000 - mape: 96.1665 - val_loss: 5950963200.0000 - val_mape: 90.9178\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 6282517504.0000 - mape: 96.1411 - val_loss: 5948808192.0000 - val_mape: 90.8252\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 6280272896.0000 - mape: 96.1091 - val_loss: 5946719232.0000 - val_mape: 90.7354\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6278035456.0000 - mape: 96.0821 - val_loss: 5944541184.0000 - val_mape: 90.6417\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 3s 235ms/step - loss: 6275776000.0000 - mape: 96.0554 - val_loss: 5942462976.0000 - val_mape: 90.5524\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6273534976.0000 - mape: 96.0270 - val_loss: 5940274176.0000 - val_mape: 90.4583\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6271261696.0000 - mape: 96.0005 - val_loss: 5938202112.0000 - val_mape: 90.3691\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6269037568.0000 - mape: 95.9724 - val_loss: 5935952896.0000 - val_mape: 90.2723\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6266678784.0000 - mape: 95.9416 - val_loss: 5933764096.0000 - val_mape: 90.1781\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6264390144.0000 - mape: 95.9147 - val_loss: 5931584000.0000 - val_mape: 90.0842\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6262073344.0000 - mape: 95.8906 - val_loss: 5929377792.0000 - val_mape: 89.9892\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6259736576.0000 - mape: 95.8627 - val_loss: 5927077888.0000 - val_mape: 89.8901\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6257354752.0000 - mape: 95.8250 - val_loss: 5924867072.0000 - val_mape: 89.7949\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 6255002624.0000 - mape: 95.7998 - val_loss: 5922608128.0000 - val_mape: 89.6975\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6252627456.0000 - mape: 95.7692 - val_loss: 5920367616.0000 - val_mape: 89.6009\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6250272256.0000 - mape: 95.7414 - val_loss: 5918111232.0000 - val_mape: 89.5036\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6247875072.0000 - mape: 95.7133 - val_loss: 5915814912.0000 - val_mape: 89.4046\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6245458944.0000 - mape: 95.6858 - val_loss: 5913516544.0000 - val_mape: 89.3054\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6243068416.0000 - mape: 95.6541 - val_loss: 5911335936.0000 - val_mape: 89.2113\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqTUlEQVR4nO3dd3hc1Z3/8fdRs5plWc2SVSz3gnDHmGJCMcYNGwwYCB2Cw4ZkYVNYAr9NstlsEkJ62ISFhWCKKYlpBmNsjDHF3cZF7lUualbvdc7vjzsGY9ylmTsafV7Po2dGVzNzv9wZfzhz7rnnGGstIiISXELcLkBERNqfwl1EJAgp3EVEgpDCXUQkCCncRUSCUJjbBQAkJSXZ7Oxst8sQEelQ1q5dW2KtTT7e3wIi3LOzs1mzZo3bZYiIdCjGmLwT/U3dMiIiQUjhLiIShBTuIiJBSOEuIhKEFO4iIkHolOFujHnWGFNsjMk9aluCMWaRMWan97a7d7sxxvzZGLPLGLPRGDPSl8WLiMjxnU7L/Tlg4jHbHgYWW2v7A4u9vwNMAvp7f2YBf2ufMkVE5Eyccpy7tfZjY0z2MZunA5d6788GPgL+3bv9eevMI7zCGBNvjEmz1ha0W8VH2bz2Ywo3f0xIaAQhoaGY0AhCwsIhPIqQiGiIiCGkSyxhkbGERcYQHhVHl6gYuoSHERkeSmR4CF3CQgkPNRhjfFGiiIgrzvYiph5HBXYh0MN7Px04cNTjDnq3fS3cjTGzcFr3ZGVlnVUR1ZsXccWeP5/RczzWUE8EdURSaiOpI5I6utBgImk0UTSERFMXGkt9aBwNYXE0hnWlKaIbnoh4orol0DU+hW4JiXSNjqZHXCS9EqPpGhl+VvWLiPhKm69QtdZaY8wZr/hhrX0KeApg9OjRZ7ViyNibH8E2fo+W5iaam5tpaW6kuamJlqY6WhtqaW2swdNYQ2tDHa2N1djGWmiqxTbXYZpqCWmuJbKljpiWOsJa6ghrPUxEaw1RLdVENtd/fYcHv7xbYyMpsIkstRm8Ez6RlqxxTBmWxsX9kknu2uVs/nNERNrN2YZ70ZHuFmNMGlDs3X4IyDzqcRnebb4RHoUJjyIcaPe2c2szNFRCfTnUV0BDBdSX01BdSl1lCa21ZcSV7+fyw+uY2rySj/afz73b7qeZMFLjIknvHkV6fNRXbrMTY+iVEE1IiLqARMS3zjbc3wbuAH7tvX3rqO3fNca8ApwPVPqqv93nQsMhJsn5OUqk9+cLzQ2w4n+4dPHPWT0wnjcyHyG3zHCooo7PD5Qzf1MBLZ4vv5jERIQyKC2Ogald6Z0YQ6/EaLKTYshKiCYyPNQv/2kiEvxOGe7GmJdxTp4mGWMOAj/FCfXXjDH3AHnATO/D5wOTgV1AHXCXD2oOLOGRMO4HEB5D/IJ/567C5TD6bpj0Hejag1aPpbi6gfyKenYX17KloIot+VXM31RARV3zFy9jDKTFRdIrMYbspGinle+93yshhqgIBb+InD4TCAtkjx492gbFrJAFG+DTP8KWNyEkHEbcAhc9AN2zj/vwirom8krr2Fday76SOvJKa9lXWkteaR2ltU1feWyq9+RtdmIMvbzhn+1t+cd0CYjJPUXEz4wxa621o4/7N4W7D5TuhmV/hvVzICQMrvoljLrTaZ6fpsr6ZvZ7gz+vtJa9X4R/HSU1jV95bHLXLmR7gz87KebL/wloJI9IUFO4u6XyELx1P+xZAumj4OLvQ9/LISK6TS9b09jiBH3Jl+G/r7SOfSW1FFd/PfgH9ujKoNSuDEztyqDUOPr3iFX/vkgQULi7yeOBDXPgo19D5QEIi4TscdB/AvS/EhJ6t+vu6ppayCv9spW/q7iG7YXV7CiqprHFA0CIgezEGAZ+EfhO6GdpJI9Ih6JwDwQtTZD3KexYCDsXQtluZ3vSQBgwAQZMhMzznVE6PtDqseSV1rK9sJpthdXe2yryyuo48hGICg9lQI9Yb+jHfdHaT4rVuH2RQKRwD0Slu2HH+7Dzfdj3GXiaIbIb9BvvBH2/8RCd4PMy6ppa2FlU82XoF1WxvbCakpovT+gmxUYwKDWOc3rGMaRnHDnp3eidGKNWvojLFO6BrrEadi/5MuxrD4MJgYwxMOAqJ+xTBp/RCdm2KqlpZHthNVsLqr7S2m9qdbp2YiJCGZzmBP2QnnHk9OxG/x6xhIdqFmkRf1G4dyQeDxR87gT9jgXO8EqAblkweCoMngaZYyDE/ydEm1s97CquIfdQJZvzq9ic79zWNbUCEBEawuCecQzP6MbwrHiGZcSTrRa+iM8o3DuyqgKnj377fNj9IbQ2QWwPGDTFCfrsi33WT386PB7L3tJaNudXkXuokg0HKth0qPKLwI+LDGNYphP0wzPjGdmrOwkxEa7VKxJMFO7BoqHKCfqtb8PORdBcB1HdYcg1MPRG54RsiPvdIq0ey67iGjYcqODzAxVsOFDB9qJqWr3TMPRJimFkr+6M8v70S45V617kLCjcg1FTHexeDJvfhG3vQku903Uz9AY4dyakDHK7wq+ob2pl06FK1u0vZ21eOevyyr+4CrdrZBgjs74M+2GZ8cTqqluRU1K4B7vGGifgN77qXDBlPZA6FIbOhJzrIS7N7Qq/xlpLXmkda/PKWbvfCfvtRdVY64zDH5wWx5jeCZzfO4HR2QkajilyHAr3zqSmGHJfd4I+fx1goPclTrfN4KshMs7tCk+oqqGZ9fsrWJNXztq8MtbmldPQ7IzO6Zscw5jeiZzfO4HzeieQHh/lcrUi7lO4d1Ylu2DTa7DxNSjf61wdO3ASDL/FmQbBhRE3Z6KpxUNufiWr9paxam8Zq/eVUd3QAkBWQjQX9k3kAu9PStfIU7yaSPBRuHd21sLBNU7Q586FulLolgkjboMRt0K3dLcrPC2tHsu2wipW7ilj+Z5SVuwp/SLs+6fEesM+ibF9EoiP1ogcCX4Kd/lSS5MzrHLtc07/vAmBflc6s1b2nwChHedEZqvHsjm/kuW7S1m2u5RVe8uob27FGDinZxwX9k3igr6JnJedoBO0EpQU7nJ8ZXvh8xfg85egphC6pjldNiNvh+693K7ujDW1eNh4sIJlu0tZtruEdXkVNLV6CAsxDMuM/6IbZ2RWd82KKUFB4S4n19riTHuw9jln/DxAvytgzLedOW4CYOz82WhobmVtXjnLdpewbHcpGw9W0uqxRISFMCqrO5cOTOayQSn0T4nF+HFqB5H2onCX01dxwGnNr30OaoogoQ+cd6+zqlRkN7era5PqhmZW7ytj2a5SPt1VwrbCagDS46O4bFAylw9K4YI+SVrSUDoMhbucuZYm50rYlf8LB1dBeAwMuwnGzAq4C6TOVkFlPR9tP8yH24r5bFcJdU2tdAkL4YK+iVw+KIXLBqaQmdC2hVVEfEnhLm2T/zmseho2/RNaG6H3N+CC7zqLjQRJd0ZjSyur9pbx4bZilmwrZl9pHQD9UmK5fFAKlw5M5rzsBM16KQFF4S7to7YE1s2G1c9A1SFIHgwXfg/OvR7CgusK0r0ltXy4rZiPthezck8ZTa0eunYJ4+L+SVzmDXuNrRe3KdylfbU2O1fBLvszFOVCbCqMvQ9G3QVR8W5X1+5qG1v4bFcJS7YXs2TbYQqrGgA4N70blw1K4bKByQzLiNfkZ+J3CnfxDWudaYiX/Rn2fAQRsc54+fPvg/hMt6vzCWstWwuqvUFfzLr95XgsJMZE8I0BzuibS/on0y3avWmYpfNQuIvvFWyEZX9xroA1Bs6ZARc9AKk5blfmU+W1TXy88zBLthXz0Y7DVNQ1ExpiGJXVnSsGpzApJ42sRJ2UFd9QuIv/VByAlU86QymbamDgFLjkB5A+yu3KfK7VY1l/oIIl24r5cFsxWwqqAOdq2Uk5qUzMSaNfSqzLVUowUbiL/9WXw8qnYMVfoaHCmahs3A8h+yK3K/ObA2V1LMgt5L3cAtbtrwBgQI9YJuakMfncVAb26KqLp6RNFO7insZqZ3TN8iechb+zLnRa8n2vCJphlKejoLKe93MLeS+3kNX7yvBY6J0Uw4QhPbi4fxLn904kIkzDLOXMKNzFfU11zpWvn/3JGUbZcyRc+uOgGit/ug5XN7JwSyELcgtZsaeU5lZLfHQ4U85NY8bIdEZmdVeLXk6Lz8LdGPMAcC9ggKettX80xgwHngQigRbgO9baVSd7HYV7J9LSCBtehk9+BxX7IX00XPaI023TCQOtrqmF5btLeWt9Pgu3FNLQ7CE9PopJOalMHprGcA2xlJPwSbgbY3KAV4AxQBOwALgP+CvwB2vte8aYycBD1tpLT/ZaCvdOqKUJNsyBpY9D1UHIHAuXP+qsGtVJ1TS28H5uIfM3FfDJzhKaWj2kdYtkUk4aU4elMSIzXi16+YqThXtbJrkeDKy01tZ5d7IUmAFY4Mhabt2A/DbsQ4JVWIQzJn7YzU53zce/g9lXO33x438KacPcrtDvYruEcd2oDK4blUFVQzMfbCli/qZCXlyRx7Of7SWjexRThqZx9dCenNMzTkEvJ9WWlvtg4C3gAqAeWAyswWm5v4/TVRMCXGitzTvO82cBswCysrJG5eV97SHSmTTXO/PXfPI7Z3RNzvVOSz6hj9uVua6qoZlFm4uYtzGfT3eW0OKx9E6K4eqhaVwzIp0+yRpe2Vn5ss/9HuA7QC2wGWjECfSl1tq5xpiZwCxr7fiTvY66ZeQL9RXOSdcVfwNPszOlwTcegtgUtysLCOW1TSzYXMg7G/NZvrsUj4URWfFcNzKDqUPTtLxgJ+OX0TLGmF8CB4FfAfHWWmuc742V1tq4kz1X4S5fU1UASx+Ddc87C3tfcL8zSVnkST9KnUpRVQNvfn6IuesOsqOohvBQwzcGJDNteDrjB6cQHaGlBYOdL1vuKdbaYmNMFrAQGAssB/7FWvuRMeYK4DfW2pNenqhwlxMq2QVLfgGb34DoROdCqPPuCbpZKNvCWsvm/Cre3pDP2+vzKaxqIDoilKvOSWXGyHQu7JtEqEbcBCVfhvsnQCLQDHzfWrvYGHMx8Ceck7UNOEMh157sdRTuckqH1sHi/3QmKOuWBVf8xJlqWCcVv8LjsazaV8Zb6w/xzsYCqhtaSOsWyTUj0rluZIamPwgyuohJgsfuJfDBT6FgA2ScBxN/DRnH/Wx3eg3NrXywtYi5aw/y8c4SWj2WYRnduHZEOlcP60lirL79dHQKdwkuHo9zIdTi/3TWeT13pjN8sluG25UFrOLqBt5en8/r6w6xpaCKsBDD5YNSuGlMJpf0TyZMK0x1SAp3CU6N1fDpH52phk2IM8XwRf8KETFuVxbQthdW8/q6g8xdd5CSmiZS4yK5flQGM0dnanriDkbhLsGtPA8++Blsfh269oTxP4OhM9UffwrNrR4Wby3m1dX7WbrjMB4LF/VL5MbzspgwpAeR4aFulyinoHCXziFvOSx4GArWO9MZTH4c0oa6XVWHkF9Rzz/XHuTV1Qc4VFFPfHQ404f15PpRmeSk62rYQKVwl87D44H1LzknXevLYfQ9zsRk0QluV9YheDyWz3aX8OrqAyzcUkRTi4cBPWK5flQG14xI16LgAUbhLp1PfTks+RWsfhqiusMVP4URt0GIThyersr6Zt7ZmM/ctQdZt7+CsBDDFYNTuHlMFuP6J2vsfABQuEvnVbgJ5v8I9i935pCf/FvICP4l/9rb7sM1vLb6AP9Ye5Cy2ibS46O46bxMZp6XSY84tebdonCXzs1a2PQPWPj/oKYYRt7mtORjktyurMNpavGwcEshL6/az2e7SgkNMYwfnMI3z+/FuH5JmnvezxTuIgANVc58NSufdIZLXv4fzsRkoZqD5WzsLanllVX7v2jNZ3SP4uYxWdwwOkN9836icBc5WvE2eO8h2LsUUofCtD9DzxFuV9VhNba0snBzEXNW7mf5nlLCQgwTc1K566JsLRnoYwp3kWNZC1vehPf+3Vm4+4L74dJHIEIX8bTFnsM1zFm5n1fXHKC6oYVz07tx54XZTB2WRpcwjZtvbwp3kROpr4BFP4F1s6F7Nlz9J+hzqctFdXy1jS288fkhnlu2j13FNSTFRnDTeVl88/wsesZHuV1e0FC4i5zK3k9g3gNQthuG3wITfqGx8e3AWsunu0qYvWwfi7cVE2IMVw7uwe0X9uKCPonqsmkjhbvI6Wiuh48fd1aCiuoOkx6Dc2ZoGoN2cqCsjhdX5vHq6gNU1DXTPyWW2y7oxYyRGcR20Unts6FwFzkThZvg7e9B/ucwYCJM+Z1mnGxHDc2tzNuQz/PL89h0qJLYLmHMGJnO7Rf0ol9KV7fL61AU7iJnytPqDJn88BfOjJPjf+ZMZaArXNuNtZb1Byp4YXke72wsoKnVw7j+SdxzcW8u6Z+sMfOnQeEucrbK98E7/wa7P4SMMc6wyZTBblcVdEpqGnll1X6eX55HcXUjfZNjuOui3swYma61YE9C4S7SFtbCxtecGScbq52JyC78V1385ANNLR7mbyrgmU/3sulQJd2iwrl5TBZ3XNiLtG4aZXMshbtIe6gtgXe/D1vegvTRcO2TkNTf7aqCkrWWtXnlPPvZXhbkFmKMYfK5adx9UTbDM+M1ysZL4S7SXqyF3Lkw/4fO6JrxP4Mx31ZfvA8dKKvj+eX7eGW1c2FU3+QYJp+bxqScNAande3UQa9wF2lv1YXOuPgdCyB7HFzzV4jPcruqoFbT2MKbnx9i/qYCVuwpxWMhOzGaq3JSuWxgCqN6dSe8k60Fq3AX8QVrnYVB3nvYGQs/6Tcw7CaNi/eD0ppGFm4pYv6mApbvLqXFY4ntEsZF/RKZmJPKFYN7EBcZ7naZPqdwF/Gl8jx44z7YvwwGXw1T/wQxiW5X1WlUNzSzfHcpS3ccZvHWYgqrGggPNVzUL4mrzkll/OAeJHft4naZPqFwF/E1Tyssf8IZFx8ZD9OfgAFXuV1Vp+PxWNYfrOC9TQW8l1vIwfJ6jIFRWd2ZcE4PJgxJJTspxu0y243CXcRfCnPhjW9DUa4zV/xV/+3MHS9+Z61lW2E1CzcXsXBLIZvzqwAY0COWacN6Mm1YOlmJHXsWUIW7iD+1NDot+GV/gcS+cN3/ab74AHCgrI4Ptjr99Kv3lQMwKLUrE85JZcKQHpzTM67DjbxRuIu4Ye/HTl98TRFc9ihc9ACEaE7zQHCgrI73NxeycEsRa/aV4bGQlRDNxJxUrjqnB8Mzu3eIBcB9Fu7GmAeAewEDPG2t/aN3+/eA+4FW4F1r7UMnex2FuwSt+nKY96CzMEivi50Ln+Iz3a5KjlJa08gHW4t4L7eQz3aV0NxqSYqNYGJOKtcMT2dkVveAnefGJ+FujMkBXgHGAE3AAuA+IBN4FJhirW00xqRYa4tP9loKdwlq1sKGl2H+j8CEwtV/gJzr3K5KjqOyvpmlOw7z/uZCFm8toqHZQ3p8FFOHpjHp3DSGZXQLqK4bX4X7DcBEa+093t//A2gERgNPWWs/ON3XUrhLp1C2B17/NhxcBUNvgsmPQ2Sc21XJCdQ0trBwcyFvrc/ns10ltHgs6fFRTMpJZdK5aYzIjHe9Re+rcB8MvAVcANQDi4E1wDjv9olAA/BDa+3qk72Wwl06jdYW+OS3sPQ30C0dZjwNWWPdrkpOobKumUVbi3hvUwGf7CyhqdVDalwkk89NY9rwnq616H3Z534P8B2gFtiM03IfDywB/hU4D3gV6GOP2ZExZhYwCyArK2tUXl7eWdch0uEcWAWv3wsVB2D8T51ZJgPo676cWFVDM4u3FvHuxkI+3nGYplYPmQlRTD43jQlDUv3aovfLaBljzC+Bg8A04DFr7RLv9t3AWGvt4RM9Vy136ZQaquDt7zqzTA6c4sxPExXvdlVyBirrm1m4uZC3N+R/MQ1CUmwXpg/vyZ0XZpOZ4Ntx9L5suadYa4uNMVnAQmAscBPQ01r7E2PMAJzumqxjW+5HU7hLp2UtrPxfWPios5TfDbOh53C3q5KzUFnfzEfbi50hlpuL8FjLmN4JTBnak4nnpPpkCgRfhvsnQCLQDHzfWrvYGBMBPAsMxxlF80Nr7Ycnex2Fu3R6B1bBP+505oyf/BsYeYe6aTqwgsp6Xl51gHc35rP7cC0hBi7om8jVQ3syMSeV+OiIdtmPLmIS6QhqS+H1bzlL+g27Gab8HiI69uXxnZ21lu1F1by7sYB5G/LZV1pHeKjhkv7JXD2sJ1cO6UFMl7Nf0UvhLtJReFrh49/CR79y1mqd+bxWewoS1lpyD1Uxb2M+8zbkU1DZQGR4CD+fnsPM0Wd3YZvCXaSj2f0hzP2WM0/NtL9Azgy3K5J25PFY1u4vZ96GfK4flcHQjPizeh2Fu0hHVHnI6Yc/uArOvw+u/C8Ia5++WgkOJwv3zrUmlUhH0i0d7poPY++HlU/C3yc54+JFToPCXSSQhYbDxF86fe+Ht8P/XgK7TntmD+nEFO4iHcGQ6TDrI4jrCS9eD0t+6Zx8FTkBhbtIR5HUD+5ZBMO/CUsfgxdnOOPiRY5D4S7SkUREO9MUTHsC9q+AJ8c5tyLHULiLdEQjb3Na8WFd4LkpsPx/nKkMRLwU7iIdVdpQ+PZSGDAR3n/EGRffXO92VRIgFO4iHVlkN7jxRbjiJ5A71xkuWZXvdlUSABTuIh2dMTDuB3DTHCjZCU9dBgfXul2VuEzhLhIsBk329sNHOC34ja+5XZG4SOEuEkx6DIF7P4KM85yVnhb9VOPhOymFu0iwiUmE296AUXfBZ3+EV77prPoknYrCXSQYhUXA1X+Eyb+FnYvgmSuhbI/bVYkfKdxFgtmYe51WfHUhPH057FnqdkXiJwp3kWDX5xswawnEpMAL18Kqp92uSPxA4S7SGST0gW99AP3Gw/wfwjv/Bq3NblclPqRwF+ksIuPg5pfhogdgzbPw/DXOuq0SlBTuIp1JSChc+XO49ik4uBqevsyZJ16CjsJdpDMadqOzylNznTOSZu/Hblck7UzhLtJZZYyGby2GrmnOidbPX3K7ImlHCneRzqx7L7j7feh1Ebz1HfjwF5o6OEgo3EU6u6h4uHUujLgVPn7cmbagucHtqqSNwtwuQEQCQGi4s7pTQh9Y/HOoPAg3vuRMZSAdklruIuI4MnXw9c/CoXXwzHgo3e12VXKWFO4i8lU518Edb0N9BfzfFZC3zO2K5Cy0KdyNMQ8YY3KNMZuNMQ8e87cfGGOsMSapTRWKiP9ljXWuaI1OhOenQ+7rblckZ+isw90YkwPcC4wBhgFTjTH9vH/LBCYA+9ujSBFxQWJfZ/GP9FHwz7thxd/crkjOQFta7oOBldbaOmttC7AUmOH92x+AhwCNqRLpyKITnFklB02BBQ/Dop+Ax+N2VXIa2hLuucA4Y0yiMSYamAxkGmOmA4estRtO9mRjzCxjzBpjzJrDhw+3oQwR8anwKJj5PIy+Gz77E7x5H7Q0uV2VnMJZD4W01m41xjwGLARqgfVAF+ARnC6ZUz3/KeApgNGjR6uFLxLIQkJhyu+ha09Y8guoPewEfpeublcmJ9CmE6rW2mestaOstZcA5cBmoDewwRizD8gA1hljUttcqYi4yxj4xo9g2l+cRT+emwo1xW5XJSfQ1tEyKd7bLJz+9tnW2hRrbba1Nhs4CIy01ha2uVIRCQwjb3emDj683Zl0TGPhA1Jbx7nPNcZsAeYB91trK9pekogEvAFXwR3znIW3n5ngXPQkAaWt3TLjrLVDrLXDrLWLj/P3bGttSVv2ISIBKvM8uGchREQ7XTS7vhYB4iJdoSoiZy+pvzMWPqEPzLkRNr/hdkXipXAXkbbpmgp3vuNc7PSPu2DN392uSFC4i0h7iIp3LnbqNx7eeRA++b3mhXeZwl1E2kdEtDOK5twbYPF/wqL/UMC7SPO5i0j7CQ13Ft+OjIdlf4H6cpj6JwhV1PibjriItK+QEJj8uDMvzdLHoKESZvwfhEe6XVmnom4ZEWl/xsBlj8DEX8PWeTBnJjTWuF1Vp6JwFxHfGfsvcM2TsO8TeOl656In8QuFu4j41vCbnaX7Dq6GF65x+uHF5xTuIuJ751zrzCJZsBFmT4O6MrcrCnoKdxHxj0FTvpxwTDNK+pzCXUT8p/+VcMtrULYHnpsCVQVuVxS0FO4i4l99LoVb50JVPvx9ElQccLuioKRwFxH/y74IbnvT6Xt/bjKU73O7oqCjcBcRd2SeB3e85QyP/PtkLfrRzhTuIuKeniOcGSVbGpwumuJtblcUNBTuIuKu1HPhzvnO/eemQGGuu/UECYW7iLgvZZAT8KERMHsq5H/udkUdnsJdRAJDUj+4az5EdIXZ0+HAarcr6tAU7iISOBJ6OwEfneBMVZC3zO2KOiyFu4gElvhMuOs9iOsJL14Hez5yu6IOSeEuIoEnLg3ufBe6Z8NLM2HnIrcr6nAU7iISmGJT4I53IHkAvPJN2Dbf7Yo6FIW7iASumES4Y54zXPK122H7Arcr6jAU7iIS2KK6w62vQ49z4LXbYNdityvqEBTuIhL4ouLhtjcgydtFs/cTtysKeAp3EekYohPg9reck6xzboT9K9yuKKAp3EWk44hJcgK+ayq8eD0cXOt2RQGrTeFujHnAGJNrjNlsjHnQu+1xY8w2Y8xGY8wbxpj49ihURARwgv2OeU5L/sVrIX+92xUFpLMOd2NMDnAvMAYYBkw1xvQDFgE51tqhwA7gx+1RqIjIF7qlOwHfJQ5euBaKNrtdUcBpS8t9MLDSWltnrW0BlgIzrLULvb8DrAAy2lqkiMjXdO/ldNGEdYHnp8PhHW5XFFDaEu65wDhjTKIxJhqYDGQe85i7gfeO92RjzCxjzBpjzJrDhw+3oQwR6bQS+8LtbwMGZl+tBT+Octbhbq3dCjwGLAQWAOuB1iN/N8Y8CrQAL53g+U9Za0dba0cnJyefbRki0tklD3Ba8K1NMHsalOe5XVFAaNMJVWvtM9baUdbaS4BynD52jDF3AlOBW6y1ts1VioicTI8hcPub0FTttOArD7pdkevaOlomxXubBcwA5hhjJgIPAdOstXVtL1FE5DSkDXMudKorc1rw1YVuV+Sqto5zn2uM2QLMA+631lYATwBdgUXGmPXGmCfbuA8RkdOTPgpunesE++xpUNN5z+eFteXJ1tpxx9nWry2vKSLSJlnnwy2vORc5PT/dWYA7OsHtqvxOV6iKSPDJvhhungOlu5wVneor3K7I7xTuIhKc+l4ON74ARVucFZ0aqtyuyK8U7iISvAZcBTf8HfI/hzkzoanW7Yr8RuEuIsFt8NVw3dNwYKUzXXBLo9sV+YXCXUSCX851MO0JZ7HtN74NntZTPqWja9NoGRGRDmPELVBXAot+AtFJMPlxMMbtqnxG4S4incdFD0BNMSx/wlmA+xsPuV2RzyjcRaRzufK/oLYElvy3s/jH6LvdrsgnFO4i0rmEhMD0J6C+DN79gdNFM2Sa21W1O51QFZHOJzQcbnjOma5g7j1BueC2wl1EOqeIGPjma9C9N7x8MxRscLuidqVwF5HOKzoBbnsdIrs5c9GU7XG7onajcBeRzq1bhhPwnmZnPdbqIrcrahcKdxGR5IFwyz+dYZIvXQcNlW5X1GYKdxERgIzRMPMFKN4Kr9wCzQ1uV9QmCncRkSP6j4dr/gb7PoHXv9WhpylQuIuIHG3oTLjqV7B1njMOvoMuA62LmEREjnXBd6C2GD79gzNNwWWPuF3RGVO4i4gczxU/hdrDsPQx5yrW82e5XdEZUbiLiByPMTD1T1BXBu895MxDkzPD7apOm/rcRUROJDQMrn8WssbC67Ng9xK3KzptCncRkZMJj4KbX4akAfDqrXBondsVnRaFu4jIqUR1h1vnOtMVvHQDlO52u6JTUriLiJyOuDS49Q3AwgvXQFWB2xWdlMJdROR0JfVzpimoK4MXr4P6CrcrOiGFu4jImUgfCTe+CCU74OWboLne7YqOS+EuInKm+l4GM56C/Svgn3dDa4vbFX1Nm8LdGPOAMSbXGLPZGPOgd1uCMWaRMWan97Z7u1QqIhJIcmbA5Mdh+3x454GAm6bgrMPdGJMD3AuMAYYBU40x/YCHgcXW2v7AYu/vIiLBZ8y9cMlD8PmL8OEv3K7mK9rSch8MrLTW1llrW4ClwAxgOjDb+5jZwDVtqlBEJJBd9giMvAM++S2se8Htar7QlnDPBcYZYxKNMdHAZCAT6GGtPTJGqBDocbwnG2NmGWPWGGPWHD58uA1liIi4yBiY8jvoezm882DAXMV61uFurd0KPAYsBBYA64HWYx5jgeN2RFlrn7LWjrbWjk5OTj7bMkRE3BcaDjfMhqSB8NrtULTF7YradkLVWvuMtXaUtfYSoBzYARQZY9IAvLfFbS9TRCTARcbBLa9BeDTMmQnVha6W09bRMine2yyc/vY5wNvAHd6H3AG81ZZ9iIh0GN0y4JuvOhc5zbkRmmpdK6Wt49znGmO2APOA+621FcCvgSuNMTuB8d7fRUQ6h57DnZkkCzfCXPeW6mvTfO7W2nHH2VYKXNGW1xUR6dAGToSJj8F7P4L3H4FJj/m9BC3WISLiC+fPgvK9sOKv0L03jL3Pr7tXuIuI+MqEX0B5Hix4GLr3goGT/LZrzS0jIuIrIaFw3dNOP/w/74b89f7btd/2JCLSGUXEwM2vQFSCM4tk5SG/7FbhLiLia11TnTHwjTXOEMnGap/vUuEuIuIPPc6Bmc9B8Ra/TBOscBcR8Zd+451pgncudE6y+nCaYI2WERHxp/PugbI9sPwJSOwLY//FJ7tRuIuI+NuVP4fyfbDgxxDfCwZNbvddqFtGRMTfQkKdZfr6T4DYFJ/sQi13ERE3RMQ4I2h8RC13EZEgpHAXEQlCCncRkSCkcBcRCUIKdxGRIKRwFxEJQgp3EZEgpHAXEQlCxvpw4prTLsKYw0DeWT49CShpx3LaU6DWprrOjOo6c4FaW7DV1ctam3y8PwREuLeFMWaNtXa023UcT6DWprrOjOo6c4FaW2eqS90yIiJBSOEuIhKEgiHcn3K7gJMI1NpU15lRXWcuUGvrNHV1+D53ERH5umBouYuIyDEU7iIiQahDh7sxZqIxZrsxZpcx5mEX68g0xiwxxmwxxmw2xjzg3f4zY8whY8x670/7r6V16tr2GWM2efe/xrstwRizyBiz03vb3c81DTzqmKw3xlQZYx5063gZY541xhQbY3KP2nbcY2Qcf/Z+5jYaY0b6ua7HjTHbvPt+wxgT792ebYypP+rYPennuk743hljfuw9XtuNMVf5qq6T1PbqUXXtM8as9273yzE7ST749jNmre2QP0AosBvoA0QAG4AhLtWSBoz03u8K7ACGAD8DfujycdoHJB2z7TfAw977DwOPufw+FgK93DpewCXASCD3VMcImAy8BxhgLLDSz3VNAMK89x87qq7sox/nwvE67nvn/XewAegC9Pb+mw31Z23H/P13wE/8ecxOkg8+/Yx15Jb7GGCXtXaPtbYJeAWY7kYh1toCa+067/1qYCuQ7kYtp2k6MNt7fzZwjXulcAWw21p7tlcot5m19mOg7JjNJzpG04HnrWMFEG+MSfNXXdbahdbaFu+vK4AMX+z7TOs6ienAK9baRmvtXmAXzr9dv9dmjDHATOBlX+3/BDWdKB98+hnryOGeDhw46veDBECgGmOygRHASu+m73q/Wj3r7+4PLwssNMasNcbM8m7rYa0t8N4vBHq4UNcRN/HVf2xuH68jTnSMAulzdzdOC++I3saYz40xS40x41yo53jvXSAdr3FAkbV251Hb/HrMjskHn37GOnK4BxxjTCwwF3jQWlsF/A3oCwwHCnC+EvrbxdbakcAk4H5jzCVH/9E63wNdGQ9rjIkApgH/8G4KhOP1NW4eoxMxxjwKtAAveTcVAFnW2hHA94E5xpg4P5YUkO/dMW7mqw0Jvx6z4+TDF3zxGevI4X4IyDzq9wzvNlcYY8Jx3riXrLWvA1hri6y1rdZaD/A0Pvw6eiLW2kPe22LgDW8NRUe+5nlvi/1dl9ckYJ21tshbo+vH6ygnOkauf+6MMXcCU4FbvKGAt9uj1Ht/LU7f9gB/1XSS98714wVgjAkDZgCvHtnmz2N2vHzAx5+xjhzuq4H+xpje3hbgTcDbbhTi7ct7Bthqrf39UduP7ie7Fsg99rk+rivGGNP1yH2ck3G5OMfpDu/D7gDe8mddR/lKS8rt43WMEx2jt4HbvSMaxgKVR3219jljzETgIWCatbbuqO3JxphQ7/0+QH9gjx/rOtF79zZwkzGmizGmt7euVf6q6yjjgW3W2oNHNvjrmJ0oH/D1Z8zXZ4p9+YNzVnkHzv9xH3WxjotxvlJtBNZ7fyYDLwCbvNvfBtL8XFcfnJEKG4DNR44RkAgsBnYCHwAJLhyzGKAU6HbUNleOF87/YAqAZpz+zXtOdIxwRjD8j/cztwkY7ee6duH0xx75nD3pfex13vd4PbAOuNrPdZ3wvQMe9R6v7cAkf7+X3u3PAfcd81i/HLOT5INPP2OafkBEJAh15G4ZERE5AYW7iEgQUriLiAQhhbuISBBSuIuIBCGFu4hIEFK4i4gEof8PU/okz47iNEMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_6, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    718.29187\n",
       "dtype: float32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_6.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d758d",
   "metadata": {},
   "source": [
    "### Train model 7, GRU X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 7th model layers architecture\n",
    "> GRU X 2\n",
    "\"\"\"\n",
    "rnn_model_7 = Sequential()\n",
    "rnn_model_7.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_7.add(GRU(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "rnn_model_7.add(GRU(units=20, activation='tanh', return_sequences=True))\n",
    "rnn_model_7.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_7.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d27674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "from typing import overload\n",
    "\n",
    "def train_rnn_model(rnn_model_7, patience=2, epochs=200):\n",
    "    es = EarlyStopping(monitor = 'val_loss',\n",
    "                    patience = patience,\n",
    "                    verbose = 0,\n",
    "                    restore_best_weights = True)\n",
    "    # The fit\n",
    "    history =  rnn_model_7.fit(X_train, y_train, \n",
    "            validation_split=0.1, # Auto split for validation data\n",
    "            batch_size = 16,\n",
    "            epochs = epochs,\n",
    "            callbacks = [es],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, None, 30)          4680      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, None, 20)          3120      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, None, 1)           11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,062\n",
      "Trainable params: 8,021\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_7.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 6s 192ms/step - loss: 6508340736.0000 - mape: 99.9999 - val_loss: 6164800000.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6508286464.0000 - mape: 99.9993 - val_loss: 6164745728.0000 - val_mape: 99.9988\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6508220928.0000 - mape: 99.9987 - val_loss: 6164642816.0000 - val_mape: 99.9978\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6508121600.0000 - mape: 99.9980 - val_loss: 6164547072.0000 - val_mape: 99.9965\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6508022784.0000 - mape: 99.9972 - val_loss: 6164463616.0000 - val_mape: 99.9954\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6507936768.0000 - mape: 99.9965 - val_loss: 6164393984.0000 - val_mape: 99.9944\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6507857920.0000 - mape: 99.9956 - val_loss: 6164306944.0000 - val_mape: 99.9925\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6507777024.0000 - mape: 99.9945 - val_loss: 6164233728.0000 - val_mape: 99.9910\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507706368.0000 - mape: 99.9937 - val_loss: 6164163584.0000 - val_mape: 99.9897\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6507638272.0000 - mape: 99.9929 - val_loss: 6164096000.0000 - val_mape: 99.9884\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6507570176.0000 - mape: 99.9922 - val_loss: 6164026880.0000 - val_mape: 99.9870\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507495424.0000 - mape: 99.9912 - val_loss: 6163943424.0000 - val_mape: 99.9850\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507414016.0000 - mape: 99.9899 - val_loss: 6163870720.0000 - val_mape: 99.9835\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507338752.0000 - mape: 99.9889 - val_loss: 6163795968.0000 - val_mape: 99.9821\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507260928.0000 - mape: 99.9880 - val_loss: 6163718144.0000 - val_mape: 99.9805\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6507180032.0000 - mape: 99.9870 - val_loss: 6163637248.0000 - val_mape: 99.9790\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6507097088.0000 - mape: 99.9861 - val_loss: 6163557376.0000 - val_mape: 99.9774\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6507014144.0000 - mape: 99.9851 - val_loss: 6163469312.0000 - val_mape: 99.9758\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6506925056.0000 - mape: 99.9840 - val_loss: 6163384832.0000 - val_mape: 99.9740\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6506835456.0000 - mape: 99.9829 - val_loss: 6163297280.0000 - val_mape: 99.9722\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6506741760.0000 - mape: 99.9801 - val_loss: 6163199488.0000 - val_mape: 99.9594\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 6506641920.0000 - mape: 99.9777 - val_loss: 6163105280.0000 - val_mape: 99.9525\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6506505728.0000 - mape: 99.9734 - val_loss: 6162980864.0000 - val_mape: 99.9473\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6506353664.0000 - mape: 99.9689 - val_loss: 6162877952.0000 - val_mape: 99.9446\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6506243584.0000 - mape: 99.9672 - val_loss: 6162772992.0000 - val_mape: 99.9415\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6506132992.0000 - mape: 99.9655 - val_loss: 6162663936.0000 - val_mape: 99.9382\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6506018816.0000 - mape: 99.9635 - val_loss: 6162557440.0000 - val_mape: 99.9345\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 6505903104.0000 - mape: 99.9615 - val_loss: 6162443264.0000 - val_mape: 99.9306\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6505783296.0000 - mape: 99.9595 - val_loss: 6162327040.0000 - val_mape: 99.9267\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6505659904.0000 - mape: 99.9574 - val_loss: 6162213888.0000 - val_mape: 99.9225\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6505538048.0000 - mape: 99.9553 - val_loss: 6162095104.0000 - val_mape: 99.9183\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6505414144.0000 - mape: 99.9531 - val_loss: 6161979392.0000 - val_mape: 99.9137\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6505290752.0000 - mape: 99.9474 - val_loss: 6161855488.0000 - val_mape: 99.9095\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6505157120.0000 - mape: 99.9453 - val_loss: 6161723904.0000 - val_mape: 99.8729\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6505018880.0000 - mape: 99.9408 - val_loss: 6161593856.0000 - val_mape: 99.8648\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6504882688.0000 - mape: 99.9380 - val_loss: 6161462272.0000 - val_mape: 99.8589\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504742912.0000 - mape: 99.9357 - val_loss: 6161327616.0000 - val_mape: 99.8528\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6504601088.0000 - mape: 99.9332 - val_loss: 6161193472.0000 - val_mape: 99.8471\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6504458240.0000 - mape: 99.9305 - val_loss: 6161055744.0000 - val_mape: 99.8412\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6504311808.0000 - mape: 99.9277 - val_loss: 6160918016.0000 - val_mape: 99.8353\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6504164864.0000 - mape: 99.9251 - val_loss: 6160771072.0000 - val_mape: 99.8290\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504010240.0000 - mape: 99.9226 - val_loss: 6160622592.0000 - val_mape: 99.8227\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6503854592.0000 - mape: 99.9198 - val_loss: 6160474112.0000 - val_mape: 99.8165\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6503698944.0000 - mape: 99.9168 - val_loss: 6160325632.0000 - val_mape: 99.8102\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503539200.0000 - mape: 99.9143 - val_loss: 6160172032.0000 - val_mape: 99.8037\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6503376896.0000 - mape: 99.9113 - val_loss: 6160019456.0000 - val_mape: 99.7973\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503216128.0000 - mape: 99.9083 - val_loss: 6159863296.0000 - val_mape: 99.7908\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6503049216.0000 - mape: 99.9054 - val_loss: 6159702528.0000 - val_mape: 99.7840\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6502880256.0000 - mape: 99.9020 - val_loss: 6159540224.0000 - val_mape: 99.7772\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6502709248.0000 - mape: 99.8991 - val_loss: 6159379456.0000 - val_mape: 99.7704\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6502535168.0000 - mape: 99.8963 - val_loss: 6159205376.0000 - val_mape: 99.7631\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6502355456.0000 - mape: 99.8926 - val_loss: 6159033344.0000 - val_mape: 99.7559\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6502174720.0000 - mape: 99.8896 - val_loss: 6158865408.0000 - val_mape: 99.7488\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6501993472.0000 - mape: 99.8865 - val_loss: 6158688768.0000 - val_mape: 99.7414\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6501807104.0000 - mape: 99.8833 - val_loss: 6158511104.0000 - val_mape: 99.7340\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6501619200.0000 - mape: 99.8799 - val_loss: 6158329344.0000 - val_mape: 99.7263\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6501428224.0000 - mape: 99.8762 - val_loss: 6158150656.0000 - val_mape: 99.7188\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6501238272.0000 - mape: 99.8730 - val_loss: 6157965824.0000 - val_mape: 99.7110\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6501043200.0000 - mape: 99.8695 - val_loss: 6157777920.0000 - val_mape: 99.7032\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500845056.0000 - mape: 99.8660 - val_loss: 6157587456.0000 - val_mape: 99.6951\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6500642304.0000 - mape: 99.8620 - val_loss: 6157396992.0000 - val_mape: 99.6872\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6500440064.0000 - mape: 99.8587 - val_loss: 6157194240.0000 - val_mape: 99.6787\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6500231168.0000 - mape: 99.8548 - val_loss: 6156998144.0000 - val_mape: 99.6704\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500022784.0000 - mape: 99.8511 - val_loss: 6156804096.0000 - val_mape: 99.6622\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6499815424.0000 - mape: 99.8474 - val_loss: 6156597248.0000 - val_mape: 99.6535\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6499598336.0000 - mape: 99.8438 - val_loss: 6156390400.0000 - val_mape: 99.6449\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6499384320.0000 - mape: 99.8394 - val_loss: 6156190208.0000 - val_mape: 99.6365\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6499167232.0000 - mape: 99.8360 - val_loss: 6155981824.0000 - val_mape: 99.6277\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6498948608.0000 - mape: 99.8315 - val_loss: 6155774976.0000 - val_mape: 99.6190\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6498726912.0000 - mape: 99.8281 - val_loss: 6155556352.0000 - val_mape: 99.6098\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6498498048.0000 - mape: 99.8242 - val_loss: 6155334656.0000 - val_mape: 99.6005\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6498265600.0000 - mape: 99.8196 - val_loss: 6155113472.0000 - val_mape: 99.5912\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6498034176.0000 - mape: 99.8155 - val_loss: 6154892288.0000 - val_mape: 99.5820\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6497800704.0000 - mape: 99.8113 - val_loss: 6154672128.0000 - val_mape: 99.5727\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6497565184.0000 - mape: 99.8069 - val_loss: 6154451456.0000 - val_mape: 99.5634\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6497328128.0000 - mape: 99.8029 - val_loss: 6154214400.0000 - val_mape: 99.5535\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6497083392.0000 - mape: 99.7986 - val_loss: 6153983488.0000 - val_mape: 99.5437\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 2s 205ms/step - loss: 6496836608.0000 - mape: 99.7947 - val_loss: 6153745408.0000 - val_mape: 99.5337\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496588800.0000 - mape: 99.7898 - val_loss: 6153518080.0000 - val_mape: 99.5242\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496348672.0000 - mape: 99.7856 - val_loss: 6153284096.0000 - val_mape: 99.5143\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6496097280.0000 - mape: 99.7812 - val_loss: 6153036288.0000 - val_mape: 99.5039\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6495840768.0000 - mape: 99.7761 - val_loss: 6152793088.0000 - val_mape: 99.4938\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6495583232.0000 - mape: 99.7715 - val_loss: 6152543744.0000 - val_mape: 99.4833\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6495321088.0000 - mape: 99.7672 - val_loss: 6152305664.0000 - val_mape: 99.4733\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6495067136.0000 - mape: 99.7626 - val_loss: 6152055296.0000 - val_mape: 99.4627\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6494800384.0000 - mape: 99.7582 - val_loss: 6151792128.0000 - val_mape: 99.4517\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6494531072.0000 - mape: 99.7531 - val_loss: 6151544832.0000 - val_mape: 99.4413\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6494263808.0000 - mape: 99.7482 - val_loss: 6151287808.0000 - val_mape: 99.4305\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6493992960.0000 - mape: 99.7432 - val_loss: 6151025152.0000 - val_mape: 99.4194\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6493714432.0000 - mape: 99.7383 - val_loss: 6150759936.0000 - val_mape: 99.4083\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6493437952.0000 - mape: 99.7332 - val_loss: 6150505472.0000 - val_mape: 99.3976\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6493167104.0000 - mape: 99.7287 - val_loss: 6150234112.0000 - val_mape: 99.3862\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6492880384.0000 - mape: 99.7234 - val_loss: 6149958656.0000 - val_mape: 99.3746\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6492593152.0000 - mape: 99.7181 - val_loss: 6149687296.0000 - val_mape: 99.3632\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6492306944.0000 - mape: 99.7134 - val_loss: 6149424128.0000 - val_mape: 99.3521\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6492025344.0000 - mape: 99.7084 - val_loss: 6149138944.0000 - val_mape: 99.3401\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6491725312.0000 - mape: 99.7026 - val_loss: 6148849664.0000 - val_mape: 99.3280\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6491425280.0000 - mape: 99.6973 - val_loss: 6148572160.0000 - val_mape: 99.3163\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6491128320.0000 - mape: 99.6926 - val_loss: 6148282368.0000 - val_mape: 99.3041\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6490828800.0000 - mape: 99.6863 - val_loss: 6148003328.0000 - val_mape: 99.2924\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6490530304.0000 - mape: 99.6818 - val_loss: 6147706880.0000 - val_mape: 99.2799\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6490221056.0000 - mape: 99.6758 - val_loss: 6147425792.0000 - val_mape: 99.2681\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489919488.0000 - mape: 99.6707 - val_loss: 6147137024.0000 - val_mape: 99.2560\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6489612288.0000 - mape: 99.6651 - val_loss: 6146836992.0000 - val_mape: 99.2433\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489298432.0000 - mape: 99.6597 - val_loss: 6146542592.0000 - val_mape: 99.2310\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6488986112.0000 - mape: 99.6542 - val_loss: 6146246144.0000 - val_mape: 99.2185\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6488670720.0000 - mape: 99.6485 - val_loss: 6145934336.0000 - val_mape: 99.2054\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6488348160.0000 - mape: 99.6426 - val_loss: 6145631744.0000 - val_mape: 99.1926\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6488026112.0000 - mape: 99.6363 - val_loss: 6145320448.0000 - val_mape: 99.1796\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6487696896.0000 - mape: 99.6308 - val_loss: 6145006592.0000 - val_mape: 99.1663\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6487367680.0000 - mape: 99.6247 - val_loss: 6144690688.0000 - val_mape: 99.1531\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 3s 224ms/step - loss: 6487036416.0000 - mape: 99.6187 - val_loss: 6144380928.0000 - val_mape: 99.1400\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 207ms/step - loss: 6486709760.0000 - mape: 99.6124 - val_loss: 6144065536.0000 - val_mape: 99.1268\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6486368256.0000 - mape: 99.6075 - val_loss: 6143734784.0000 - val_mape: 99.1129\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6486023680.0000 - mape: 99.6012 - val_loss: 6143398912.0000 - val_mape: 99.0988\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6485678592.0000 - mape: 99.5951 - val_loss: 6143091200.0000 - val_mape: 99.0858\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6485347328.0000 - mape: 99.5890 - val_loss: 6142763008.0000 - val_mape: 99.0720\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6484999680.0000 - mape: 99.5825 - val_loss: 6142427136.0000 - val_mape: 99.0579\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6484645376.0000 - mape: 99.5769 - val_loss: 6142093312.0000 - val_mape: 99.0438\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6484298752.0000 - mape: 99.5694 - val_loss: 6141759488.0000 - val_mape: 99.0298\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6483947008.0000 - mape: 99.5636 - val_loss: 6141423104.0000 - val_mape: 99.0156\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6483587072.0000 - mape: 99.5571 - val_loss: 6141078016.0000 - val_mape: 99.0011\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6483226112.0000 - mape: 99.5513 - val_loss: 6140733440.0000 - val_mape: 98.9866\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6482865664.0000 - mape: 99.5443 - val_loss: 6140393472.0000 - val_mape: 98.9723\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6482504192.0000 - mape: 99.5381 - val_loss: 6140046336.0000 - val_mape: 98.9577\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6482135040.0000 - mape: 99.5315 - val_loss: 6139686400.0000 - val_mape: 98.9425\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6481763840.0000 - mape: 99.5241 - val_loss: 6139338752.0000 - val_mape: 98.9279\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6481391616.0000 - mape: 99.5186 - val_loss: 6138976768.0000 - val_mape: 98.9127\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6481013760.0000 - mape: 99.5111 - val_loss: 6138623488.0000 - val_mape: 98.8978\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6480641024.0000 - mape: 99.5044 - val_loss: 6138269696.0000 - val_mape: 98.8829\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6480264704.0000 - mape: 99.4981 - val_loss: 6137910784.0000 - val_mape: 98.8678\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6479885824.0000 - mape: 99.4907 - val_loss: 6137543168.0000 - val_mape: 98.8523\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6479494656.0000 - mape: 99.4842 - val_loss: 6137167872.0000 - val_mape: 98.8365\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6479104512.0000 - mape: 99.4775 - val_loss: 6136788992.0000 - val_mape: 98.8206\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6478711808.0000 - mape: 99.4695 - val_loss: 6136430592.0000 - val_mape: 98.8055\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6478324736.0000 - mape: 99.4633 - val_loss: 6136050688.0000 - val_mape: 98.7895\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6477929472.0000 - mape: 99.4555 - val_loss: 6135675392.0000 - val_mape: 98.7737\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477530112.0000 - mape: 99.4483 - val_loss: 6135296000.0000 - val_mape: 98.7577\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477130240.0000 - mape: 99.4417 - val_loss: 6134906368.0000 - val_mape: 98.7413\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6476723200.0000 - mape: 99.4340 - val_loss: 6134532096.0000 - val_mape: 98.7256\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6476329472.0000 - mape: 99.4268 - val_loss: 6134149120.0000 - val_mape: 98.7094\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6475919360.0000 - mape: 99.4200 - val_loss: 6133744640.0000 - val_mape: 98.6924\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6475498496.0000 - mape: 99.4128 - val_loss: 6133349376.0000 - val_mape: 98.6758\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6475084288.0000 - mape: 99.4054 - val_loss: 6132956160.0000 - val_mape: 98.6592\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6474669056.0000 - mape: 99.3976 - val_loss: 6132567040.0000 - val_mape: 98.6428\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6474255360.0000 - mape: 99.3904 - val_loss: 6132171776.0000 - val_mape: 98.6262\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6473837568.0000 - mape: 99.3823 - val_loss: 6131761664.0000 - val_mape: 98.6089\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6473412096.0000 - mape: 99.3750 - val_loss: 6131362816.0000 - val_mape: 98.5921\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 6472987648.0000 - mape: 99.3669 - val_loss: 6130956288.0000 - val_mape: 98.5749\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6472557568.0000 - mape: 99.3603 - val_loss: 6130549248.0000 - val_mape: 98.5578\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 3s 242ms/step - loss: 6472132096.0000 - mape: 99.3515 - val_loss: 6130147328.0000 - val_mape: 98.5409\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6471698944.0000 - mape: 99.3448 - val_loss: 6129713152.0000 - val_mape: 98.5226\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6471254528.0000 - mape: 99.3361 - val_loss: 6129302016.0000 - val_mape: 98.5052\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 3s 223ms/step - loss: 6470817280.0000 - mape: 99.3281 - val_loss: 6128883200.0000 - val_mape: 98.4876\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6470372864.0000 - mape: 99.3205 - val_loss: 6128456704.0000 - val_mape: 98.4697\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6469926400.0000 - mape: 99.3127 - val_loss: 6128030720.0000 - val_mape: 98.4517\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 3s 253ms/step - loss: 6469479424.0000 - mape: 99.3053 - val_loss: 6127604736.0000 - val_mape: 98.4337\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 3s 257ms/step - loss: 6469031424.0000 - mape: 99.2967 - val_loss: 6127178240.0000 - val_mape: 98.4158\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 3s 208ms/step - loss: 6468581376.0000 - mape: 99.2887 - val_loss: 6126760448.0000 - val_mape: 98.3982\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6468137472.0000 - mape: 99.2809 - val_loss: 6126326784.0000 - val_mape: 98.3799\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6467680768.0000 - mape: 99.2721 - val_loss: 6125899776.0000 - val_mape: 98.3619\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6467228160.0000 - mape: 99.2648 - val_loss: 6125454848.0000 - val_mape: 98.3431\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6466763264.0000 - mape: 99.2553 - val_loss: 6125008384.0000 - val_mape: 98.3243\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6466293760.0000 - mape: 99.2467 - val_loss: 6124583936.0000 - val_mape: 98.3064\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6465838080.0000 - mape: 99.2402 - val_loss: 6124129280.0000 - val_mape: 98.2873\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 6465360896.0000 - mape: 99.2313 - val_loss: 6123678208.0000 - val_mape: 98.2682\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6464889856.0000 - mape: 99.2221 - val_loss: 6123215360.0000 - val_mape: 98.2487\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6464412672.0000 - mape: 99.2134 - val_loss: 6122796032.0000 - val_mape: 98.2311\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6463960576.0000 - mape: 99.2060 - val_loss: 6122342912.0000 - val_mape: 98.2119\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6463479808.0000 - mape: 99.1978 - val_loss: 6121879552.0000 - val_mape: 98.1924\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6462995968.0000 - mape: 99.1884 - val_loss: 6121416192.0000 - val_mape: 98.1729\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6462513152.0000 - mape: 99.1788 - val_loss: 6120978432.0000 - val_mape: 98.1544\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6462044672.0000 - mape: 99.1713 - val_loss: 6120514560.0000 - val_mape: 98.1348\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6461553152.0000 - mape: 99.1620 - val_loss: 6120049664.0000 - val_mape: 98.1152\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6461061120.0000 - mape: 99.1543 - val_loss: 6119570432.0000 - val_mape: 98.0951\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6460563456.0000 - mape: 99.1446 - val_loss: 6119093760.0000 - val_mape: 98.0749\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6460056576.0000 - mape: 99.1356 - val_loss: 6118606848.0000 - val_mape: 98.0544\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6459552768.0000 - mape: 99.1270 - val_loss: 6118130688.0000 - val_mape: 98.0343\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6459054080.0000 - mape: 99.1177 - val_loss: 6117667328.0000 - val_mape: 98.0147\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6458561024.0000 - mape: 99.1088 - val_loss: 6117209600.0000 - val_mape: 97.9954\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6458073088.0000 - mape: 99.1001 - val_loss: 6116715520.0000 - val_mape: 97.9746\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6457556480.0000 - mape: 99.0904 - val_loss: 6116232192.0000 - val_mape: 97.9542\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6457046528.0000 - mape: 99.0812 - val_loss: 6115746816.0000 - val_mape: 97.9337\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6456534528.0000 - mape: 99.0720 - val_loss: 6115257344.0000 - val_mape: 97.9131\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6456018944.0000 - mape: 99.0633 - val_loss: 6114757632.0000 - val_mape: 97.8920\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6455497216.0000 - mape: 99.0532 - val_loss: 6114275328.0000 - val_mape: 97.8716\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6454986752.0000 - mape: 99.0450 - val_loss: 6113780736.0000 - val_mape: 97.8508\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6454462976.0000 - mape: 99.0353 - val_loss: 6113285120.0000 - val_mape: 97.8299\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6453937152.0000 - mape: 99.0260 - val_loss: 6112766976.0000 - val_mape: 97.8080\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6453396992.0000 - mape: 99.0159 - val_loss: 6112254976.0000 - val_mape: 97.7864\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6452864512.0000 - mape: 99.0060 - val_loss: 6111764480.0000 - val_mape: 97.7657\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6452339712.0000 - mape: 98.9964 - val_loss: 6111243264.0000 - val_mape: 97.7437\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6451791360.0000 - mape: 98.9878 - val_loss: 6110729216.0000 - val_mape: 97.7220\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6451253248.0000 - mape: 98.9784 - val_loss: 6110217728.0000 - val_mape: 97.7004\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6450710016.0000 - mape: 98.9674 - val_loss: 6109691904.0000 - val_mape: 97.6782\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6450161664.0000 - mape: 98.9586 - val_loss: 6109177344.0000 - val_mape: 97.6565\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6449615360.0000 - mape: 98.9490 - val_loss: 6108666368.0000 - val_mape: 97.6349\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6449075712.0000 - mape: 98.9388 - val_loss: 6108140544.0000 - val_mape: 97.6127\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6448521216.0000 - mape: 98.9280 - val_loss: 6107608064.0000 - val_mape: 97.5902\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 3s 209ms/step - loss: 6447969792.0000 - mape: 98.9178 - val_loss: 6107094016.0000 - val_mape: 97.5685\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtVUlEQVR4nO3dd3xW5f3/8dcnm+xFQkgIISxZsiJDZSko4gb3wmqljrZSu7R+W2uHrevX2jparVtRUFBxg1TBBSXsKZsEyB5k7+v3x3WwkSaMrHMn9+f5eORx3zm5T87Hk9v7zbmuc12XGGNQSinlfXzcLkAppZQ7NACUUspLaQAopZSX0gBQSikvpQGglFJeys/tAk5GbGysSUlJcbsMpZTqNNasWZNvjOne1M86VQCkpKSQnp7udhlKKdVpiMj+5n6mTUBKKeWlNACUUspLaQAopZSX0gBQSikvpQGglFJe6rgBICLPiUiuiGxutC1aRJaKyE7nMcrZLiLyNxHZJSIbRWRUM79ztIhscl73NxGRtvtPUkopdSJO5ArgBWD6UdvuBpYZY/oDy5zvAc4D+jtfc4CnmvmdTwG3NHrt0b9fKaVUOzvuOABjzAoRSTlq88XAZOf5i8BnwC+d7S8ZO8f0ShGJFJEEY0zWkR1FJAEIN8asdL5/CbgE+LBV/yXHsPL5X9LgE0BDUCR0i0KCo/AJjsIvJBr/0GiCgsPoFuBHtwBfggP86Obvi6+PXpQopbq2lg4Ei2/0oZ4NxDvPE4HMRq874GzLarQt0dl+9GuaJCJzsFcTJCcnn3ylxjBs3/OESHWzL6kxvhwmlMMmhAxCKTYhlEoYFT6hVPiGU+kXTrl/DNWB0dQFxdIQ0h3fbpGEdvMnNNCPsCA/QgP9nOf+hAXZbfHhQQT5+558zUop1QFaPRLYGGNEpN1WlTHGPA08DZCWlnbyxxEh+L5sqirLqCopoLqsgLqyAurKiqivKMRUFEJVMVJZjH91MT1qDpNcc5iA2iyC6koIqq+AeqAaKPvvr63FlwITQb4JJ99EUEA4e0wEBSacAhNOPhGsN/2JielO/7hQBsSHMbBHGKN7R9EzslvbnByllGqFlgZAzpGmHadJJ9fZfhDo1eh1Sc62xg4624/1mjYlPj4EhYQTFBIO9Dm5netroaIQKvKhLBfK86E8F//yPOLL8uhelktDWR5Svgefinx86v97pVEW0J0Xw37CotzBLNueS32Dza/IYH/iw4KICw8kPjyIeOcxLsw+7xERRGxoIP6+epOWUqr9tDQAFgOzgT87j+802v5DEXkdGAscbtz+D+CERomIjANWATcAf29hHe3P1x/C4u1X/JDv/EgAX+cLAGOguhTK86BwL6FL/o87sn7FHTH9qZtyERmRY/mqrAc7SnzJPlxFTmk1u3LzyS2t/jYcjvARiAsLIiEyiNTYUCYP7M7gnuEkRnbTZiWlVJuQ460JLCKvYTt8Y4Ec4D7gbWABkAzsB64wxhQ6t3M+jr2rpwL4njEm3fk9640xI5znadi7i7phO39/ZE5gceK0tDTTqSaDq62CTQtgw3zI+ApMg90eGg/dB0L3UyB+CPUJoygM6UtOWR05JVXklFSTfbiSQ4eryDpcydZDJRRV1ALg5yMMSYxgcEIYvWNC6B0dbB9jggkJ7FRz+ymlOoCIrDHGpDX5s860KHynC4DGKoshcxXkbYe8b5zHHVBTan/u1w16joDE0ZA4yj5G9gYR6hsMGw4Usy+/nF25ZaTvL2J3bhkF5TXfOURsaCApMcEkxwST4oRCamwoKbHBhAX5d/h/slLKfRoAnsoYKNoLB9bAQecrawMc6UcIjoGkMZA8DpLHQ68x0GjMXElVLRkFFewvqGBfQTkZRx4LK8g6XPWdQ8WGBpIaG0JKbDB9YkPpExtCv7hQescEa1+DUl2YBkBnUl8LOVv+GwiZq6Bgl/3ZpF/ClF+d0K+pqq0no7CCPXnl7M0vZ1++fdxbUE5e6X87qv19hT6xIfSPC6N/fOi3jykxIQT4aTAo1dlpAHR2ZXnw3lzY/W+4cyOENrm4zwkrraplr9OctDO3jJ05pezMLSOjsIIjbwc/HyElNoT+caH0jwulX3wYA+LtlUOgn3ZCK9VZaAB0Bfk74YkxMPY2mP5AuxyiqraeXbllTjCUsjPHBsT+gnKO3KTk6yP06x7KkJ7hDHa+hiREEBGsfQxKeaJjBYDeNtJZxPaH4VfDyiegYCeMvA76TYWAkDY7RJC/L0MTIxiaGPGd7VW19ezNL2dHTinfZJeyLauEL3bls2jdf4dvJEV1Y3BCOEN6RnwbDgkRQeg8f0p5Lg2AzuS8ByEyGdKfg51L7J1D/c6GQRfBgHOhW2S7HDbI35dBCeEMSgj/zva80mq2ZpWw5dBhth4qYeuhEpZuy/m2GSkq2J8hPSPsVULPcEb0iiQ5OlhDQSkPoU1AnVF9HWR8DdvetV+lh8DHH1In2TA45XwIiXWltPLqOrZnl7DFCYQth0r4JruUmno7BiIy2J9TkyIZkRTBqUmRDO8VSfewQFdqVcobaB9AV9bQYO8W2rbYfhXtA/GB3mfYMBh0AYT3dLXE2voGduSUsvHAYTZkFrPhwGG+yS75tl8hMbIbpyZFMLxXJMOTIhmWFEGoDmpTqk1oAHgLYyBnM2xdbK8M8rbZ7UmnwaAL7Vd0qrs1Oipq6thyqOTbQNiQWUxGYQVghzr06x7K6N5RjOodxejeUaTGhmjTkVItoAHgrfJ32quCrYsha73d1nMUDLschs6EsB6ulne0wvIaNh4oZkPmYdZnFrE2o5jDlXYKjMhgf0Yn/zcQhidF0i1Ab0dV6ng0ABQUZ8DWd2DTmzYMxAdSJsCpV9grg6CI4/6KjtbQYNiTX87a/UWs2V/EmowiduXaObl9fYTBCeGMdgJhbJ9o4sKDXK5YKc+jAaC+K28HbH4TNi6wU1H4BsKAc+yVQf9zwd9zP0iLK2pYl1FsA2F/Eeszi6msrQcgJSaYMX2iGdMnhnGp0SRFBbtcrVLu0wBQTTMGDq6FTW/A5oVQnguB4bZ5aPSN0HOk2xUeV219A9uySvjP3kJW7ilk9b7Cb5uNkqK6MS41hvGpMYzrG0OiLsSjvJAGgDq++jrYt8JeFWx5G+oqIWE4jJptrwyCwo/7KzxBQ4Phm5xSVu0pYOWeQlbtLfh2Ku1e0d0Y1yeG8X1jGJcaoyuzKa+gAaBOTmWxvSpY84K9q8g/GIbOslcFiaO/MyOppzsSCCv3FLByTwGr9hZS7ARCcnQw41KjGZcawxn9YonXPgTVBWkAqJY50kS05nnbRFRbAfFDbRAMu7zdRh63p4YGw/bs7wbCkSajgfFhTBwQy4T+3RnTJ1pXXlNdggaAar2qEttxnP48ZG+001AMuRRGz4ZeYzvVVUFjDQ2GbdklfLEznxU781i9t4ia+gYC/XwYmxrDxP6xTBzQnf5xoToOQXVKGgCqbR1aZ5uHNr0JNWV2acvRN8KIazzydtKTUVlTz8q9BazYkceKHXnszisHICEiiAlOGJzRN5aokACXK1XqxGgAqPZRXWabhta8AIfWQkAojLgWxt3qMSOOW+tgcSWf78hjxc48vtiZT0lVHSJwalIkk5xAGNErEj9dVU15KA0A1f4OrYOVT9lAaKiHgTNg3G2QcmanbR462pG1mY9cHazPLKbBQFigH6f3i2HigO5M7N+dXtE6/kB5Dg0A1XFKsmD1v+yU1ZWF0GMYjLvDji3w61qzfh6urOWrXbbvYMWOfA4WVwLQJzaEKQPjmDY4ntNSovTqQLlKA0B1vNpK2DjfXhXkbYfQeDjt+5B2M4TEuF1dmzPGTluxYkcey3fk8dXuAmrqGojo5s9Zp9gwmDigu85yqjqcBoByjzF2LeOVT8GupXZMwajZMP4OiOzldnXtpry6js935rN0aw7/3p5DUUUtAb4+jO8bw7TB8UwbHK/jDlSH0ABQniF3O3z5GGxaYL8fdgWccSfEneJuXe2srr6BNfuLWLo1h6XbcthfYKe9PjUpgmmD4pk2JJ6B8WF6m6lqFxoAyrMUZ8DXT8CaF+2UE6dcAGf+BJKafI92KcYYduWWsWRrDp9sy2FdRjFgp6mYOsheGYxJidZ+A9VmNACUZyovgP/8E1b9E6qK7fTUZ86Fvmd3mTuHjie3tIpl23L5ZGsOn+/K/59+g0kDuhOi/QaqFTQAlGerLrVXA18/DqVZdhbSyfdA/3O8JgjgSL9BHku25vDv7bkUV9QS6OfDpAHdOf/UBM4eFK+dyOqkaQCozqGuGja8Dp8/YpuJeo5ygmCaVwUB2H6D1fuK+HhLNh9syiK3tJqAI2EwLIGzB8URFuTvdpmqE9AAUJ1LfS2snwcrHoHDGZCYZoOgn/c0DTXW0GBYk1HE+xuz+HBzFjklNgwm9u/O+af2YOqgeA0D1SwNANU51dXAhiNBkGkXt598t1f1ERytocGwNqOI9zdl8eGmbLJLqgjw9WHigFhmDEtg6uB4wjUMVCMaAKpzq6uB9a/Aikeh5AAkjYGz7oXUyW5X5qqGBsO6zCLe35jNh5uzyDpsw+CsU+K4dFQiUwbGEeCndxN5Ow0A1TXUVcO6V+DzR6HkIKROgam/hZ4j3K7MdTYMinl3wyHe23iI/LIaIoP9OX9YAjNHJTIqOUrHGXipdgsAEbkTuAUQ4BljzF9FZDjwDyAU2Adca4wpaWLffUApUA/UNVdgYxoACoDaKkh/FlY8DJVFMPQye0XQRWYgba26+gY+35XP2+sO8vGWbKpqG+gV3Y1pg3pw7pB4xvSJ1jDwIu0SACIyFHgdGAPUAB8BtwKvAT8zxiwXkZuAPsaYXzex/z4gzRiTf6LH1ABQ31F12I4s/vpJaKiFtJtg4i8gtLvblXmMsuo6Pt6czeINh/h6j52fKDk6mFmjkpg5KlFnLvUC7RUAlwPTjTE3O9//GqgG7gUijTFGRHoBHxtjBjex/z40AFRbKMmC5Q/C2pfAvxuc/iM711BgmNuVeZTy6jqWbM3mzTUH+HJXAQBjUqK5eGRPzh+WQGSwLnLTFbVXAAwC3gHGA5XAMiAdGA08ZIx5W0TuAu43xvzP/4kishcoAgzwT2PM080cZw4wByA5OXn0/v37W1Sv8gL5O2HZ72DbYgjpbq8GRt8IfvrBdrQDRRW8s/4Qb607yK7cMvx9hckD45g5MpGzBsUR6KfrIXcV7dkHcDNwO1AObMFeAfwD+BsQAywGfmyM+Z/5f0Uk0RhzUETigKXAj4wxK451PL0CUCfkQDosvQ/2fwFRKXDWr2HITPDRO2KOZoxhy6ES3l53kMUbDpFbWk1ksD8XntqTWaOTGJ4Uof0FnVyH3AUkIg8AB4wxTzbaNgB4xRgz5jj7/hYoM8Y8cqzXaQCoE2YM7PrEBkHuFkgYAec9BMlj3a7MY9XVN/DFrnwWrbWdx9V1DfSLC+Wq03oxa1SSroPcSbXnFUCcMSZXRJKBJcA4IMDZ5gO8AHxmjHnuqP1CAB9jTKnzfCnwO2PMR8c6ngaAOmkN9bBxgW0aKj0Ew6+GqfdDWLzblXm0kqpaPtiYxYL0TNZmFBPg68P0oT24Iq0Xp/eNwcdHrwo6i/YMgM+xTT21wF3GmGXOraF3OC9ZBNzjdAj3BP5ljJkhIqnAW85r/IB5xpg/Hu94GgCqxarL7BxDXz0OfkEw5R4YMwd8ddTs8WzPLuG1VRm8te4gJVV1JEZ2Y9aoRC5P66V3EXUCOhBMqSPyd8FHv7TNQ91PgfMe9PoRxSeqqraeJVtzeCM9ky922Zv3zuwXy1WnJTN1sHYceyoNAKUaMwa++RA+uhuK98PgS+CcP3TpJSrb2sHiSt5Iz2TB6kwOHa4iOiSAmSMTuWpML/rF6e23nkQDQKmm1FbCV3+3U0uID0y4C8b/CPx1rd4TVd9g+HxnHvNXZ7J0aw51DYa03lFcNSaZ84cl0C1ArwrcpgGg1LEUZ8DH99rxA1F9YPqfYeB0t6vqdPJKq1m09gDzV2eyJ7+csEA/Lh7Zk6tOS2ZoYoTb5XktDQClTsTuf8OHv4T8HdD/XJj+J4jp63ZVnY4xhv/sLWT+6kze35RFdV0DQ3qGc/243lwyMpEgf70q6EgaAEqdqLoau07xZ3+G+ho7rcSEn0JAiNuVdUqHK2tZvP4gr67KYHt2KWFBfpw3tAfXj0thWJJeFXQEDQClTlZpNiz9DWycD+GJcO4fbWexjoptEWMMq/YW8kb6AT7anEV5TT3jU2OYOSqRGcMSdOH7dqQBoFRL7f8aPvw5ZG+CPpPgwscguo/bVXVqJVW1zFuVwbxVGWQUVhAe5MfVY5O58fQUEiK6uV1el6MBoFRrNNRD+nPwyf1g6u3cQmN/AD7alt0axhjW7C/i+S/38eHmLHxEmD60BzeMT+G0FF3Apq1oACjVFg4fhPd+Ajs/tusTX/Q4xJ3idlVdQmZhBS9+tY8F6ZmUVNVxSo8wrh/fm0tGJGrzUCtpACjVVoyBTW/Ch7+AmjI75fSZc3VKiTZSWVPPO+sP8tLX+9maVUJYoB+zRidx/fje9O0e6nZ5nZIGgFJtrSzPhsCWRRA/FC5+HHqOdLuqLsMYw9qMIl7+ej/vb8qitt4woX8ss8enMOWUOHx1MroTpgGgVHvZ/j68dxeU59lbRiffbVclU20mr7Sa+aszeGVlBtklVfSK7sb143pzRVovXcXsBGgAKNWeKothyf/Bupchph9c9HfofbrbVXU5tfUNLNmSw4tf7+M/ewsJ8vdh5qgkbp3Yl+QYnZW0ORoASnWEPZ/B4h/bCeZOuwWm3qfrEreTrYdKeOnrfSxae5B6Y7hoeE/mTExlUEK426V5HA0ApTpKTTks+z2s+gdEJMGFf4V+U92uqsvKPlzFM5/vYd6qDCpr6zmlRxjXjuvNZaOSdCI6hwaAUh0tYxUs/qGdV2j4NXYkcXC021V1WYXlNby74RAL1x5g44HDRAX7c/34FG4Y35vY0EC3y3OVBoBSbqitghUPwxd/geAYOP8RGHyx21V1acYYVu8r4ukVe/hkWw6Bfj7MGp3EzWf28drbSDUAlHJT1kZ45w7I3mjnE7rgL3o10AF25Zbx7Bd7Wbj2ALX1DUwbFM9tk/syMjnK7dI6lAaAUm6rr4Mv/2pnGQ2OgUue0L6BDpJXWs1LX+/jxa/2UVJVx7jUaH44pT9n9IvxiukmNACU8hRZG2DRHMjbbu8UmvY7CNBbGDtCWXUdr/8ng2c+30NOSTVpvaO44fQUpg/pQYCfj9vltRsNAKU8SW0V/Pv38PXjdtzAzKchcbTbVXmNqtp6FqRn8vSKPRwoqqRnRBC3Tu7LrFFJXXLeIQ0ApTzRnuXw9m127YFJv7QLz/h2vQ8gT9XQYFi+I4/HP93Fmv1FhAb6ceVpvfjBxFTiwrvOutAaAEp5qspi+OBnsOkN6DXOXg1E9Xa7Kq9yZN6hl77ez7sbDuHv68M1Y5O5dVJf4rtAEGgAKOXpNi6A939qn5//KJx6hbv1eKl9+eU8/uku3lp3EF8RZo1O4gcTU0mJ7bxLgmoAKNUZFO23HcSZK2HY5TYIgnTdXDdkFFTwzxW7eWPNAerqG5gxLIHbJvdlSM/O9/fQAFCqs6ivswPHPvuTbQq6/EVIONXtqrxWbkkVz365l1dXZlBWXcfkgd25fXI/xvTpPOM4NACU6mwyVsIb34OKAjjvQRh9oy5I76LDlbW8snI/z32xl4LyGk7vG8OdZ/dnbGqM26UdlwaAUp1Reb5tEtq9zDYJXfBXCPTO6Qw8RWVNPfP+k8FTn+0mv6ya8akxzJ3q2UGgAaBUZ9XQAF88Cp8+YMcMXP4ixA92uyqvdyQI/rF8N3ml1YzpE80dU/oxsX+sx40u1gBQqrPbuwIWfh+qSmzn8Mhr3a5IYQeVzVuVwdMr9pBdUsWwxAjumNKXcwb3wMdDlq3UAFCqKyjNgYU3w77PYcR1MONhnUbCQ1TX1fP2uoM89dlu9hVU0D8ulJ9MG8D0Ie4HwbECoFUTYIjInSKyWUS2iMhcZ9twEflaRDaJyLsi0uQSPSIyXUS+EZFdInJ3a+pQyiuExcMN78DEX8D6V+FfZ0PeDrerUkCgny9XnpbMJ3dN4rGrRmCA219dy4WPf8Gn23Px1H9ot/gKQESGAq8DY4Aa4CPgVuA14GfGmOUichPQxxjz66P29QV2ANOAA8Bq4GpjzNZjHVOvAJRy7FoGi26x8wpd+BicernbFalG6hsMb687yF+X7SCzsJK03lH87NyBjHOhs7i9rgAGAauMMRXGmDpgOTATGACscF6zFJjVxL5jgF3GmD3GmBpskOhKGUqdqH5nw61f2DECi74P7861YaA8gq+PHUW87K7J/OGSoWQWVXDV0yu5/tlVrM8sdru8b7UmADYDE0QkRkSCgRlAL2AL//0wv9zZdrREILPR9wecbUqpExXeE2a/B2fMhTXPw7NToWC321WpRgL8fLhuXG+W/3wK984YxJZDJVzyxJfc8lI627NL3C6v5QFgjNkGPAgswTb/rAfqgZuA20VkDRCGbR5qMRGZIyLpIpKel5fXml+lVNfj6wfT7oer50NxJjw9Gba+43ZV6ihB/r7cMjGVFb+Ywl3TBrBydwHnPfY5P35tHXvzy12rq83uAhKRB4ADxpgnG20bALxijBlz1GvHA781xpzrfH8PgDHmT8c6hvYBKHUMxRl29PDBdBh7K0z7PfgFuF2VakJxRQ3/XLGHF77cR019A5eNSuLHU/uTGNmtzY/VbreBikicMSZXRJKxVwLjgABnmw/wAvCZMea5o/bzw3YCnw0cxHYCX2OM2XKs42kAKHUcdTXwyX2w8knoOQouf0Gnl/ZguaVVPPnpbuatygDgmrHJ3D6lL3FhbTcNdbvdBgosFJGtwLvAHcaYYuBqEdkBbAcOAc87RfQUkQ8AnE7jHwIfA9uABcf78FdKnQC/AJj+J7jyFdsf8M8JsP0Dt6tSzYgLC+K3Fw3h059PZuaoRF5euZ9JD33Ggx9tp7iiVa3nJ0QHginVVRXuhTdm23WIT/8RnH0f+Pq7XZU6hr355fxl6Q7e3XiI0AA/bpmYyk1n9iG0FUtV6khgpbxVbRUsuRdW/wv6TLRzCQV3nqmMvdX27BIeXbKDpVtziA4J4LZJfZl9ekqLFq9vzyYgpZQn8w+ycwdd8g87xfQzUyDnmOMtlQc4pUc4z9yQxtt3nMGQnuG8tjqD9phRQq8AlPIWmath/rVQU27XHj7lfLcrUieouKKGyOCW3dGlVwBKKeh1Gsz5DGIHwOvXwPKHoRP9A9CbtfTD/3g0AJTyJuE94XsfwKlXwqd/gDdutFcEyitpACjlbfy7waX/tAPFti2G5861g8iU19EAUMobicAZP4ZrFkBRBjw9BfZ/5XZVqoNpACjlzfpPg1uWQbdIePEiSH/e7YpUB9IAUMrbxfaH7y+D1Enw3lx4/6dQX+t2VaoDaAAopewVwDUL7Ijh1f+Cly+F8gK3q1LtTANAKWX5+MI5f7AdxJn/gWcmQ45O0dWVaQAopb5r+FXwvQ/tzKL/mgbffOR2RaqdaAAopf5X0mhn0Fh/eP1qWPmUDhrrgjQAlFJNC0+wg8YGzoCP7oYPfgb1dW5XpdqQBoBSqnkBIXDFy3D6j23n8GtXQpX7a9mqtqEBoJQ6Nh8fOOf3cOFjsOczHTnchWgAKKVOzOgb4bqFcPggPHMWHNCZeTs7DQCl1IlLnQzfX2qbhl44H7a85XZFqhU0AJRSJ6f7QDtyOGE4vPE9+PpJtytSLaQBoJQ6eSGxcMM7MOgC+Pge+OhX0NDgdlXqJGkAKKVaxr+bXWN47K2w8glYeJNdg1h1Gi1fal4ppXx8YfqfITwRlv4aynLhqlehW5TblakToFcASqnWObK2wKxn7RxCz02H4ky3q1InQANAKdU2hl0G1y+Ckix4dppOJNcJaAAopdpOn4lw04f2+XPnwb4v3a1HHZMGgFKqbcUPgZuXQFi8XVdg27tuV6SaoQGglGp7kclw08eQcCosuAFWP+t2RaoJGgBKqfYRHA03LIb+58D7d8GnD+iU0h5GA0Ap1X4CguHKV2HkdbD8QXj3Tmiod7sq5dBxAEqp9uXrBxc9DqHx8PmjUFUMM58Bv0C3K/N6GgBKqfYnAmf/BoJj4ONf2TUFrnwFAkPdrsyraROQUqrjjL8DLn4S9i6Hly+BikK3K/JqrQoAEblTRDaLyBYRmetsGyEiK0VkvYiki8iYZvatd16zXkQWt6YOpVQnMvJau8pY1gY7pXRJltsVea0WB4CIDAVuAcYAw4ELRKQf8BBwvzFmBPAb5/umVBpjRjhfF7W0DqVUJzToArj2Tbuy2HPnQuEetyvySq25AhgErDLGVBhj6oDlwEzAAOHOayKAQ60rUSnVJaVOgtmLobrUzh+UvdntirxOawJgMzBBRGJEJBiYAfQC5gIPi0gm8AhwTzP7BzlNRCtF5JLmDiIic5zXpefl5bWiXKWUx0kcDd/7EMQXXphhJ5NTHUZMKwZmiMjNwO1AObAFqMaGynJjzEIRuQKYY4yZ2sS+icaYgyKSCvwbONsYs/tYx0tLSzPp6boOqVJdTnEGvHQxlObANfOhzwS3K+oyRGSNMSatqZ+1qhPYGPOsMWa0MWYiUATsAGYDi5yXvIHtI2hq34PO4x7gM2Bka2pRSnVikcn2SiCyF7x6Gez6xO2KvEJr7wKKcx6Tse3/87Bt/pOcl5wF7GxivygRCXSexwJnAFtbU4tSqpML6wE3vg+x/eG1q2H7+25X1OW1dhzAQhHZCrwL3GGMKcbeGfSoiGwAHgDmAIhImoj8y9lvEJDuvOZT4M/GGA0ApbxdSCzMfhd6DLOTyG1edPx9VIu1qg+go2kfgFJeoqoE5l0BmavswLERV7tdUafVbn0ASinVLoLC4bqFkDIB3r4V0p9zu6IuSQNAKeWZAkLgmgXQ/1x47yfw9ZNuV9TlaAAopTyXf5CdNG7QRfDxPXY2UdVmNACUUp7NLwAuex6GXQ7Lfgf//qMuLNNGdDpopZTn8/WDS/9p1xBY8RDUVcK039tpplWLaQAopToHH1+48O/g1w2++jvUVsJ5D4OPNmS0lAaAUqrz8PGBGQ/bvoGv/g51VXDh32w4qJOmAaCU6lxEbPOPf7BdZ7i2Ci79B/j6u11Zp6MBoJTqfERgyq9sn8Cy39krgcuetx3G6oRp45lSqvOa8FOY/mfY/h7Mvw7qqt2uqFPRAFBKdW7jboPz/x/s/Bje+B7U17pdUaehAaCU6vxOu9neEfTN+7DwZqivc7uiTkH7AJRSXcPYOVBfA0vuBZ8fwMyn9e6g49AAUEp1Haf/0IbAsvvBNwAufkLHCRyDBoBSqmuZcJftB/jsATuC+ILHNASaoQGglOp6Jv0C6qvt5HG+ATDjEZ02ogkaAEqprkcEzvq1bQ766u82BM59QEPgKBoASqmu6ciI4fpaWPmkHSk89X4NgUY0AJRSXZeIHShWXwNfPga+gXDWvW5X5TE0AJRSXZsIzHjUhsCKh2xz0KSfu12VR9AAUEp1fT4+dtbQ+jr49A+2OejMuW5X5ToNAKWUd/DxteMC6mvgk/vslcD4292uylUaAEop7+HrZ0cIN9TaNYZ9/WHMLW5X5RodHaGU8i6+/jDrORhwHnzwM1jzotsVuUYDQCnlffwC4IoXod9UePdOWP+a2xW5QgNAKeWd/ALhylegz0R453bY9KbbFXU4DQCllPfy7wZXvw7Jp8OiObD1Hbcr6lAaAEop7xYQDNfMh6Q0ePMm2P6B2xV1GA0ApZQKDIVr34CE4bDgBtj1idsVdQgNAKWUAgiKgOsWQtwp8Pp1kLHS7YranQaAUkod0S0KrnsLIhLh1Ssga6PbFbUrDQCllGostDtc/zYEhsHLl0L+LrcrajetCgARuVNENovIFhGZ62wbISIrRWS9iKSLyJhm9p0tIjudr9mtqUMppdpUZC+4wbkj6KWLoTjT3XraSYsDQESGArcAY4DhwAUi0g94CLjfGDMC+I3z/dH7RgP3AWOd/e8TkaiW1qKUUm0uth9c/xZUl9oQKMt1u6I215orgEHAKmNMhTGmDlgOzAQMEO68JgI41MS+5wJLjTGFxpgiYCkwvRW1KKVU20s41d4dVJoFL8+EyiK3K2pTrQmAzcAEEYkRkWBgBtALmAs8LCKZwCPAPU3smwg0vqY64Gz7HyIyx2lKSs/Ly2tFuUop1QLJY+2I4bzttmO4ptztitpMiwPAGLMNeBBYAnwErAfqgduAnxhjegE/AZ5tTYHGmKeNMWnGmLTu3bu35lcppVTL9DsbLnsWDqbD69dCXY3bFbWJVnUCG2OeNcaMNsZMBIqAHcBsYJHzkjewbfxHO4i9WjgiydmmlFKeafDFcNHfYc+n8PZt0NDgdkWt1tq7gOKcx2Rs+/88bJv/JOclZwE7m9j1Y+AcEYlyOn/PcbYppZTnGnkdTP0tbH7TridgjNsVtUprF4RZKCIxQC1whzGmWERuAR4TET+gCpgDICJpwK3GmO8bYwpF5PfAauf3/M4YU9jKWpRSqv2dMRfK8mDlExDSHSb+zO2KWkxMJ0qwtLQ0k56e7nYZSilv19AAb98KG+fbtYZHe+5QJhFZY4xJa+pnuiSkUkqdLB8fu75wRQG8NxeCY2DQBW5XddJ0KgillGoJX3+44iXoOcpOI73vS7crOmkaAEop1VIBIXagWFRveO1qyN7sdkUnRQNAKaVaIzgarltk1xR4ZRYU7XO7ohOmAaCUUq0V2cuuJVBXZaeMKOscsxZoACilVFuIGwTXLICSQ/DqZXYSOQ+nAaCUUm0leSxc8SJkb4L513n8lBEaAEop1ZYGnAsXPw57PoN3bvfo0cI6DkAppdraiGvsFNLLfgdRKXDW/7ldUZM0AJRSqj2ceRcU7oUVD9sQGHmd2xX9Dw0ApZRqDyJwwV/g8AF4906ISILUyW5X9R3aB6CUUu3F1992CscOgPk3QO52tyv6Dg0ApZRqT0ER9vZQ/yB49XIozXG7om9pACilVHuL7AXXzIeKfHjtSo9ZVlIDQCmlOkLPkXDZc5C1ARbeAg31blekAaCUUh1m4Hkw/c/wzfuwxP1bQ/UuIKWU6khjf2BvD135JET1gbFzXCtFA0AppTrauX+E4gz46JcQmQwDp7tShjYBKaVUR/PxhVnPQMJwePN7cGidO2W4clSllPJ2ASFw9Xy7nOS8K6E4s8NL0ABQSim3hMXbFcVqK2HeFVB1uEMPrwGglFJuihsEV74M+TtgwWyor+2wQ2sAKKWU21Inw4WPwZ5P4f27OmwKab0LSCmlPMHI6+ztoZ8/Ym8PnXBXux9SA0AppTzFWf9nF5Vfdj9E9Yahs9r1cBoASinlKUTgkieh5CC8dRuEJ0LyuHY7nPYBKKWUJ/ELhKvm2fUDXrsaCna326E0AJRSytMER9vbQ8FOIV1R2C6H0QBQSilPFNMXrn7Nrij2+jVQW9Xmh9AAUEopT5U8Di59yq4o5uPb5r9eO4GVUsqTDZ3VbncDtSoARORO4BZAgGeMMX8VkfnAQOclkUCxMWZEE/vuA0qBeqDOGJPWmlqUUkqdnBYHgIgMxX74jwFqgI9E5D1jzJWNXvMocKzJLaYYY/JbWoNSSqmWa00fwCBglTGmwhhTBywHZh75oYgIcAXwWutKVEop1R5aEwCbgQkiEiMiwcAMoFejn08AcowxO5vZ3wBLRGSNiDS7JI6IzBGRdBFJz8vLa0W5SimlGmtxE5AxZpuIPAgsAcqB9dj2/COu5tj/+j/TGHNQROKApSKy3RizoonjPA08DZCWltYxMyQppZQXaNVtoMaYZ40xo40xE4EiYAeAiPhhm4PmH2Pfg85jLvAWti9BKaVUB2lVADj/ekdEkrEf+POcH00FthtjDjSzX4iIhB15DpyDbVJSSinVQVo7DmChiMQAtcAdxphiZ/tVHNX8IyI9gX8ZY2YA8cBbtp8YP2CeMeajVtailFLqJIjpoIUH2oKI5AH7W7h7LOCJt5xqXSfPU2vTuk6O1nXyWlJbb2NM96Z+0KkCoDVEJN0TB5tpXSfPU2vTuk6O1nXy2ro2nQtIKaW8lAaAUkp5KW8KgKfdLqAZWtfJ89TatK6To3WdvDatzWv6AJRSSn2XN10BKKWUakQDQCmlvFSXDwARmS4i34jILhG528U6eonIpyKyVUS2OGspICK/FZGDIrLe+ZrhUn37RGSTU0O6sy1aRJaKyE7nMaqDaxrY6LysF5ESEZnrxjkTkedEJFdENjfa1uT5Eetvzntuo4iMcqG2h0Vku3P8t0Qk0tmeIiKVjc7dPzq4rmb/diJyj3POvhGRczu4rvmNatonIuud7R15vpr7jGi/95kxpst+Ab7AbiAVCAA2AINdqiUBGOU8D8POmzQY+C3wMw84V/uA2KO2PQTc7Ty/G3jQ5b9lNtDbjXMGTARGAZuPd36wM+N+iF0oaRx22vSOru0cwM95/mCj2lIav86Fupr82zn/L2wAAoE+zv+3vh1V11E/fxT4jQvnq7nPiHZ7n3X1K4AxwC5jzB5jTA3wOnCxG4UYY7KMMWud56XANiDRjVpOwsXAi87zF4FL3CuFs4HdxpiWjgRvFWNnqi08anNz5+di4CVjrQQiRSShI2szxiwxdp0OgJVAUnsd/2TqOoaLgdeNMdXGmL3ALtppgshj1SXi3jomx/iMaLf3WVcPgEQgs9H3B/CAD10RSQFGAqucTT90LuGe6+hmlkaaWp8h3hiT5TzPxs7h5Jaj55fyhHPW3PnxtPfdTdh/KR7RR0TWichyEZngQj1N/e085Zw1tY5Jh5+voz4j2u191tUDwOOISCiwEJhrjCkBngL6AiOALOzlpxvONMaMAs4D7hCRiY1/aOw1pyv3DItIAHAR8IazyVPO2bfcPD/HIiL3AnXAq86mLCDZGDMSuAuYJyLhHViSx/3tjnL0OiYdfr6a+Iz4Vlu/z7p6ABzku6uUJTnbXCEi/tg/7KvGmEUAxpgcY0y9MaYBeAaX1kUwTa/PkHPkktJ5zHWjNmworTXG5Dg1esQ5o/nz4xHvOxG5EbgAuNb54MBpYilwnq/BtrUP6KiajvG3c/2cSRPrmHT0+WrqM4J2fJ919QBYDfQXkT7OvyKvAha7UYjTtvgssM0Y8/8abW/cZncpLqyLIM2vz7AYmO28bDbwTkfX5vjOv8o84Zw5mjs/i4EbnLs0xgGHG13CdwgRmQ78ArjIGFPRaHt3EfF1nqcC/YE9HVhXc3+7xcBVIhIoIn2cuv7TUXU5/mcdk448X819RtCe77OO6N128wvbU74Dm9z3uljHmdhLt43Y5TPXO7W9DGxyti8GElyoLRV7B8YGYMuR8wTEAMuAncAnQLQLtYUABUBEo20dfs6wAZSFXfviAHBzc+cHe1fGE857bhOQ5kJtu7Dtw0fea/9wXjvL+RuvB9YCF3ZwXc3+7YB7nXP2DXBeR9blbH8BuPWo13bk+WruM6Ld3mc6FYRSSnmprt4EpJRSqhkaAEop5aU0AJRSyktpACillJfSAFBKKS+lAaCUUl5KA0AppbzU/wdXP5PWbSKQowAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(rnn_model_7, patience=5, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[166.99881]\n",
      "  [359.90118]\n",
      "  [420.147  ]\n",
      "  ...\n",
      "  [423.9484 ]\n",
      "  [423.94827]\n",
      "  [423.94815]]\n",
      "\n",
      " [[166.51483]\n",
      "  [359.48444]\n",
      "  [420.10287]\n",
      "  ...\n",
      "  [423.94675]\n",
      "  [423.94656]\n",
      "  [423.9465 ]]\n",
      "\n",
      " [[166.45361]\n",
      "  [359.4035 ]\n",
      "  [420.09558]\n",
      "  ...\n",
      "  [423.94693]\n",
      "  [423.94684]\n",
      "  [423.9467 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[166.50644]\n",
      "  [359.47742]\n",
      "  [420.10526]\n",
      "  ...\n",
      "  [423.94702]\n",
      "  [423.9469 ]\n",
      "  [423.9468 ]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/3491075012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_7.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e588d92b224e11b16adbbadd39936dea13a6488171770263a646fc57f44563d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
